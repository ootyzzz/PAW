# P2W LoRAè®­ç»ƒé¡¹ç›®å”¯ä¸€å…¥å£è¯´æ˜

æœ¬é¡¹ç›®æ‰€æœ‰æ“ä½œï¼ˆç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½ã€è®­ç»ƒã€æ€§èƒ½ç›‘æ§ç­‰ï¼‰å‡é€šè¿‡ `baidu_gpu_lora_training.ipynb` notebook å®Œæˆã€‚

- ä¸å†ä½¿ç”¨ Makefileã€setup.shã€train.py ç­‰å•ç‹¬å…¥å£è„šæœ¬ã€‚
- åªéœ€ä¾æ¬¡è¿è¡Œ notebook å„å•å…ƒæ ¼ï¼Œå³å¯å®Œæˆå…¨éƒ¨æµç¨‹ã€‚
- åº•å±‚åŠŸèƒ½å¦‚æ¨¡å‹ä¸‹è½½ã€è®­ç»ƒç­‰ï¼Œå‡å¯åœ¨ notebook å†…é€šè¿‡ `!python scripts/xxx.py` æˆ– `subprocess` è°ƒç”¨ã€‚

## å¿«é€Ÿå¼€å§‹

1. æ‰“å¼€å¹¶è¿è¡Œ `baidu_gpu_lora_training.ipynb`
2. æŒ‰é¡ºåºæ‰§è¡Œå„å•å…ƒæ ¼ï¼Œå®Œæˆç¯å¢ƒæ£€æŸ¥ã€æ•°æ®å‡†å¤‡ã€æ¨¡å‹ä¸‹è½½ã€è®­ç»ƒã€æ€§èƒ½åˆ†æç­‰å…¨éƒ¨æµç¨‹ã€‚
3. å¦‚éœ€è‡ªå®šä¹‰å‚æ•°æˆ–æµç¨‹ï¼Œå¯ç›´æ¥åœ¨ notebook ä¸­ä¿®æ”¹ç›¸å…³ä»£ç ã€‚

---

> å…¶ä»–è„šæœ¬ä»…ä½œä¸º notebook çš„åº•å±‚å·¥å…·ï¼Œä¸å†å•ç‹¬æš´éœ²å…¥å£ã€‚

---

# P2W: ç°ä»£åŒ–LoRAè®­ç»ƒæ¡†æ¶

ä¸€ä¸ªåŸºäºHugging Faceç”Ÿæ€çš„ç°ä»£åŒ–LoRAè®­ç»ƒæ¡†æ¶ï¼Œä¸“ä¸ºfoundation modelsï¼ˆå¦‚Qwen2.5ï¼‰å¾®è°ƒè€Œè®¾è®¡ã€‚

## âœ¨ ç‰¹æ€§

- ğŸš€ **ç°ä»£åŒ–æ¶æ„**: åŸºäºHugging Face Transformers + PEFT + Accelerate
- ğŸ“Š **å®Œæ•´ç›‘æ§**: é›†æˆWandBã€TensorBoardç­‰è®­ç»ƒç›‘æ§å·¥å…·
- ğŸ”§ **æ˜“äºé…ç½®**: ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§è®­ç»ƒç­–ç•¥
- ğŸ“ˆ **å®éªŒç®¡ç†**: å®Œæ•´çš„å®éªŒè·Ÿè¸ªå’Œç»“æœç®¡ç†
- ğŸ¯ **ä¸“ä¸šçº§**: æ”¯æŒæ··åˆç²¾åº¦ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ã€åˆ†å¸ƒå¼è®­ç»ƒ
- ğŸ”„ **å¯æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°åŠŸèƒ½

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
P2W/
â”œâ”€â”€ configs/                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ models/                  # Foundation models å­˜å‚¨
â”œâ”€â”€ src/                     # æºä»£ç 
â”‚   â”œâ”€â”€ core/               # æ ¸å¿ƒè®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ data/               # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ adapters/           # æ•°æ®é€‚é…å™¨
â”œâ”€â”€ scripts/                # æ‰§è¡Œè„šæœ¬
â”œâ”€â”€ experiments/            # å®éªŒç®¡ç†
â”œâ”€â”€ logs/                   # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ checkpoints/            # æ£€æŸ¥ç‚¹
â””â”€â”€ outputs/                # è¾“å‡ºç»“æœ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install -r requirements_modern.txt
```

æˆ–è€…å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼š

```bash
pip install -e .
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
python scripts/model_manager.py --action download --model Qwen/Qwen2.5-0.5B
```

### 3. é…ç½®è®­ç»ƒ

ç¼–è¾‘ `configs/training_config.yaml`ï¼š

```yaml
model:
  name: "Qwen/Qwen2.5-0.5B"
  cache_dir: "./models"

lora:
  r: 8
  alpha: 16
  dropout: 0.05

training:
  output_dir: "./checkpoints/qwen2.5_lora"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  learning_rate: 2.0e-4
  run_name: "qwen2.5_lora_exp1"
```

### 4. å¼€å§‹è®­ç»ƒ

```bash
python scripts/train_lora.py --config configs/training_config.yaml --dataset your_dataset
```

## ğŸ“Š è®­ç»ƒç›‘æ§

æœ¬æ¡†æ¶é›†æˆäº†å¤šç§ç›‘æ§å·¥å…·ï¼š

### WandB ç›‘æ§
- å®æ—¶lossæ›²çº¿
- å­¦ä¹ ç‡å˜åŒ–
- æ¢¯åº¦åˆ†å¸ƒ
- ç³»ç»Ÿèµ„æºä½¿ç”¨

### TensorBoard
- æ ‡é‡æŒ‡æ ‡
- æ¨¡å‹ç»“æ„
- åµŒå…¥å¯è§†åŒ–

### æ—¥å¿—æ–‡ä»¶
- è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—
- é”™è¯¯ä¿¡æ¯è®°å½•
- æ€§èƒ½æŒ‡æ ‡

## ğŸ”§ é«˜çº§åŠŸèƒ½

### åˆ†å¸ƒå¼è®­ç»ƒ
```bash
accelerate launch --multi_gpu scripts/train_lora.py
```

### æ··åˆç²¾åº¦è®­ç»ƒ
```yaml
training:
  fp16: true
  gradient_checkpointing: true
```

### é‡åŒ–æ”¯æŒ
```yaml
model:
  load_in_8bit: true
  # æˆ–è€…
  load_in_4bit: true
```

## ğŸ“ˆ å®éªŒç®¡ç†

æ¯ä¸ªå®éªŒéƒ½ä¼šåœ¨ `experiments/` ç›®å½•ä¸‹åˆ›å»ºç‹¬ç«‹çš„æ–‡ä»¶å¤¹ï¼š

```
experiments/
â”œâ”€â”€ qwen2.5_lora_exp1/
â”‚   â”œâ”€â”€ config.yaml      # å®éªŒé…ç½®
â”‚   â”œâ”€â”€ results/         # è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ checkpoints/     # æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ logs/           # æ—¥å¿—
â””â”€â”€ qwen2.5_lora_exp2/
    â””â”€â”€ ...
```

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

- Qwen2.5 (0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B)
- Llama2/Llama3 ç³»åˆ—
- Mistral ç³»åˆ—
- Phi ç³»åˆ—
- å…¶ä»–åŸºäºTransformersçš„æ¨¡å‹

## ğŸ“ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®
```yaml
model:
  name: "Qwen/Qwen2.5-0.5B"
  cache_dir: "./models"
  torch_dtype: "auto"
  device_map: "auto"
```

### LoRAé…ç½®
```yaml
lora:
  r: 8                    # LoRAç§©
  alpha: 16               # LoRAç¼©æ”¾å› å­
  dropout: 0.05           # Dropoutç‡
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
```

### è®­ç»ƒé…ç½®
```yaml
training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  learning_rate: 2.0e-4
  warmup_steps: 100
  evaluation_strategy: "steps"
  logging_steps: 10
```

## ğŸ” æœ€ä½³å®è·µ

1. **æ¨¡å‹é€‰æ‹©**: ä»å°æ¨¡å‹å¼€å§‹ï¼ˆå¦‚Qwen2.5-0.5Bï¼‰
2. **æ‰¹é‡å¤§å°**: æ ¹æ®æ˜¾å­˜è°ƒæ•´batch_size
3. **å­¦ä¹ ç‡**: LoRAæ¨è2e-4åˆ°5e-4
4. **ç›‘æ§**: ä½¿ç”¨WandBè¿½è¸ªå®éªŒ
5. **æ£€æŸ¥ç‚¹**: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
6. **è¯„ä¼°**: è®¾ç½®åˆé€‚çš„è¯„ä¼°é—´éš”

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š
- Hugging Face Transformers
- PEFT
- Accelerate
- WandB
- PyTorch Lightning
