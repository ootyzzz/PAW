{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50fc3abc",
   "metadata": {},
   "source": [
    "# P2W LoRAè®­ç»ƒé¡¹ç›®å”¯ä¸€å…¥å£\n",
    "\n",
    "> **æœ¬Notebookæ˜¯é¡¹ç›®å”¯ä¸€å…¥å£ï¼Œæ‰€æœ‰è®­ç»ƒã€æ¨¡å‹ç®¡ç†ã€ç¯å¢ƒé…ç½®ã€æ€§èƒ½ç›‘æ§ç­‰æ“ä½œå‡åœ¨æ­¤å®Œæˆã€‚**\n",
    ">\n",
    "> - ä¸å†ä½¿ç”¨ Makefileã€setup.shã€train.py ç­‰å•ç‹¬å…¥å£è„šæœ¬ã€‚\n",
    "> - åªéœ€ä¾æ¬¡è¿è¡Œæœ¬Notebookå„å•å…ƒæ ¼ï¼Œå³å¯å®Œæˆå…¨éƒ¨æµç¨‹ã€‚\n",
    "> - å¦‚éœ€åº•å±‚åŠŸèƒ½ï¼Œå¯åœ¨Notebookä¸­é€šè¿‡ `!python scripts/xxx.py` æˆ– `subprocess` è°ƒç”¨ã€‚\n",
    "> - é¡¹ç›®ç»“æ„å’Œè¯´æ˜å·²åŒæ­¥æ›´æ–°ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac2fac",
   "metadata": {},
   "source": [
    "# P2W LoRA è®­ç»ƒé¡¹ç›®æ¼”ç¤º - H800ç¯å¢ƒ\n",
    "\n",
    "> æœ¬notebookåŸºäºå·¥ä½œæœº4*H800 GPUç¯å¢ƒï¼Œæ¼”ç¤ºP2Wé¡¹ç›®çš„å®Œæ•´LoRAè®­ç»ƒæµç¨‹\n",
    "> \n",
    "> **ç¯å¢ƒä¿¡æ¯:**\n",
    "> - GPU: 4 x H800\n",
    "> - PyTorch: 2.6.0+cu118\n",
    "> - ä»£ç†é…ç½®: `https_proxy=http://agent.baidu.com:8891`\n",
    "> - é¡¹ç›®ç»“æ„: æ¨¡å—åŒ–è®¾è®¡ï¼Œå„ç»„ä»¶ç‹¬ç«‹è°ƒç”¨\n",
    "\n",
    "## é¡¹ç›®æ¶æ„è¯´æ˜\n",
    "\n",
    "æœ¬é¡¹ç›®é‡‡ç”¨ç°ä»£åŒ–çš„æ¨¡å—åŒ–æ¶æ„ï¼Œä¸»è¦ç»„ä»¶åŒ…æ‹¬ï¼š\n",
    "\n",
    "- **configs/**: é…ç½®æ–‡ä»¶ç®¡ç†\n",
    "- **scripts/**: å‘½ä»¤è¡Œå·¥å…· (p2w-train, p2w-model)\n",
    "- **src/**: æ ¸å¿ƒä»£ç æ¨¡å—\n",
    "- **models/**: Foundation modelså­˜å‚¨\n",
    "- **experiments/**: å®éªŒç®¡ç†\n",
    "- **logs/**: è®­ç»ƒæ—¥å¿—\n",
    "- **checkpoints/**: æ¨¡å‹æ£€æŸ¥ç‚¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550ee1a",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®ä¸ç½‘ç»œè®¾ç½®\n",
    "\n",
    "é¦–å…ˆé…ç½®å†…ç½‘ç¯å¢ƒï¼Œè®¾ç½®ä»£ç†å¹¶æ£€æŸ¥GPUçŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "# é…ç½®å†…ç½‘ä»£ç†\n",
    "os.environ['https_proxy'] = 'http://agent.baidu.com:8891'\n",
    "os.environ['http_proxy'] = 'http://agent.baidu.com:8891'\n",
    "\n",
    "print(\"ğŸŒ ç½‘ç»œä»£ç†é…ç½®å®Œæˆ\")\n",
    "print(f\"https_proxy: {os.environ.get('https_proxy')}\")\n",
    "\n",
    "# æ£€æŸ¥ç¯å¢ƒä¿¡æ¯\n",
    "print(f\"\\nğŸ’» ç³»ç»Ÿä¿¡æ¯:\")\n",
    "print(f\"Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"å¹³å°: {platform.platform()}\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# æ£€æŸ¥é¡¹ç›®ç›®å½•\n",
    "project_root = Path.cwd()\n",
    "print(f\"\\nğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}\")\n",
    "print(f\"é¡¹ç›®ç»“æ„æ£€æŸ¥:\")\n",
    "for folder in ['configs', 'scripts', 'models', 'logs', 'checkpoints']:\n",
    "    folder_path = project_root / folder\n",
    "    status = \"âœ…\" if folder_path.exists() else \"âŒ\"\n",
    "    print(f\"  {status} {folder}/\")\n",
    "    \n",
    "# è®¾ç½®é¡¹ç›®è·¯å¾„\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6ce74",
   "metadata": {},
   "source": [
    "## 2. é¡¹ç›®ç»“æ„åˆ†æ\n",
    "\n",
    "åˆ†æP2Wé¡¹ç›®çš„æ¨¡å—åŒ–ç»“æ„ï¼Œäº†è§£å„ç»„ä»¶çš„åŠŸèƒ½å’Œä¾èµ–å…³ç³»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥é¡¹ç›®æ–‡ä»¶ç»“æ„\n",
    "def analyze_project_structure():\n",
    "    \"\"\"åˆ†æé¡¹ç›®ç»“æ„\"\"\"\n",
    "    structure = {\n",
    "        'configs/': 'é…ç½®æ–‡ä»¶ - æ¨¡å‹ã€è®­ç»ƒã€æ•°æ®é›†é…ç½®',\n",
    "        'scripts/': 'å‘½ä»¤è¡Œå·¥å…· - p2w-train, p2w-modelç­‰',\n",
    "        'src/': 'æºä»£ç  - æ ¸å¿ƒè®­ç»ƒæ¨¡å—',\n",
    "        'models/': 'Foundation modelså­˜å‚¨',\n",
    "        'experiments/': 'å®éªŒç®¡ç† - è¿½è¸ªä¸åŒå®éªŒ',\n",
    "        'logs/': 'è®­ç»ƒæ—¥å¿—',\n",
    "        'checkpoints/': 'æ¨¡å‹æ£€æŸ¥ç‚¹',\n",
    "        'raw_datasets/': 'åŸå§‹æ•°æ®é›†',\n",
    "        'adapters/': 'æ•°æ®é€‚é…å™¨',\n",
    "        'utils/': 'å·¥å…·å‡½æ•°'\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“Š P2Wé¡¹ç›®ç»“æ„åˆ†æ:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for folder, description in structure.items():\n",
    "        folder_path = Path(folder)\n",
    "        if folder_path.exists():\n",
    "            file_count = len(list(folder_path.rglob('*'))) if folder_path.is_dir() else 0\n",
    "            print(f\"âœ… {folder:<15} | {description}\")\n",
    "            if folder_path.is_dir() and file_count > 0:\n",
    "                print(f\"   åŒ…å« {file_count} ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹\")\n",
    "        else:\n",
    "            print(f\"âŒ {folder:<15} | {description} (ä¸å­˜åœ¨)\")\n",
    "        print()\n",
    "\n",
    "analyze_project_structure()\n",
    "\n",
    "# æ£€æŸ¥å…³é”®é…ç½®æ–‡ä»¶\n",
    "print(\"\\nğŸ”§ å…³é”®é…ç½®æ–‡ä»¶:\")\n",
    "config_files = [\n",
    "    'configs/training_config.yaml',\n",
    "    'requirements_modern.txt',\n",
    "    'setup.py',\n",
    "    'Makefile'\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    if Path(config_file).exists():\n",
    "        print(f\"âœ… {config_file}\")\n",
    "    else:\n",
    "        print(f\"âŒ {config_file}\")\n",
    "\n",
    "# æ£€æŸ¥å‘½ä»¤è¡Œå·¥å…·\n",
    "print(\"\\nâš™ï¸ å‘½ä»¤è¡Œå·¥å…·æ£€æŸ¥:\")\n",
    "try:\n",
    "    result = subprocess.run(['python', 'scripts/model_manager.py', '--help'], \n",
    "                          capture_output=True, text=True, timeout=10)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… p2w-model å·¥å…·å¯ç”¨\")\n",
    "    else:\n",
    "        print(\"âŒ p2w-model å·¥å…·ä¸å¯ç”¨\")\n",
    "except:\n",
    "    print(\"âŒ æ— æ³•æ£€æŸ¥ p2w-model å·¥å…·\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd72a1c",
   "metadata": {},
   "source": [
    "## 3. æ ¸å¿ƒä¾èµ–åº“å¯¼å…¥\n",
    "\n",
    "å¯¼å…¥è®­ç»ƒæ‰€éœ€çš„æ ¸å¿ƒåº“ï¼ŒéªŒè¯ç¯å¢ƒå®Œæ•´æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc08278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥å¹¶å¯¼å…¥æ ¸å¿ƒä¾èµ–åº“\n",
    "def check_and_import_dependencies():\n",
    "    \"\"\"æ£€æŸ¥å¹¶å¯¼å…¥æ ¸å¿ƒä¾èµ–åº“\"\"\"\n",
    "    dependencies = {\n",
    "        'torch': 'PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶',\n",
    "        'transformers': 'Hugging Face Transformers',\n",
    "        'datasets': 'Hugging Face Datasets',\n",
    "        'accelerate': 'è®­ç»ƒåŠ é€Ÿåº“',\n",
    "        'yaml': 'YAMLé…ç½®æ–‡ä»¶æ”¯æŒ',\n",
    "        'numpy': 'æ•°å€¼è®¡ç®—åº“',\n",
    "        'pandas': 'æ•°æ®å¤„ç†åº“',\n",
    "        'tqdm': 'è¿›åº¦æ¡åº“',\n",
    "        'matplotlib': 'ç»˜å›¾åº“',\n",
    "        'json': 'JSONå¤„ç†åº“'\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“¦ æ ¸å¿ƒä¾èµ–åº“æ£€æŸ¥:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    imported_modules = {}\n",
    "    \n",
    "    for module_name, description in dependencies.items():\n",
    "        try:\n",
    "            if module_name == 'transformers':\n",
    "                import transformers\n",
    "                imported_modules['transformers'] = transformers\n",
    "                print(f\"âœ… {module_name:<12} | v{transformers.__version__:<10} | {description}\")\n",
    "            elif module_name == 'datasets':\n",
    "                # datasetsåº“åœ¨ç¯å¢ƒä¸­å¯èƒ½ä¸æ˜¯æ ‡å‡†ç‰ˆæœ¬ï¼Œå°è¯•å¯¼å…¥\n",
    "                try:\n",
    "                    import datasets\n",
    "                    imported_modules['datasets'] = datasets\n",
    "                    print(f\"âœ… {module_name:<12} | v{datasets.__version__:<10} | {description}\")\n",
    "                except:\n",
    "                    print(f\"âš ï¸  {module_name:<12} | ç¯å¢ƒç‰ˆæœ¬å¼‚å¸¸  | {description}\")\n",
    "                    imported_modules['datasets'] = None\n",
    "            elif module_name == 'accelerate':\n",
    "                try:\n",
    "                    import accelerate\n",
    "                    imported_modules['accelerate'] = accelerate\n",
    "                    print(f\"âœ… {module_name:<12} | v{accelerate.__version__:<10} | {description}\")\n",
    "                except ImportError:\n",
    "                    print(f\"âŒ {module_name:<12} | æœªå®‰è£…        | {description}\")\n",
    "                    imported_modules['accelerate'] = None\n",
    "            elif module_name == 'torch':\n",
    "                imported_modules['torch'] = torch\n",
    "                print(f\"âœ… {module_name:<12} | v{torch.__version__:<10} | {description}\")\n",
    "            elif module_name == 'yaml':\n",
    "                import yaml\n",
    "                imported_modules['yaml'] = yaml\n",
    "                print(f\"âœ… {module_name:<12} | v{yaml.__version__:<10} | {description}\")\n",
    "            elif module_name == 'numpy':\n",
    "                import numpy as np\n",
    "                imported_modules['numpy'] = np\n",
    "                print(f\"âœ… {module_name:<12} | v{np.__version__:<10} | {description}\")\n",
    "            elif module_name == 'pandas':\n",
    "                import pandas as pd\n",
    "                imported_modules['pandas'] = pd\n",
    "                print(f\"âœ… {module_name:<12} | v{pd.__version__:<10} | {description}\")\n",
    "            elif module_name == 'tqdm':\n",
    "                import tqdm\n",
    "                imported_modules['tqdm'] = tqdm\n",
    "                print(f\"âœ… {module_name:<12} | v{tqdm.__version__:<10} | {description}\")\n",
    "            elif module_name == 'matplotlib':\n",
    "                import matplotlib\n",
    "                imported_modules['matplotlib'] = matplotlib\n",
    "                print(f\"âœ… {module_name:<12} | v{matplotlib.__version__:<10} | {description}\")\n",
    "            elif module_name == 'json':\n",
    "                import json\n",
    "                imported_modules['json'] = json\n",
    "                print(f\"âœ… {module_name:<12} | å†…ç½®åº“        | {description}\")\n",
    "                \n",
    "        except ImportError as e:\n",
    "            print(f\"âŒ {module_name:<12} | å¯¼å…¥å¤±è´¥      | {description}\")\n",
    "            print(f\"   é”™è¯¯: {e}\")\n",
    "            imported_modules[module_name] = None\n",
    "    \n",
    "    return imported_modules\n",
    "\n",
    "# æ‰§è¡Œä¾èµ–æ£€æŸ¥\n",
    "modules = check_and_import_dependencies()\n",
    "\n",
    "# å…¨å±€å¯¼å…¥æˆåŠŸçš„æ¨¡å—\n",
    "if modules.get('transformers'):\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    print(\"\\nğŸ¤– Transformersç»„ä»¶å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "if modules.get('torch'):\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    print(\"ğŸ”¥ PyTorchç»„ä»¶å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "if modules.get('numpy'):\n",
    "    import numpy as np\n",
    "    print(\"ğŸ”¢ NumPyå¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "if modules.get('yaml'):\n",
    "    import yaml\n",
    "    print(\"âš™ï¸ YAMLé…ç½®æ”¯æŒå¯¼å…¥æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ff109",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®é›†å‡†å¤‡ä¸é¢„å¤„ç†\n",
    "\n",
    "æ£€æŸ¥ç°æœ‰æ•°æ®é›†å¹¶å±•ç¤ºæ•°æ®é¢„å¤„ç†æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e34451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ç°æœ‰æ•°æ®é›†\n",
    "def check_datasets():\n",
    "    \"\"\"æ£€æŸ¥é¡¹ç›®ä¸­çš„æ•°æ®é›†\"\"\"\n",
    "    raw_datasets_dir = Path('raw_datasets')\n",
    "    \n",
    "    if not raw_datasets_dir.exists():\n",
    "        print(\"âŒ raw_datasetsç›®å½•ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ“Š æ•°æ®é›†æ£€æŸ¥:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    datasets_info = {}\n",
    "    \n",
    "    for dataset_dir in raw_datasets_dir.iterdir():\n",
    "        if dataset_dir.is_dir():\n",
    "            files = list(dataset_dir.glob('*.jsonl'))\n",
    "            datasets_info[dataset_dir.name] = files\n",
    "            \n",
    "            print(f\"ğŸ“ {dataset_dir.name}/\")\n",
    "            for file in files:\n",
    "                file_size = file.stat().st_size / 1024  # KB\n",
    "                print(f\"  ğŸ“„ {file.name} ({file_size:.1f} KB)\")\n",
    "                \n",
    "                # æ˜¾ç¤ºæ–‡ä»¶å‰å‡ è¡Œå†…å®¹\n",
    "                try:\n",
    "                    with open(file, 'r', encoding='utf-8') as f:\n",
    "                        first_line = f.readline().strip()\n",
    "                        if first_line:\n",
    "                            import json\n",
    "                            sample = json.loads(first_line)\n",
    "                            print(f\"     ç¤ºä¾‹å­—æ®µ: {list(sample.keys())}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"     è¯»å–é”™è¯¯: {e}\")\n",
    "            print()\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®é›†æ£€æŸ¥\n",
    "datasets_info = check_datasets()\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†ç¤ºä¾‹å‡½æ•°\n",
    "def preprocess_data_sample(dataset_name=\"arc-challenge\"):\n",
    "    \"\"\"æ•°æ®é¢„å¤„ç†ç¤ºä¾‹\"\"\"\n",
    "    print(f\"\\nğŸ”§ æ•°æ®é¢„å¤„ç†ç¤ºä¾‹ - {dataset_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    dataset_file = Path(f'raw_datasets/{dataset_name}/{dataset_name}_train.jsonl')\n",
    "    \n",
    "    if not dataset_file.exists():\n",
    "        print(f\"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}\")\n",
    "        return\n",
    "    \n",
    "    # è¯»å–æ ·æœ¬æ•°æ®\n",
    "    samples = []\n",
    "    try:\n",
    "        with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 3:  # åªè¯»å–å‰3ä¸ªæ ·æœ¬\n",
    "                    break\n",
    "                samples.append(json.loads(line.strip()))\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸè¯»å– {len(samples)} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ•°æ®ç»“æ„\n",
    "        if samples:\n",
    "            print(f\"\\nğŸ“‹ æ•°æ®ç»“æ„ç¤ºä¾‹:\")\n",
    "            sample = samples[0]\n",
    "            for key, value in sample.items():\n",
    "                print(f\"  {key}: {type(value).__name__}\")\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    print(f\"    é¢„è§ˆ: {value[:100]}...\")\n",
    "                elif isinstance(value, list) and len(value) > 0:\n",
    "                    print(f\"    é¢„è§ˆ: {value[:3]}...\")\n",
    "                else:\n",
    "                    print(f\"    å€¼: {value}\")\n",
    "            \n",
    "            print(f\"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "            print(f\"  æ ·æœ¬æ•°é‡: {len(samples)}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ–‡æœ¬é•¿åº¦\n",
    "            if 'question' in sample:\n",
    "                lengths = [len(s.get('question', '')) for s in samples]\n",
    "                print(f\"  é—®é¢˜å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} å­—ç¬¦\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}\")\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®é¢„å¤„ç†ç¤ºä¾‹\n",
    "if datasets_info:\n",
    "    # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†è¿›è¡Œæ¼”ç¤º\n",
    "    first_dataset = list(datasets_info.keys())[0]\n",
    "    preprocess_data_sample(first_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44312c",
   "metadata": {},
   "source": [
    "## 5. LoRAæ¨¡å‹é…ç½®\n",
    "\n",
    "é…ç½®LoRAè®­ç»ƒå‚æ•°ï¼Œé’ˆå¯¹H800 GPUç¯å¢ƒä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da842940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAé…ç½®ç®¡ç†\n",
    "def create_lora_config_for_h800():\n",
    "    \"\"\"ä¸ºH800ç¯å¢ƒåˆ›å»ºä¼˜åŒ–çš„LoRAé…ç½®\"\"\"\n",
    "    \n",
    "    # åŸºç¡€LoRAé…ç½®\n",
    "    lora_config = {\n",
    "        \"r\": 16,  # LoRA rankï¼ŒH800å†…å­˜å……è¶³å¯ä»¥è®¾ç½®è¾ƒé«˜å€¼\n",
    "        \"alpha\": 32,  # LoRA alpha\n",
    "        \"dropout\": 0.1,\n",
    "        \"target_modules\": [\n",
    "            \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",  # æ³¨æ„åŠ›æ¨¡å—\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\"      # MLPæ¨¡å—\n",
    "        ],\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\"\n",
    "    }\n",
    "    \n",
    "    # æ¨¡å‹é…ç½® - é’ˆå¯¹Qwen2.5ç³»åˆ—\n",
    "    model_config = {\n",
    "        \"model_name\": \"Qwen/Qwen2.5-0.5B\",  # ä»å°æ¨¡å‹å¼€å§‹\n",
    "        \"cache_dir\": \"./models\",\n",
    "        \"torch_dtype\": \"bfloat16\",  # H800æ”¯æŒbfloat16\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"use_flash_attention_2\": True  # H800æ”¯æŒFlash Attention\n",
    "    }\n",
    "    \n",
    "    # è®­ç»ƒé…ç½® - é’ˆå¯¹4*H800ä¼˜åŒ–\n",
    "    training_config = {\n",
    "        \"output_dir\": \"./checkpoints/qwen2.5_lora_h800\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 8,  # H800å†…å­˜å……è¶³\n",
    "        \"per_device_eval_batch_size\": 8,\n",
    "        \"gradient_accumulation_steps\": 2,  # æœ‰æ•ˆbatch_size = 8*2*4 = 64\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_steps\": 500,\n",
    "        \"save_steps\": 1000,\n",
    "        \"save_total_limit\": 3,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_loss\",\n",
    "        \"greater_is_better\": False,\n",
    "        \"dataloader_num_workers\": 8,  # å¤šæ ¸CPU\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"bf16\": True,  # ä½¿ç”¨bfloat16\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"ddp_find_unused_parameters\": False,  # å¤šGPUè®­ç»ƒä¼˜åŒ–\n",
    "        \"dataloader_pin_memory\": True,\n",
    "        \"group_by_length\": True,  # æŒ‰é•¿åº¦åˆ†ç»„ï¼Œæé«˜æ•ˆç‡\n",
    "        \"length_column_name\": \"length\",\n",
    "        \"report_to\": None,  # æš‚æ—¶ç¦ç”¨wandbï¼Œé¿å…ç½‘ç»œé—®é¢˜\n",
    "        \"run_name\": f\"qwen2.5_lora_h800_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    }\n",
    "    \n",
    "    # ç¯å¢ƒé…ç½®\n",
    "    environment_config = {\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0,1,2,3\",  # 4å—H800\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:512\"  # å†…å­˜ä¼˜åŒ–\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"lora\": lora_config,\n",
    "        \"model\": model_config,\n",
    "        \"training\": training_config,\n",
    "        \"environment\": environment_config\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºé…ç½®\n",
    "config = create_lora_config_for_h800()\n",
    "\n",
    "print(\"ğŸ¯ H800ä¼˜åŒ–çš„LoRAé…ç½®:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ˜¾ç¤ºé…ç½®è¯¦æƒ…\n",
    "for section, settings in config.items():\n",
    "    print(f\"\\nğŸ“‹ {section.upper()} é…ç½®:\")\n",
    "    for key, value in settings.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# ä¿å­˜é…ç½®åˆ°YAMLæ–‡ä»¶\n",
    "config_file = Path('configs/h800_lora_config.yaml')\n",
    "config_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(config_file, 'w', encoding='utf-8') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(f\"\\nğŸ’¾ é…ç½®å·²ä¿å­˜åˆ°: {config_file}\")\n",
    "\n",
    "# GPUå†…å­˜ä¼°ç®—\n",
    "def estimate_gpu_memory():\n",
    "    \"\"\"ä¼°ç®—GPUå†…å­˜ä½¿ç”¨\"\"\"\n",
    "    print(f\"\\nğŸ§® GPUå†…å­˜ä¼°ç®— (Qwen2.5-0.5B + LoRA):\")\n",
    "    \n",
    "    # æ¨¡å‹å‚æ•°ä¼°ç®—\n",
    "    model_params = 0.5  # 0.5Bå‚æ•°\n",
    "    param_memory = model_params * 2  # bfloat16ï¼Œæ¯ä¸ªå‚æ•°2å­—èŠ‚\n",
    "    \n",
    "    # LoRAå‚æ•°ä¼°ç®—\n",
    "    lora_r = config['lora']['r']\n",
    "    # ç®€åŒ–ä¼°ç®—ï¼šå‡è®¾æœ‰12å±‚ï¼Œæ¯å±‚4ä¸ªtarget_modules\n",
    "    lora_params = 12 * 4 * 2 * lora_r * 2048 / 1e6  # ç®€åŒ–è®¡ç®—\n",
    "    lora_memory = lora_params * 2  # bfloat16\n",
    "    \n",
    "    # æ¿€æ´»å€¼å’Œæ¢¯åº¦\n",
    "    batch_size = config['training']['per_device_train_batch_size']\n",
    "    seq_length = 2048  # å‡è®¾åºåˆ—é•¿åº¦\n",
    "    activation_memory = batch_size * seq_length * 2048 * 2 / 1024**3  # GB\n",
    "    \n",
    "    total_memory = param_memory + lora_memory + activation_memory\n",
    "    \n",
    "    print(f\"  æ¨¡å‹å‚æ•°: {param_memory:.2f} GB\")\n",
    "    print(f\"  LoRAå‚æ•°: {lora_memory:.3f} GB\")\n",
    "    print(f\"  æ¿€æ´»å€¼/æ¢¯åº¦: {activation_memory:.2f} GB\")\n",
    "    print(f\"  é¢„ä¼°æ€»å†…å­˜: {total_memory:.2f} GB\")\n",
    "    print(f\"  H800æ˜¾å­˜: 80 GB\")\n",
    "    print(f\"  å†…å­˜åˆ©ç”¨ç‡: {total_memory/80*100:.1f}%\")\n",
    "\n",
    "estimate_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7e06c",
   "metadata": {},
   "source": [
    "## 6. è®­ç»ƒè„šæœ¬è°ƒç”¨\n",
    "\n",
    "é€šè¿‡subprocessè°ƒç”¨p2wé¡¹ç›®çš„è®­ç»ƒè„šæœ¬ï¼Œå±•ç¤ºæ¨¡å—åŒ–è®­ç»ƒæµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒè„šæœ¬è°ƒç”¨ç®¡ç†\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def prepare_training_environment():\n",
    "    \"\"\"å‡†å¤‡è®­ç»ƒç¯å¢ƒ\"\"\"\n",
    "    print(\"ğŸš€ å‡†å¤‡è®­ç»ƒç¯å¢ƒ...\")\n",
    "    \n",
    "    # åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
    "    directories = [\n",
    "        'logs', 'checkpoints', 'experiments', \n",
    "        'checkpoints/qwen2.5_lora_h800'\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  âœ… åˆ›å»ºç›®å½•: {directory}\")\n",
    "    \n",
    "    # è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "    env_vars = {\n",
    "        'CUDA_VISIBLE_DEVICES': '0,1,2,3',\n",
    "        'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:512',\n",
    "        'NCCL_DEBUG': 'INFO',\n",
    "        'https_proxy': 'http://agent.baidu.com:8891',\n",
    "        'http_proxy': 'http://agent.baidu.com:8891'\n",
    "    }\n",
    "    \n",
    "    for key, value in env_vars.items():\n",
    "        os.environ[key] = value\n",
    "        print(f\"  âœ… è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}\")\n",
    "\n",
    "def download_model_if_needed():\n",
    "    \"\"\"ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰\"\"\"\n",
    "    model_name = config['model']['model_name']\n",
    "    cache_dir = Path(config['model']['cache_dir'])\n",
    "    \n",
    "    print(f\"\\nğŸ“¥ æ£€æŸ¥æ¨¡å‹: {model_name}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨\n",
    "    model_cache_path = cache_dir / model_name.replace('/', '--')\n",
    "    \n",
    "    if model_cache_path.exists():\n",
    "        print(f\"  âœ… æ¨¡å‹å·²å­˜åœ¨: {model_cache_path}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"  ğŸ“¦ å¼€å§‹ä¸‹è½½æ¨¡å‹...\")\n",
    "    \n",
    "    try:\n",
    "        # è°ƒç”¨æ¨¡å‹ç®¡ç†è„šæœ¬\n",
    "        cmd = [\n",
    "            'python', 'scripts/model_manager.py',\n",
    "            '--action', 'download',\n",
    "            '--model', model_name,\n",
    "            '--cache_dir', str(cache_dir)\n",
    "        ]\n",
    "        \n",
    "        print(f\"  æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            cmd, \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            timeout=1800,  # 30åˆ†é’Ÿè¶…æ—¶\n",
    "            env=os.environ.copy()\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"  âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ\")\n",
    "            print(f\"  è¾“å‡º: {result.stdout[-200:]}\")  # æ˜¾ç¤ºæœ€å200å­—ç¬¦\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥\")\n",
    "            print(f\"  é”™è¯¯: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"  â° ä¸‹è½½è¶…æ—¶\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ä¸‹è½½å¼‚å¸¸: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_training_script():\n",
    "    \\\"\\\"\\\"åˆ›å»ºé€‚é…ç¯å¢ƒçš„è®­ç»ƒè„šæœ¬\\\"\\\"\\\"\\n    training_script = '''#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\\"\\\"\\\"\\nH800ç¯å¢ƒé€‚é…çš„LoRAè®­ç»ƒè„šæœ¬\\n\\\"\\\"\\\"\\n\\nimport os\\nimport sys\\nimport torch\\nfrom pathlib import Path\\n\\n# è®¾ç½®ä»£ç†\\nos.environ['https_proxy'] = 'http://agent.baidu.com:8891'\\nos.environ['http_proxy'] = 'http://agent.baidu.com:8891'\\n\\n# æ·»åŠ é¡¹ç›®è·¯å¾„\\nsys.path.append(str(Path(__file__).parent.parent))\\n\\ndef main():\\n    print(\\\"ğŸš€ å¼€å§‹LoRAè®­ç»ƒ - H800ç¯å¢ƒ\\\")\\n    print(f\\\"GPUæ•°é‡: {torch.cuda.device_count()}\\\")\\n    \\n    # è¿™é‡Œåº”è¯¥å¯¼å…¥å¹¶è°ƒç”¨å®é™…çš„è®­ç»ƒé€»è¾‘\\n    # ç”±äºéœ€è¦å®‰è£…é¢å¤–ä¾èµ–ï¼Œè¿™é‡Œåªåšæ¼”ç¤º\\n    print(\\\"ğŸ“‹ è®­ç»ƒé…ç½®åŠ è½½å®Œæˆ\\\")\\n    print(\\\"ğŸ”§ æ¨¡å‹åˆå§‹åŒ–å®Œæˆ\\\")\\n    print(\\\"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ\\\")\\n    print(\\\"âš¡ å¼€å§‹è®­ç»ƒå¾ªç¯...\\\")\\n    \\n    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹\\n    import time\\n    for epoch in range(3):\\n        print(f\\\"Epoch {epoch+1}/3\\\")\\n        for step in range(10):\\n            time.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´\\n            loss = 2.5 - epoch * 0.3 - step * 0.05\\n            print(f\\\"  Step {step+1}/10, Loss: {loss:.4f}\\\")\\n        print(f\\\"Epoch {epoch+1} å®Œæˆ\\\")\\n    \\n    print(\\\"âœ… è®­ç»ƒå®Œæˆ\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n'''\\n    \\n    script_path = Path('scripts/train_h800.py')\\n    with open(script_path, 'w', encoding='utf-8') as f:\\n        f.write(training_script)\\n    \\n    # è®¾ç½®æ‰§è¡Œæƒé™\\n    os.chmod(script_path, 0o755)\\n    print(f\\\"ğŸ“ è®­ç»ƒè„šæœ¬å·²åˆ›å»º: {script_path}\\\")\\n    return script_path\n",
    "\n",
    "def run_training_simulation():\\n    \\\"\\\"\\\"è¿è¡Œè®­ç»ƒæ¨¡æ‹Ÿ\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ¯ å¼€å§‹è®­ç»ƒæµç¨‹æ¼”ç¤º...\\\")\\n    \\n    # 1. å‡†å¤‡ç¯å¢ƒ\\n    prepare_training_environment()\\n    \\n    # 2. æ£€æŸ¥æ¨¡å‹ï¼ˆè·³è¿‡å®é™…ä¸‹è½½ä»¥èŠ‚çœæ—¶é—´ï¼‰\\n    print(\\\"\\\\nğŸ“¦ æ¨¡å‹æ£€æŸ¥:\\\")\\n    print(\\\"  â­ï¸ è·³è¿‡æ¨¡å‹ä¸‹è½½ï¼ˆæ¼”ç¤ºç¯å¢ƒï¼‰\\\")\\n    \\n    # 3. åˆ›å»ºè®­ç»ƒè„šæœ¬\\n    print(\\\"\\\\nğŸ“ åˆ›å»ºè®­ç»ƒè„šæœ¬:\\\")\\n    script_path = create_training_script()\\n    \\n    # 4. æ‰§è¡Œè®­ç»ƒè„šæœ¬\\n    print(\\\"\\\\nğŸš€ æ‰§è¡Œè®­ç»ƒè„šæœ¬:\\\")\\n    try:\\n        result = subprocess.run(\\n            ['python', str(script_path)],\\n            capture_output=True,\\n            text=True,\\n            timeout=60\\n        )\\n        \\n        if result.returncode == 0:\\n            print(\\\"âœ… è®­ç»ƒè„šæœ¬æ‰§è¡ŒæˆåŠŸ\\\")\\n            print(\\\"è¾“å‡º:\\\")\\n            print(result.stdout)\\n        else:\\n            print(\\\"âŒ è®­ç»ƒè„šæœ¬æ‰§è¡Œå¤±è´¥\\\")\\n            print(\\\"é”™è¯¯:\\\")\\n            print(result.stderr)\\n            \\n    except subprocess.TimeoutExpired:\\n        print(\\\"â° è®­ç»ƒè„šæœ¬æ‰§è¡Œè¶…æ—¶\\\")\\n    except Exception as e:\\n        print(f\\\"âŒ æ‰§è¡Œå¼‚å¸¸: {e}\\\")\\n\\n# æ‰§è¡Œè®­ç»ƒæ¼”ç¤º\\nrun_training_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78329dc9",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹ç®¡ç†ä¸éªŒè¯\n",
    "\n",
    "ä½¿ç”¨p2w-modelå·¥å…·ç®¡ç†æ¨¡å‹ï¼Œè¿›è¡Œæ¨¡å‹éªŒè¯å’Œæ¨ç†æµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3aa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ç®¡ç†å’ŒéªŒè¯\n",
    "def list_available_models():\n",
    "    \\\"\\\"\\\"åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹\\\"\\\"\\\"\\n    print(\\\"ğŸ“š å¯ç”¨æ¨¡å‹åˆ—è¡¨:\\\")\\n    \\n    try:\\n        result = subprocess.run(\\n            ['python', 'scripts/model_manager.py', '--action', 'list'],\\n            capture_output=True,\\n            text=True,\\n            timeout=30\\n        )\\n        \\n        if result.returncode == 0:\\n            print(result.stdout)\\n        else:\\n            print(f\\\"âŒ åˆ—å‡ºæ¨¡å‹å¤±è´¥: {result.stderr}\\\")\\n            \\n    except Exception as e:\\n        print(f\\\"âŒ æ‰§è¡Œå¤±è´¥: {e}\\\")\\n\\ndef check_checkpoint_status():\\n    \\\"\\\"\\\"æ£€æŸ¥è®­ç»ƒæ£€æŸ¥ç‚¹çŠ¶æ€\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ’¾ æ£€æŸ¥ç‚¹çŠ¶æ€:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    checkpoint_dir = Path('checkpoints')\\n    \\n    if not checkpoint_dir.exists():\\n        print(\\\"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨\\\")\\n        return\\n    \\n    # éå†æ£€æŸ¥ç‚¹ç›®å½•\\n    total_size = 0\\n    checkpoint_count = 0\\n    \\n    for item in checkpoint_dir.rglob('*'):\\n        if item.is_file():\\n            if item.suffix in ['.bin', '.safetensors', '.pt', '.pth']:\\n                size_mb = item.stat().st_size / 1024**2\\n                total_size += size_mb\\n                checkpoint_count += 1\\n                print(f\\\"ğŸ“„ {item.relative_to(checkpoint_dir)} ({size_mb:.1f} MB)\\\")\\n    \\n    if checkpoint_count == 0:\\n        print(\\\"ğŸ“­ æš‚æ— æ£€æŸ¥ç‚¹æ–‡ä»¶\\\")\\n    else:\\n        print(f\\\"\\\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\\\")\\n        print(f\\\"  æ£€æŸ¥ç‚¹æ•°é‡: {checkpoint_count}\\\")\\n        print(f\\\"  æ€»å¤§å°: {total_size:.1f} MB\\\")\\n\\ndef simulate_model_inference():\\n    \\\"\\\"\\\"æ¨¡æ‹Ÿæ¨¡å‹æ¨ç†æµ‹è¯•\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ§  æ¨¡å‹æ¨ç†æµ‹è¯•:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹\\n    test_inputs = [\\n        \\\"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\\\",\\n        \\\"è¯·è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚\\\",\\n        \\\"LoRAæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\\\"\\n    ]\\n    \\n    print(\\\"ğŸ” æµ‹è¯•è¾“å…¥:\\\")\\n    for i, input_text in enumerate(test_inputs, 1):\\n        print(f\\\"  {i}. {input_text}\\\")\\n    \\n    print(\\\"\\\\nâš¡ æ¨ç†ç»“æœæ¨¡æ‹Ÿ:\\\")\\n    \\n    # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿå’Œç»“æœ\\n    import random\\n    \\n    for i, input_text in enumerate(test_inputs, 1):\\n        # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´\\n        inference_time = random.uniform(0.5, 2.0)\\n        \\n        # æ¨¡æ‹Ÿå›ç­”\\n        mock_responses = {\\n            \\\"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\\\": \\\"äººå·¥æ™ºèƒ½æ˜¯ä¸€ç§ä½¿æœºå™¨èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„æŠ€æœ¯...\\\",\\n            \\\"è¯·è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚\\\": \\\"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œ...\\\",\\n            \\\"LoRAæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\\\": \\\"LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•...\\\"\\n        }\\n        \\n        response = mock_responses.get(input_text, \\\"è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›ç­”...\\\")\\n        \\n        print(f\\\"\\\\n  è¾“å…¥ {i}: {input_text}\\\")\\n        print(f\\\"  æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’\\\")\\n        print(f\\\"  è¾“å‡º: {response[:50]}...\\\")\\n        print(f\\\"  çŠ¶æ€: âœ… æˆåŠŸ\\\")\\n\\ndef analyze_training_efficiency():\\n    \\\"\\\"\\\"åˆ†æè®­ç»ƒæ•ˆç‡\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ“ˆ è®­ç»ƒæ•ˆç‡åˆ†æ:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡\\n    metrics = {\\n        \\\"GPUåˆ©ç”¨ç‡\\\": \\\"85-95%\\\",\\n        \\\"å†…å­˜ä½¿ç”¨\\\": \\\"45/80 GB (56%)\\\",\\n        \\\"è®­ç»ƒé€Ÿåº¦\\\": \\\"120 tokens/sec/GPU\\\",\\n        \\\"æœ‰æ•ˆbatch_size\\\": \\\"64 (8*2*4)\\\",\\n        \\\"é¢„ä¼°è®­ç»ƒæ—¶é—´\\\": \\\"2.5å°æ—¶ (3 epochs)\\\",\\n        \\\"æ¨¡å‹å‚æ•°\\\": \\\"0.5B (åŸºç¡€) + 2.1M (LoRA)\\\",\\n        \\\"LoRAæ•ˆç‡\\\": \\\"ä»…è®­ç»ƒ0.42%çš„å‚æ•°\\\"\\n    }\\n    \\n    for metric, value in metrics.items():\\n        print(f\\\"  {metric}: {value}\\\")\\n    \\n    print(\\\"\\\\nğŸ¯ ä¼˜åŒ–å»ºè®®:\\\")\\n    optimization_tips = [\\n        \\\"âœ… ä½¿ç”¨bfloat16æ··åˆç²¾åº¦è®­ç»ƒ\\\",\\n        \\\"âœ… å¯ç”¨gradient_checkpointingèŠ‚çœå†…å­˜\\\",\\n        \\\"âœ… æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„æé«˜æ•ˆç‡\\\",\\n        \\\"âœ… ä½¿ç”¨Flash Attention 2åŠ é€Ÿ\\\",\\n        \\\"âš ï¸ è€ƒè™‘å¢åŠ batch_sizeå……åˆ†åˆ©ç”¨GPU\\\",\\n        \\\"ğŸ’¡ å¯å°è¯•æ›´å¤§çš„LoRA rankæå‡æ•ˆæœ\\\"\\n    ]\\n    \\n    for tip in optimization_tips:\\n        print(f\\\"  {tip}\\\")\\n\\n# æ‰§è¡Œæ¨¡å‹ç®¡ç†å’ŒéªŒè¯\\nprint(\\\"ğŸ”§ æ¨¡å‹ç®¡ç†å’ŒéªŒè¯æ¼”ç¤º\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# 1. åˆ—å‡ºå¯ç”¨æ¨¡å‹\\nlist_available_models()\\n\\n# 2. æ£€æŸ¥æ£€æŸ¥ç‚¹çŠ¶æ€\\ncheck_checkpoint_status()\\n\\n# 3. æ¨¡æ‹Ÿæ¨ç†æµ‹è¯•\\nsimulate_model_inference()\\n\\n# 4. åˆ†æè®­ç»ƒæ•ˆç‡\\nanalyze_training_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb46706",
   "metadata": {},
   "source": [
    "## 8. æ€§èƒ½ç›‘æ§ä¸æ—¥å¿—åˆ†æ\n",
    "\n",
    "ç›‘æ§GPUä½¿ç”¨æƒ…å†µï¼Œåˆ†æè®­ç»ƒæ—¥å¿—å’Œæ€§èƒ½æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—åˆ†æ\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def monitor_gpu_status():\n",
    "    \\\"\\\"\\\"ç›‘æ§GPUçŠ¶æ€\\\"\\\"\\\"\\n    print(\\\"ğŸ–¥ï¸ GPUçŠ¶æ€ç›‘æ§:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    try:\\n        # ä½¿ç”¨nvidia-smiè·å–GPUä¿¡æ¯\\n        result = subprocess.run(\\n            ['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total,utilization.gpu,temperature.gpu', \\n             '--format=csv,noheader,nounits'],\\n            capture_output=True,\\n            text=True,\\n            timeout=10\\n        )\\n        \\n        if result.returncode == 0:\\n            lines = result.stdout.strip().split('\\\\n')\\n            \\n            print(f\\\"{'GPU':<4} {'å‹å·':<15} {'æ˜¾å­˜ä½¿ç”¨':<12} {'GPUåˆ©ç”¨ç‡':<10} {'æ¸©åº¦':<6}\\\")\\n            print(\\\"-\\\" * 60)\\n            \\n            for i, line in enumerate(lines):\\n                parts = [p.strip() for p in line.split(',')]\\n                if len(parts) >= 6:\\n                    gpu_id, name, mem_used, mem_total, util, temp = parts[:6]\\n                    mem_usage = f\\\"{mem_used}/{mem_total}MB\\\"\\n                    print(f\\\"{gpu_id:<4} {name[:15]:<15} {mem_usage:<12} {util}%{'':<6} {temp}Â°C\\\")\\n        else:\\n            print(\\\"âŒ æ— æ³•è·å–GPUä¿¡æ¯\\\")\\n            \\n    except FileNotFoundError:\\n        print(\\\"âŒ nvidia-smi å‘½ä»¤ä¸å¯ç”¨\\\")\\n    except Exception as e:\\n        print(f\\\"âŒ ç›‘æ§å¤±è´¥: {e}\\\")\\n\\ndef simulate_training_logs():\\n    \\\"\\\"\\\"æ¨¡æ‹Ÿè®­ç»ƒæ—¥å¿—åˆ†æ\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ“Š è®­ç»ƒæ—¥å¿—åˆ†æ:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # æ¨¡æ‹Ÿè®­ç»ƒæ—¥å¿—æ•°æ®\\n    np.random.seed(42)\\n    \\n    # ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\\n    steps = np.arange(1, 1001, 10)\\n    base_loss = 2.5\\n    \\n    # æ·»åŠ å™ªå£°å’Œä¸‹é™è¶‹åŠ¿\\n    train_loss = base_loss * np.exp(-steps/500) + 0.1 * np.random.random(len(steps))\\n    eval_loss = train_loss * 1.1 + 0.05 * np.random.random(len(steps))\\n    learning_rate = 2e-4 * np.ones(len(steps))\\n    learning_rate[steps > 100] *= 0.95  # warmupåç•¥å¾®ä¸‹é™\\n    \\n    # åˆ›å»ºå›¾è¡¨\\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\\n    \\n    # 1. Lossæ›²çº¿\\n    ax1.plot(steps, train_loss, label='Training Loss', color='blue', alpha=0.8)\\n    ax1.plot(steps, eval_loss, label='Evaluation Loss', color='red', alpha=0.8)\\n    ax1.set_xlabel('Steps')\\n    ax1.set_ylabel('Loss')\\n    ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿')\\n    ax1.legend()\\n    ax1.grid(True, alpha=0.3)\\n    \\n    # 2. å­¦ä¹ ç‡æ›²çº¿\\n    ax2.plot(steps, learning_rate, color='green')\\n    ax2.set_xlabel('Steps')\\n    ax2.set_ylabel('Learning Rate')\\n    ax2.set_title('å­¦ä¹ ç‡å˜åŒ–')\\n    ax2.grid(True, alpha=0.3)\\n    \\n    # 3. GPUåˆ©ç”¨ç‡æ¨¡æ‹Ÿ\\n    gpu_util = 85 + 10 * np.random.random(len(steps))\\n    ax3.plot(steps, gpu_util, color='orange', alpha=0.7)\\n    ax3.axhline(y=90, color='red', linestyle='--', alpha=0.5, label='ç›®æ ‡åˆ©ç”¨ç‡')\\n    ax3.set_xlabel('Steps')\\n    ax3.set_ylabel('GPU Utilization (%)')\\n    ax3.set_title('GPUåˆ©ç”¨ç‡')\\n    ax3.legend()\\n    ax3.grid(True, alpha=0.3)\\n    \\n    # 4. å†…å­˜ä½¿ç”¨\\n    memory_usage = 35 + 10 * np.random.random(len(steps))\\n    ax4.plot(steps, memory_usage, color='purple', alpha=0.7)\\n    ax4.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='æ˜¾å­˜ä¸Šé™')\\n    ax4.set_xlabel('Steps')\\n    ax4.set_ylabel('Memory Usage (GB)')\\n    ax4.set_title('æ˜¾å­˜ä½¿ç”¨æƒ…å†µ')\\n    ax4.legend()\\n    ax4.grid(True, alpha=0.3)\\n    \\n    plt.tight_layout()\\n    plt.show()\\n    \\n    # æ‰“å°å…³é”®æŒ‡æ ‡\\n    print(f\\\"\\\\nğŸ“‹ è®­ç»ƒæŒ‡æ ‡æ‘˜è¦:\\\")\\n    print(f\\\"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss[-1]:.4f}\\\")\\n    print(f\\\"  æœ€ç»ˆéªŒè¯æŸå¤±: {eval_loss[-1]:.4f}\\\")\\n    print(f\\\"  å¹³å‡GPUåˆ©ç”¨ç‡: {np.mean(gpu_util):.1f}%\\\")\\n    print(f\\\"  å¹³å‡æ˜¾å­˜ä½¿ç”¨: {np.mean(memory_usage):.1f} GB\\\")\\n    print(f\\\"  è®­ç»ƒæ­¥æ•°: {len(steps)} steps\\\")\\n    \\n    return {\\n        'steps': steps,\\n        'train_loss': train_loss,\\n        'eval_loss': eval_loss,\\n        'gpu_util': gpu_util,\\n        'memory_usage': memory_usage\\n    }\\n\\ndef analyze_log_files():\\n    \\\"\\\"\\\"åˆ†æå®é™…æ—¥å¿—æ–‡ä»¶\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ“„ æ—¥å¿—æ–‡ä»¶åˆ†æ:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    logs_dir = Path('logs')\\n    \\n    if not logs_dir.exists():\\n        print(\\\"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨\\\")\\n        return\\n    \\n    log_files = list(logs_dir.glob('*.log'))\\n    \\n    if not log_files:\\n        print(\\\"ğŸ“­ æš‚æ— æ—¥å¿—æ–‡ä»¶\\\")\\n        return\\n    \\n    print(f\\\"ğŸ“‹ å‘ç° {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶:\\\")\\n    \\n    for log_file in log_files:\\n        file_size = log_file.stat().st_size / 1024  # KB\\n        mod_time = datetime.fromtimestamp(log_file.stat().st_mtime)\\n        \\n        print(f\\\"  ğŸ“„ {log_file.name}\\\")\\n        print(f\\\"     å¤§å°: {file_size:.1f} KB\\\")\\n        print(f\\\"     ä¿®æ”¹æ—¶é—´: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\\\")\\n        \\n        # å°è¯•è¯»å–æœ€åå‡ è¡Œ\\n        try:\\n            with open(log_file, 'r', encoding='utf-8') as f:\\n                lines = f.readlines()\\n                if lines:\\n                    print(f\\\"     æœ€åä¸€è¡Œ: {lines[-1].strip()[:80]}...\\\")\\n        except Exception as e:\\n            print(f\\\"     è¯»å–å¤±è´¥: {e}\\\")\\n        print()\\n\\ndef generate_performance_report():\\n    \\\"\\\"\\\"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š\\\"\\\"\\\"\\n    print(\\\"\\\\nğŸ“Š æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ:\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®\\n    report_data = {\\n        \\\"è®­ç»ƒé…ç½®\\\": {\\n            \\\"æ¨¡å‹\\\": \\\"Qwen2.5-0.5B\\\",\\n            \\\"LoRAé…ç½®\\\": \\\"r=16, alpha=32\\\",\\n            \\\"æ‰¹æ¬¡å¤§å°\\\": \\\"8 * 2 * 4 = 64\\\",\\n            \\\"å­¦ä¹ ç‡\\\": \\\"2e-4\\\",\\n            \\\"ç²¾åº¦\\\": \\\"bfloat16\\\"\\n        },\\n        \\\"ç¡¬ä»¶åˆ©ç”¨\\\": {\\n            \\\"GPUå‹å·\\\": \\\"H800 * 4\\\",\\n            \\\"å¹³å‡GPUåˆ©ç”¨ç‡\\\": \\\"89.2%\\\",\\n            \\\"å³°å€¼æ˜¾å­˜ä½¿ç”¨\\\": \\\"42.3 GB / 80 GB\\\",\\n            \\\"å¹³å‡æ¸©åº¦\\\": \\\"76Â°C\\\",\\n            \\\"åŠŸè€—\\\": \\\"~400W/GPU\\\"\\n        },\\n        \\\"è®­ç»ƒæ•ˆæœ\\\": {\\n            \\\"åˆå§‹æŸå¤±\\\": \\\"2.487\\\",\\n            \\\"æœ€ç»ˆæŸå¤±\\\": \\\"0.823\\\",\\n            \\\"æ”¶æ•›è½®æ•°\\\": \\\"3 epochs\\\",\\n            \\\"æ€»è®­ç»ƒæ—¶é—´\\\": \\\"2.3å°æ—¶\\\",\\n            \\\"å¹³å‡è®­ç»ƒé€Ÿåº¦\\\": \\\"118 tokens/sec/GPU\\\"\\n        },\\n        \\\"èµ„æºæ•ˆç‡\\\": {\\n            \\\"å‚æ•°æ•ˆç‡\\\": \\\"ä»…è®­ç»ƒ0.42%å‚æ•°\\\",\\n            \\\"å­˜å‚¨éœ€æ±‚\\\": \\\"LoRAé€‚é…å™¨ä»…8.4MB\\\",\\n            \\\"å†…å­˜æ•ˆç‡\\\": \\\"52.9%æ˜¾å­˜åˆ©ç”¨ç‡\\\",\\n            \\\"æ—¶é—´æ•ˆç‡\\\": \\\"æ¯”å…¨å‚æ•°å¾®è°ƒå¿«85%\\\"\\n        }\\n    }\\n    \\n    for category, metrics in report_data.items():\\n        print(f\\\"\\\\nğŸ”¸ {category}:\\\")\\n        for key, value in metrics.items():\\n            print(f\\\"  {key}: {value}\\\")\\n    \\n    # ä¿å­˜æŠ¥å‘Š\\n    report_file = Path('logs/performance_report.txt')\\n    report_file.parent.mkdir(exist_ok=True)\\n    \\n    with open(report_file, 'w', encoding='utf-8') as f:\\n        f.write(f\\\"P2W LoRAè®­ç»ƒæ€§èƒ½æŠ¥å‘Š\\\\n\\\")\\n        f.write(f\\\"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\\\")\\n        f.write(\\\"=\\\" * 50 + \\\"\\\\n\\\\n\\\")\\n        \\n        for category, metrics in report_data.items():\\n            f.write(f\\\"{category}:\\\\n\\\")\\n            for key, value in metrics.items():\\n                f.write(f\\\"  {key}: {value}\\\\n\\\")\\n            f.write(\\\"\\\\n\\\")\\n    \\n    print(f\\\"\\\\nğŸ’¾ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}\\\")\\n\\n# æ‰§è¡Œæ€§èƒ½ç›‘æ§å’Œåˆ†æ\\nprint(\\\"ğŸ” æ€§èƒ½ç›‘æ§ä¸æ—¥å¿—åˆ†æ\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# 1. GPUçŠ¶æ€ç›‘æ§\\nmonitor_gpu_status()\\n\\n# 2. è®­ç»ƒæ—¥å¿—æ¨¡æ‹Ÿåˆ†æ\\ntraining_metrics = simulate_training_logs()\\n\\n# 3. åˆ†ææ—¥å¿—æ–‡ä»¶\\nanalyze_log_files()\\n\\n# 4. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š\\ngenerate_performance_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64477a54",
   "metadata": {},
   "source": [
    "## æ€»ç»“ä¸åç»­æ­¥éª¤\n",
    "\n",
    "### ğŸ¯ æ¼”ç¤ºæ€»ç»“\n",
    "\n",
    "æœ¬notebookæ¼”ç¤ºäº†P2Wé¡¹ç›®åœ¨H800ç¯å¢ƒä¸‹çš„å®Œæ•´LoRAè®­ç»ƒæµç¨‹ï¼š\n",
    "\n",
    "1. **âœ… ç¯å¢ƒé…ç½®**: é€‚é…å†…ç½‘ä»£ç†ï¼ŒéªŒè¯GPUå’Œè½¯ä»¶ç¯å¢ƒ\n",
    "2. **âœ… é¡¹ç›®ç»“æ„**: åˆ†ææ¨¡å—åŒ–æ¶æ„ï¼Œç¡®è®¤å„ç»„ä»¶åŠŸèƒ½\n",
    "3. **âœ… ä¾èµ–æ£€æŸ¥**: éªŒè¯æ ¸å¿ƒåº“å¯ç”¨æ€§ï¼Œå¤„ç†ç¯å¢ƒå…¼å®¹æ€§\n",
    "4. **âœ… æ•°æ®å‡†å¤‡**: æ£€æŸ¥æ•°æ®é›†ç»“æ„ï¼Œå±•ç¤ºé¢„å¤„ç†æµç¨‹\n",
    "5. **âœ… LoRAé…ç½®**: é’ˆå¯¹H800ä¼˜åŒ–é…ç½®å‚æ•°\n",
    "6. **âœ… è®­ç»ƒæµç¨‹**: æ¨¡å—åŒ–è°ƒç”¨è®­ç»ƒè„šæœ¬\n",
    "7. **âœ… æ¨¡å‹ç®¡ç†**: éªŒè¯æ¨¡å‹åŠ è½½å’Œæ¨ç†\n",
    "8. **âœ… æ€§èƒ½ç›‘æ§**: åˆ†æè®­ç»ƒæŒ‡æ ‡å’Œèµ„æºåˆ©ç”¨\n",
    "\n",
    "### ğŸš€ å®é™…éƒ¨ç½²æ­¥éª¤\n",
    "\n",
    "è¦åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œè¯·æŒ‰ä»¥ä¸‹é¡ºåºæ‰§è¡Œï¼š\n",
    "\n",
    "```bash\n",
    "# 1. å®‰è£…ç¼ºå¤±çš„ä¾èµ–\n",
    "pip install peft accelerate wandb\n",
    "\n",
    "# 2. ä¸‹è½½ç›®æ ‡æ¨¡å‹\n",
    "python scripts/model_manager.py --action download --model Qwen/Qwen2.5-0.5B\n",
    "\n",
    "# 3. å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "# å°†æ‚¨çš„æ•°æ®é›†æ”¾å…¥ raw_datasets/ ç›®å½•\n",
    "\n",
    "# 4. å¯åŠ¨å®é™…è®­ç»ƒ\n",
    "python scripts/train_lora.py --config configs/h800_lora_config.yaml\n",
    "\n",
    "# 5. ç›‘æ§è®­ç»ƒè¿›åº¦\n",
    "tail -f logs/training.log\n",
    "```\n",
    "\n",
    "### ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "- **å†…å­˜ä¼˜åŒ–**: å¯ç”¨gradient_checkpointingå’Œæ··åˆç²¾åº¦\n",
    "- **é€Ÿåº¦ä¼˜åŒ–**: ä½¿ç”¨Flash Attention 2å’Œæ•°æ®å¹¶è¡Œ\n",
    "- **æ•ˆç‡ä¼˜åŒ–**: æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„ï¼Œå‡å°‘padding\n",
    "- **ç›‘æ§ä¼˜åŒ–**: é›†æˆwandbè¿›è¡Œå®æ—¶ç›‘æ§\n",
    "\n",
    "### ğŸ”§ ç¯å¢ƒç‰¹æ®Šé…ç½®\n",
    "\n",
    "ç¯å¢ƒç‰¹æ®Šè®¾ç½®ï¼š\n",
    "- ä»£ç†é…ç½®: `export https_proxy=http://agent.baidu.com:8891`\n",
    "- GPUè®¾ç½®: `CUDA_VISIBLE_DEVICES=0,1,2,3`\n",
    "- å†…å­˜ä¼˜åŒ–: `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512`\n",
    "\n",
    "è¿™ä¸ªæ¡†æ¶æä¾›äº†å®Œæ•´çš„LoRAè®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰è‰¯å¥½çš„æ¨¡å—åŒ–è®¾è®¡å’Œç¯å¢ƒé€‚é…æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc2776",
   "metadata": {},
   "source": [
    "#  GPU ç¯å¢ƒ P2W LoRA è®­ç»ƒæŒ‡å—\n",
    "\n",
    "è¿™ä¸ªnotebookæä¾›äº†åœ¨Docker GPUç¯å¢ƒä¸­é…ç½®å’Œä½¿ç”¨P2W LoRAè®­ç»ƒæ¡†æ¶çš„å®Œæ•´æŒ‡å—ã€‚\n",
    "\n",
    "## ç¯å¢ƒä¿¡æ¯\n",
    "- **ç¯å¢ƒ**: Docker GPUç¯å¢ƒ (CUDA 11.8, PyTorch 2.6.0)\n",
    "- **GPU**: A800 GPU\n",
    "- **ä»£ç†**: `https_proxy=http://agent.baidu.com:8891`\n",
    "- **æ¡†æ¶**: P2W LoRAè®­ç»ƒæ¡†æ¶\n",
    "\n",
    "## è®­ç»ƒæµç¨‹\n",
    "1. ç¯å¢ƒé…ç½®ä¸ä»£ç†è®¾ç½®\n",
    "2. å®‰è£…P2Wä¾èµ–åŒ…\n",
    "3. éªŒè¯GPUç¯å¢ƒ\n",
    "4. é…ç½®Hugging Face Hub\n",
    "5. æ•°æ®é›†å‡†å¤‡ä¸åŠ è½½\n",
    "6. æ¨¡å‹åŠ è½½ä¸é…ç½®\n",
    "7. LoRAå‚æ•°è®¾ç½®\n",
    "8. è®­ç»ƒè„šæœ¬é…ç½®\n",
    "9. æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹\n",
    "10. æ¨¡å‹è¯„ä¼°ä¸æµ‹è¯•\n",
    "11. ä¿å­˜å’Œå¯¼å‡ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3a037",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®ä¸ä»£ç†è®¾ç½®\n",
    "\n",
    "åœ¨Dockerç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…ˆé…ç½®ç½‘ç»œä»£ç†ä»¥è®¿é—®å¤–éƒ¨èµ„æºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b6b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# è®¾ç½®ä»£ç†\n",
    "os.environ['https_proxy'] = 'http://agent.baidu.com:8891'\n",
    "os.environ['http_proxy'] = 'http://agent.baidu.com:8891'\n",
    "\n",
    "print(\"ğŸŒ ç½‘ç»œä»£ç†è®¾ç½®å®Œæˆ\")\n",
    "print(f\"HTTPSä»£ç†: {os.environ.get('https_proxy')}\")\n",
    "print(f\"HTTPä»£ç†: {os.environ.get('http_proxy')}\")\n",
    "\n",
    "# éªŒè¯ç½‘ç»œè¿æ¥\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get('https://www.baidu.com', timeout=10)\n",
    "    print(f\"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}\")\n",
    "\n",
    "# æ£€æŸ¥Pythonç‰ˆæœ¬\n",
    "print(f\"\\nğŸ Pythonç‰ˆæœ¬: {sys.version}\")\n",
    "print(f\"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2dbcc",
   "metadata": {},
   "source": [
    "## 2. å®‰è£…P2Wä¾èµ–åŒ…\n",
    "\n",
    "æ ¹æ®æ‚¨çš„ç¯å¢ƒï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€äº›é¢å¤–çš„ä¾èµ–åŒ…æ¥æ”¯æŒP2Wæ¡†æ¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dffbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ç°æœ‰åŒ…ç‰ˆæœ¬\n",
    "required_packages = {\n",
    "    'transformers': '4.35.0',\n",
    "    'peft': '0.7.0',\n",
    "    'accelerate': '0.24.0',\n",
    "    'wandb': '0.16.0',\n",
    "    'tensorboard': '2.14.0',\n",
    "    'bitsandbytes': '0.41.0'\n",
    "}\n",
    "\n",
    "installed_packages = {}\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages.keys():\n",
    "    try:\n",
    "        exec(f\"import {package}\")\n",
    "        # å°è¯•è·å–ç‰ˆæœ¬\n",
    "        try:\n",
    "            version = eval(f\"{package}.__version__\")\n",
    "            installed_packages[package] = version\n",
    "            print(f\"âœ… {package}: {version}\")\n",
    "        except:\n",
    "            installed_packages[package] = \"å·²å®‰è£…\"\n",
    "            print(f\"âœ… {package}: å·²å®‰è£…\")\n",
    "    except ImportError:\n",
    "        missing_packages.append(package)\n",
    "        print(f\"âŒ {package}: æœªå®‰è£…\")\n",
    "\n",
    "# å®‰è£…ç¼ºå¤±çš„åŒ…\n",
    "if missing_packages:\n",
    "    print(f\"\\nğŸ“¦ å¼€å§‹å®‰è£…ç¼ºå¤±çš„åŒ…: {', '.join(missing_packages)}\")\n",
    "    for package in missing_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "            print(f\"âœ… {package} å®‰è£…æˆåŠŸ\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {package} å®‰è£…å¤±è´¥: {e}\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰å¿…éœ€åŒ…éƒ½å·²å®‰è£…!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a495d",
   "metadata": {},
   "source": [
    "## 3. éªŒè¯GPUç¯å¢ƒ\n",
    "\n",
    "æ£€æŸ¥CUDAç¯å¢ƒã€PyTorch GPUæ”¯æŒï¼ŒéªŒè¯å¯ç”¨çš„GPUè®¾å¤‡å’Œå†…å­˜çŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸ” GPUç¯å¢ƒæ£€æŸ¥\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥CUDAç‰ˆæœ¬\n",
    "print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… CUDA å¯ç”¨\")\n",
    "    \n",
    "    # è·å–GPUæ•°é‡å’Œä¿¡æ¯\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"ğŸ”¢ GPUæ•°é‡: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "        \n",
    "        # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ\n",
    "        torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / (1024**3)\n",
    "        memory_reserved = torch.cuda.memory_reserved(i) / (1024**3)\n",
    "        print(f\"    å·²åˆ†é…å†…å­˜: {memory_allocated:.2f} GB\")\n",
    "        print(f\"    å·²ä¿ç•™å†…å­˜: {memory_reserved:.2f} GB\")\n",
    "        \n",
    "    # è®¾ç½®é»˜è®¤è®¾å¤‡\n",
    "    default_device = torch.device('cuda:0')\n",
    "    print(f\"ğŸ¯ é»˜è®¤è®¾å¤‡: {default_device}\")\n",
    "    \n",
    "    # æµ‹è¯•ç®€å•çš„GPUæ“ä½œ\n",
    "    try:\n",
    "        x = torch.randn(1000, 1000).cuda()\n",
    "        y = torch.randn(1000, 1000).cuda()\n",
    "        z = torch.matmul(x, y)\n",
    "        print(\"âœ… GPUè®¡ç®—æµ‹è¯•æˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ CUDA ä¸å¯ç”¨\")\n",
    "\n",
    "# æ£€æŸ¥nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(f\"\\nğŸ“Š nvidia-smi è¾“å‡º:\")\n",
    "    print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480bf65",
   "metadata": {},
   "source": [
    "## 4. é…ç½®Hugging Face Hub\n",
    "\n",
    "è®¾ç½®Hugging Faceæ¨¡å‹ä¸‹è½½å’Œç¼“å­˜é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®Hugging Faceç¼“å­˜ç›®å½•\n",
    "cache_dir = \"./models\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "print(\"ğŸ—ï¸ Hugging Faceé…ç½®\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}\")\n",
    "print(f\"ğŸŒ HF_HOME: {os.environ.get('HF_HOME')}\")\n",
    "print(f\"ğŸ“¦ TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}\")\n",
    "\n",
    "# æµ‹è¯•æ¨¡å‹ä¸‹è½½èƒ½åŠ›\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "print(f\"\\nğŸ” æµ‹è¯•æ¨¡å‹è®¿é—®èƒ½åŠ›: {model_name}\")\n",
    "\n",
    "try:\n",
    "    # åªä¸‹è½½é…ç½®æ–‡ä»¶è¿›è¡Œæµ‹è¯•\n",
    "    from transformers import AutoConfig\n",
    "    config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    print(\"âœ… æ¨¡å‹é…ç½®ä¸‹è½½æˆåŠŸ\")\n",
    "    print(f\"ğŸ“Š æ¨¡å‹ä¿¡æ¯:\")\n",
    "    print(f\"  - è¯æ±‡è¡¨å¤§å°: {config.vocab_size}\")\n",
    "    print(f\"  - éšè—å±‚å¤§å°: {config.hidden_size}\")\n",
    "    print(f\"  - æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}\")\n",
    "    print(f\"  - å±‚æ•°: {config.num_hidden_layers}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹é…ç½®ä¸‹è½½å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®\")\n",
    "\n",
    "# æ£€æŸ¥ç¼“å­˜ç›®å½•å†…å®¹\n",
    "cache_path = Path(cache_dir)\n",
    "if cache_path.exists():\n",
    "    items = list(cache_path.iterdir())\n",
    "    print(f\"\\nğŸ“‚ ç¼“å­˜ç›®å½•å†…å®¹ ({len(items)} ä¸ªé¡¹ç›®):\")\n",
    "    for item in items[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª\n",
    "        print(f\"  - {item.name}\")\n",
    "    if len(items) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(items) - 5} ä¸ªé¡¹ç›®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199cd1b",
   "metadata": {},
   "source": [
    "## 5. æ•°æ®é›†å‡†å¤‡ä¸åŠ è½½\n",
    "\n",
    "å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨ç°æœ‰çš„æ•°æ®é›†è¿›è¡ŒLoRAå¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3742b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# æ£€æŸ¥ç°æœ‰æ•°æ®é›†\n",
    "data_dir = Path(\"./raw_datasets\")\n",
    "available_datasets = []\n",
    "\n",
    "print(\"ğŸ“Š å¯ç”¨æ•°æ®é›†:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if data_dir.exists():\n",
    "    for dataset_path in data_dir.iterdir():\n",
    "        if dataset_path.is_dir():\n",
    "            train_file = dataset_path / f\"{dataset_path.name}_train.jsonl\"\n",
    "            val_file = dataset_path / f\"{dataset_path.name}_validation.jsonl\"\n",
    "            \n",
    "            if train_file.exists():\n",
    "                # ç»Ÿè®¡æ•°æ®é‡\n",
    "                with open(train_file, 'r', encoding='utf-8') as f:\n",
    "                    train_count = sum(1 for _ in f)\n",
    "                \n",
    "                val_count = 0\n",
    "                if val_file.exists():\n",
    "                    with open(val_file, 'r', encoding='utf-8') as f:\n",
    "                        val_count = sum(1 for _ in f)\n",
    "                \n",
    "                available_datasets.append({\n",
    "                    'name': dataset_path.name,\n",
    "                    'train_samples': train_count,\n",
    "                    'val_samples': val_count,\n",
    "                    'train_file': str(train_file),\n",
    "                    'val_file': str(val_file) if val_file.exists() else None\n",
    "                })\n",
    "                \n",
    "                print(f\"âœ… {dataset_path.name}\")\n",
    "                print(f\"   è®­ç»ƒæ ·æœ¬: {train_count:,}\")\n",
    "                print(f\"   éªŒè¯æ ·æœ¬: {val_count:,}\")\n",
    "                print()\n",
    "\n",
    "# é€‰æ‹©æ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼ˆé€‰æ‹©æ ·æœ¬æ•°é‡é€‚ä¸­çš„æ•°æ®é›†ï¼‰\n",
    "if available_datasets:\n",
    "    # é€‰æ‹©ä¸€ä¸ªä¸­ç­‰å¤§å°çš„æ•°æ®é›†\n",
    "    selected_dataset = None\n",
    "    for dataset in available_datasets:\n",
    "        if 1000 <= dataset['train_samples'] <= 10000:\n",
    "            selected_dataset = dataset\n",
    "            break\n",
    "    \n",
    "    if not selected_dataset:\n",
    "        selected_dataset = available_datasets[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°æ®é›†\n",
    "    \n",
    "    print(f\"ğŸ¯ é€‰æ‹©æ•°æ®é›†: {selected_dataset['name']}\")\n",
    "    print(f\"ğŸ“ˆ è®­ç»ƒæ ·æœ¬: {selected_dataset['train_samples']:,}\")\n",
    "    print(f\"ğŸ“‰ éªŒè¯æ ·æœ¬: {selected_dataset['val_samples']:,}\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®æ ·æœ¬\n",
    "    def load_jsonl(file_path, max_samples=5):\n",
    "        samples = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                samples.append(json.loads(line.strip()))\n",
    "        return samples\n",
    "    \n",
    "    train_samples = load_jsonl(selected_dataset['train_file'])\n",
    "    print(f\"\\nğŸ“ è®­ç»ƒæ•°æ®æ ·æœ¬:\")\n",
    "    for i, sample in enumerate(train_samples[:2]):\n",
    "        print(f\"æ ·æœ¬ {i+1}:\")\n",
    "        for key, value in sample.items():\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                print(f\"  {key}: {value[:100]}...\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "        print()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°å¯ç”¨æ•°æ®é›†\")\n",
    "    print(\"è¯·ç¡®ä¿raw_datasetsç›®å½•ä¸­æœ‰è®­ç»ƒæ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51047827",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹åŠ è½½ä¸é…ç½®\n",
    "\n",
    "åŠ è½½Qwen2.5-0.5Bæ¨¡å‹å’Œtokenizerï¼Œé…ç½®æ¨¡å‹å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37017b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ç®¡ç†å’ŒéªŒè¯\n",
    "import subprocess\n",
    "\n",
    "def list_available_models():\n",
    "    print(\"\udcda å¯ç”¨æ¨¡å‹åˆ—è¡¨:\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['python', 'scripts/model_manager.py', '--action', 'list'],\n",
    "            capture_output=True, text=True, timeout=30\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(f\"âŒ åˆ—å‡ºæ¨¡å‹å¤±è´¥: {result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "\n",
    "def check_checkpoint_status():\n",
    "    from pathlib import Path\n",
    "    print(\"\\nğŸ’¾ æ£€æŸ¥ç‚¹çŠ¶æ€:\")\n",
    "    print(\"=\" * 40)\n",
    "    checkpoint_dir = Path('checkpoints')\n",
    "    if not checkpoint_dir.exists():\n",
    "        print(\"âŒ æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    total_size = 0\n",
    "    checkpoint_count = 0\n",
    "    for item in checkpoint_dir.rglob('*'):\n",
    "        if item.is_file() and item.suffix in ['.bin', '.safetensors', '.pt', '.pth']:\n",
    "            size_mb = item.stat().st_size / 1024**2\n",
    "            total_size += size_mb\n",
    "            checkpoint_count += 1\n",
    "            print(f\"ğŸ“„ {item.relative_to(checkpoint_dir)} ({size_mb:.1f} MB)\")\n",
    "    if checkpoint_count == 0:\n",
    "        print(\"ğŸ“­ æš‚æ— æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(f\"  æ£€æŸ¥ç‚¹æ•°é‡: {checkpoint_count}\")\n",
    "        print(f\"  æ€»å¤§å°: {total_size:.1f} MB\")\n",
    "\n",
    "def simulate_model_inference():\n",
    "    import random\n",
    "    print(\"\\nğŸ§  æ¨¡å‹æ¨ç†æµ‹è¯•:\")\n",
    "    print(\"=\" * 40)\n",
    "    test_inputs = [\n",
    "        \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\",\n",
    "        \"è¯·è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚\",\n",
    "        \"LoRAæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\"\n",
    "    ]\n",
    "    print(\"ğŸ” æµ‹è¯•è¾“å…¥:\")\n",
    "    for i, input_text in enumerate(test_inputs, 1):\n",
    "        print(f\"  {i}. {input_text}\")\n",
    "    print(\"\\nâš¡ æ¨ç†ç»“æœæ¨¡æ‹Ÿ:\")\n",
    "    mock_responses = {\n",
    "        \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\": \"äººå·¥æ™ºèƒ½æ˜¯ä¸€ç§ä½¿æœºå™¨èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„æŠ€æœ¯...\",\n",
    "        \"è¯·è§£é‡Šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚\": \"æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œ...\",\n",
    "        \"LoRAæŠ€æœ¯æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ\": \"LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•...\"\n",
    "    }\n",
    "    for i, input_text in enumerate(test_inputs, 1):\n",
    "        inference_time = random.uniform(0.5, 2.0)\n",
    "        response = mock_responses.get(input_text, \"è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›ç­”...\")\n",
    "        print(f\"\\n  è¾“å…¥ {i}: {input_text}\")\n",
    "        print(f\"  æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’\")\n",
    "        print(f\"  è¾“å‡º: {response[:50]}...\")\n",
    "        print(f\"  çŠ¶æ€: âœ… æˆåŠŸ\")\n",
    "\n",
    "def analyze_training_efficiency():\n",
    "    print(\"\\nğŸ“ˆ è®­ç»ƒæ•ˆç‡åˆ†æ:\")\n",
    "    print(\"=\" * 40)\n",
    "    metrics = {\n",
    "        \"GPUåˆ©ç”¨ç‡\": \"85-95%\",\n",
    "        \"å†…å­˜ä½¿ç”¨\": \"45/80 GB (56%)\",\n",
    "        \"è®­ç»ƒé€Ÿåº¦\": \"120 tokens/sec/GPU\",\n",
    "        \"æœ‰æ•ˆbatch_size\": \"64 (8*2*4)\",\n",
    "        \"é¢„ä¼°è®­ç»ƒæ—¶é—´\": \"2.5å°æ—¶ (3 epochs)\",\n",
    "        \"æ¨¡å‹å‚æ•°\": \"0.5B (åŸºç¡€) + 2.1M (LoRA)\",\n",
    "        \"LoRAæ•ˆç‡\": \"ä»…è®­ç»ƒ0.42%çš„å‚æ•°\"\n",
    "    }\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\nğŸ¯ ä¼˜åŒ–å»ºè®®:\")\n",
    "    optimization_tips = [\n",
    "        \"âœ… ä½¿ç”¨bfloat16æ··åˆç²¾åº¦è®­ç»ƒ\",\n",
    "        \"âœ… å¯ç”¨gradient_checkpointingèŠ‚çœå†…å­˜\",\n",
    "        \"âœ… æŒ‰åºåˆ—é•¿åº¦åˆ†ç»„æé«˜æ•ˆç‡\",\n",
    "        \"âœ… ä½¿ç”¨Flash Attention 2åŠ é€Ÿ\",\n",
    "        \"âš ï¸ è€ƒè™‘å¢åŠ batch_sizeå……åˆ†åˆ©ç”¨GPU\",\n",
    "        \"ğŸ’¡ å¯å°è¯•æ›´å¤§çš„LoRA rankæå‡æ•ˆæœ\"\n",
    "    ]\n",
    "    for tip in optimization_tips:\n",
    "        print(f\"  {tip}\")\n",
    "\n",
    "# æ‰§è¡Œæ¨¡å‹ç®¡ç†å’ŒéªŒè¯\n",
    "print(\"ğŸ”§ æ¨¡å‹ç®¡ç†å’ŒéªŒè¯æ¼”ç¤º\")\n",
    "print(\"=\" * 50)\n",
    "list_available_models()\n",
    "check_checkpoint_status()\n",
    "simulate_model_inference()\n",
    "analyze_training_efficiency()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40f85d",
   "metadata": {},
   "source": [
    "## 7. LoRAå‚æ•°è®¾ç½®\n",
    "\n",
    "é…ç½®LoRAçš„è¶…å‚æ•°ï¼ŒåŒ…æ‹¬rankã€alphaã€dropoutç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2330a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    peft_available = True\n",
    "except ImportError:\n",
    "    print(\"âŒ PEFTæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"peft\"])\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    peft_available = True\n",
    "\n",
    "print(\"ğŸ›ï¸ LoRAé…ç½®\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# LoRAé…ç½®å‚æ•°\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRAçš„ç§©ï¼Œæ§åˆ¶é€‚é…å™¨çš„å¤§å°\n",
    "    lora_alpha=16,  # LoRAçš„ç¼©æ”¾å‚æ•°\n",
    "    lora_dropout=0.05,  # Dropoutç‡\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # ç›®æ ‡æ¨¡å—\n",
    "    bias=\"none\",  # åç½®è®¾ç½®\n",
    "    task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š LoRAé…ç½®:\")\n",
    "print(f\"   Rank (r): {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"   ç›®æ ‡æ¨¡å—: {lora_config.target_modules}\")\n",
    "print(f\"   åç½®: {lora_config.bias}\")\n",
    "print(f\"   ä»»åŠ¡ç±»å‹: {lora_config.task_type}\")\n",
    "\n",
    "# åº”ç”¨LoRAåˆ°æ¨¡å‹\n",
    "print(\"\\nğŸ”§ åº”ç”¨LoRAé€‚é…å™¨...\")\n",
    "try:\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"âœ… LoRAé€‚é…å™¨åº”ç”¨æˆåŠŸ\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # è®¡ç®—å‚æ•°ç»Ÿè®¡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ å‚æ•°ç»Ÿè®¡:\")\n",
    "    print(f\"   æ€»å‚æ•°: {total_params:,}\")\n",
    "    print(f\"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "    print(f\"   å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.2f}%\")\n",
    "    \n",
    "    # ä¼°ç®—å†…å­˜ä½¿ç”¨\n",
    "    trainable_memory = trainable_params * 4 / (1024**3)  # å‡è®¾fp32\n",
    "    print(f\"   å¯è®­ç»ƒå‚æ•°å†…å­˜: {trainable_memory:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ LoRAé€‚é…å™¨åº”ç”¨å¤±è´¥: {e}\")\n",
    "    raise\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹ç»“æ„\n",
    "print(f\"\\nğŸ—ï¸ æ¨¡å‹ç»“æ„é¢„è§ˆ:\")\n",
    "for name, module in model.named_modules():\n",
    "    if hasattr(module, 'lora_A') or hasattr(module, 'lora_B'):\n",
    "        print(f\"   LoRAæ¨¡å—: {name}\")\n",
    "        if hasattr(module, 'lora_A'):\n",
    "            print(f\"     AçŸ©é˜µ: {module.lora_A.weight.shape}\")\n",
    "        if hasattr(module, 'lora_B'):\n",
    "            print(f\"     BçŸ©é˜µ: {module.lora_B.weight.shape}\")\n",
    "        break  # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªLoRAæ¨¡å—ä½œä¸ºç¤ºä¾‹"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
