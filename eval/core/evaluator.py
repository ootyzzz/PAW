"""
Lightningè¯„ä¼°æ¨¡å—
åŒ…å«LightningModelEvaluatorç±»å’Œç›¸å…³è¯„ä¼°é€»è¾‘
"""

from .config import *
import psutil
import gc


# å…¨å±€æ¨¡å‹ç¼“å­˜ï¼Œé˜²æ­¢é‡å¤åŠ è½½
_MODEL_CACHE = {}
_LOADING_LOCK = {}  # åŠ è½½é”ï¼Œé˜²æ­¢å¹¶å‘åŠ è½½


def log_memory_usage(stage="", verbose=False):
    """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ - ç²¾ç®€ç‰ˆ"""
    if not verbose:
        return
    try:
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            ram_usage = psutil.virtual_memory()
            print(f"ğŸ’¾ [{stage}] GPU: {gpu_allocated:.1f}GB/{gpu_total:.1f}GB | RAM: {ram_usage.percent:.1f}%")
    except Exception:
        pass


def detailed_exception_handler(func):
    """è¯¦ç»†å¼‚å¸¸å¤„ç†è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            print(f"âŒ å‡½æ•° {func.__name__} å‘ç”Ÿå¼‚å¸¸:")
            print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"âŒ å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            print(f"âŒ è¯¦ç»†traceback:")
            traceback.print_exc()
            print(f"âŒ ç³»ç»Ÿä¿¡æ¯:")
            log_memory_usage("å¼‚å¸¸å‘ç”Ÿæ—¶")
            raise
    return wrapper


def analyze_lora_adapter(model_path):
    """åˆ†æLoRAé€‚é…å™¨ç»“æ„å¹¶ç”Ÿæˆä¿¡æ¯å¡ç‰‡"""
    try:
        from peft import PeftConfig
        import json
        
        # è¯»å–adapteré…ç½®
        config_path = Path(model_path) / "adapter_config.json"
        if not config_path.exists():
            return None
            
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        # è¯»å–adapteræ¨¡å‹æ–‡ä»¶è·å–æƒé‡ä¿¡æ¯
        adapter_model_path = Path(model_path) / "adapter_model.safetensors"
        if adapter_model_path.exists():
            try:
                from safetensors import safe_open
                with safe_open(adapter_model_path, framework="pt") as f:
                    keys = list(f.keys())
            except:
                keys = []
        else:
            keys = []
        
        # åˆ†ææƒé‡ç»“æ„
        component_stats = {
            'q_proj': {'layers': set()},
            'k_proj': {'layers': set()},
            'v_proj': {'layers': set()},
            'o_proj': {'layers': set()},
            'gate_proj': {'layers': set()},
            'up_proj': {'layers': set()},
            'down_proj': {'layers': set()}
        }
        
        # ç»Ÿè®¡å®é™…å­˜åœ¨çš„æƒé‡
        for key in keys:
            for comp in component_stats.keys():
                if comp in key:
                    import re
                    layer_match = re.search(r'layers\.(\d+)\.', key)
                    if layer_match:
                        layer_num = int(layer_match.group(1))
                        component_stats[comp]['layers'].add(layer_num)
        
        target_modules = config.get('target_modules', [])
        
        # ç”Ÿæˆå¡ç‰‡
        print("\n" + "=" * 60)
        print("ğŸ¯ LoRA é€‚é…å™¨ä¿¡æ¯å¡ç‰‡")
        print("=" * 60)
        print(f"ğŸ“ è·¯å¾„: {Path(model_path).name}")
        print(f"ğŸ”§ LoRA rank: {config.get('r', 'N/A')}")
        print(f"ğŸ”§ LoRA alpha: {config.get('lora_alpha', 'N/A')}")
        print(f"ğŸ”§ ç›®æ ‡æ¨¡å—: {', '.join(target_modules)}")
        print(f"ğŸ”§ åŸºç¡€æ¨¡å‹: {config.get('base_model_name_or_path', 'N/A')}")
        
        print("\nğŸ“Š LoRAç»„ä»¶åˆ†å¸ƒ:")
        for comp, stats in component_stats.items():
            if comp in target_modules and stats['layers']:
                layers = sorted(list(stats['layers']))
                layer_count = len(layers)
                if layer_count > 0:
                    layer_range = f"{min(layers)}-{max(layers)} å…± {layer_count} å±‚" if len(layers) > 1 else f"ç¬¬{layers[0]}å±‚"
                    print(f"  {comp:>10}: {layer_range}")
        
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"âš ï¸ LoRAä¿¡æ¯åˆ†æå¤±è´¥: {e}")
        return False


class LightningModelEvaluator(pl.LightningModule):
    """Lightningæ¨¡å‹è¯„ä¼°æ¨¡å—"""
    
    def __init__(self, model_path: str, base_model_path: str = None, max_length: int = 512):
        super().__init__()
        self.save_hyperparameters()
        
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.max_length = max_length
        
        # åˆ›å»ºæ¨¡å‹åç§°ç”¨äºæŠ¥å‘Š
        self.model_name = Path(model_path).name
        
        # æ¨¡å‹åŠ è½½çŠ¶æ€æ ‡å¿—
        self._model_loaded = False
        self.model = None
        self.tokenizer = None
        
        # åˆ›å»ºç¼“å­˜é”®
        self.cache_key = f"{self.model_path}_{self.base_model_path or 'none'}"
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()
    
    def setup(self, stage=None):
        """Lightningç”Ÿå‘½å‘¨æœŸæ–¹æ³• - ç¡®ä¿æ¨¡å‹å·²åŠ è½½"""
        if not self._model_loaded:
            print(f"ğŸ”„ Lightning setupé˜¶æ®µé‡æ–°åŠ è½½æ¨¡å‹...")
            self._load_model()
    
    def on_test_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„é’©å­"""
        if not self._model_loaded:
            print(f"ğŸ”„ æµ‹è¯•å¼€å§‹æ—¶é‡æ–°åŠ è½½æ¨¡å‹...")
            self._load_model()
        
        # åˆå§‹åŒ–ç´¯ç§¯ç»Ÿè®¡å˜é‡
        self._total_correct = 0
        self._total_samples = 0
        print(f"ğŸš€ å¼€å§‹æµ‹è¯•è¯„ä¼°ï¼Œå°†æ˜¾ç¤ºå®æ—¶ç´¯ç§¯å‡†ç¡®ç‡...")

    def on_test_end(self):
        """æµ‹è¯•ç»“æŸæ—¶çš„é’©å­"""
        if hasattr(self, '_total_correct') and hasattr(self, '_total_samples'):
            if self._total_samples > 0:
                final_accuracy = self._total_correct / self._total_samples
                print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
                print(f"ğŸ“Š æœ€ç»ˆç´¯ç§¯å‡†ç¡®ç‡: {final_accuracy:.4f} ({self._total_correct}/{self._total_samples})")
                print(f"ğŸ“Š æ­£ç¡®æ ·æœ¬æ•°: {self._total_correct}")
                print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {self._total_samples}")
            else:
                print(f"\nâš ï¸ æµ‹è¯•å®Œæˆï¼Œä½†æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
        
    @detailed_exception_handler
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        # åˆ›å»ºç¼“å­˜é”®
        cache_key = f"{self.model_path}_{self.base_model_path or 'none'}"
        
        # æ£€æŸ¥å…¨å±€ç¼“å­˜
        if cache_key in _MODEL_CACHE:
            print(f"âœ… ä»ç¼“å­˜åŠ è½½æ¨¡å‹: {Path(self.model_path).name}")
            cached_data = _MODEL_CACHE[cache_key]
            self.model = cached_data['model']
            self.tokenizer = cached_data['tokenizer']
            self._model_loaded = True
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½è¿‡æ¨¡å‹
        if self._model_loaded and self.model is not None and self.tokenizer is not None:
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {Path(self.model_path).name}")
            return
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {Path(self.model_path).name}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        is_local_path = os.path.exists(self.model_path)
        if not is_local_path:
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        try:
            # æ¨¡å‹åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "trust_remote_code": True,
                "use_cache": True,
                "device_map": None,  # ç¦ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼Œè®©Lightningæ§åˆ¶
            }
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            config_path = Path(self.model_path) / "adapter_config.json"
            
            if config_path.exists():
                # æ˜¾ç¤ºLoRAä¿¡æ¯å¡ç‰‡
                analyze_lora_adapter(self.model_path)
                
                # LoRAæ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ”§ LoRAæ¨¡å‹åŠ è½½ä¸­...")
                try:
                    # åŠ è½½PEFTé…ç½®è·å–åŸºç¡€æ¨¡å‹ä¿¡æ¯
                    peft_config = PeftConfig.from_pretrained(self.model_path)
                    detected_base_model = peft_config.base_model_name_or_path
                    
                    # ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–æ£€æµ‹åˆ°çš„è·¯å¾„
                    actual_base_model = self.base_model_path or detected_base_model
                    
                    # ç¡®è®¤åŸºç¡€æ¨¡å‹è·¯å¾„
                    if not os.path.exists(actual_base_model) and "/" not in actual_base_model:
                        for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                            test_path = f"{prefix}{actual_base_model}"
                            if os.path.exists(test_path):
                                actual_base_model = test_path
                                break
                    
                    # åŠ è½½tokenizer
                    tokenizer_kwargs = {"trust_remote_code": True}
                    self.tokenizer = AutoTokenizer.from_pretrained(actual_base_model, **tokenizer_kwargs)
                    
                    # ç‰¹æ®Šå¤„ç†Gemmaæ¨¡å‹
                    if "gemma" in actual_base_model.lower():
                        load_kwargs.update({
                            "attn_implementation": "eager",
                            "use_cache": False,
                            "_attn_implementation_internal": "eager"
                        })
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹
                    base_model = AutoModelForCausalLM.from_pretrained(actual_base_model, **load_kwargs)
                    
                    # è¿‡æ»¤PEFTè­¦å‘Šå¹¶åŠ è½½PEFTæ¨¡å‹
                    import warnings
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore", message="Found missing adapter keys")
                        self.model = PeftModel.from_pretrained(base_model, self.model_path)
                    print(f"âœ… LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
                    
                except Exception as e:
                    raise RuntimeError(f"æ— æ³•åŠ è½½LoRAæ¨¡å‹: {self.model_path}ï¼Œé”™è¯¯: {e}")
            else:
                # å¸¸è§„æ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ“¦ å¸¸è§„æ¨¡å‹åŠ è½½ä¸­...")
                
                # åŠ è½½tokenizer
                tokenizer_kwargs = {"trust_remote_code": True}
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                except Exception as e:
                    tokenizer_kwargs["use_fast"] = False
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                
                # é’ˆå¯¹ç‰¹æ®Šæ¨¡å‹çš„å¤„ç†
                model_name_lower = self.model_path.lower()
                if "gemma" in model_name_lower:
                    load_kwargs.update({
                        "attn_implementation": "eager",
                        "use_cache": False,
                        "_attn_implementation_internal": "eager"
                    })
                elif "llama" in model_name_lower:
                    load_kwargs.update({"use_cache": True})
                
                # åŠ è½½æ¨¡å‹
                self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **load_kwargs)
                print("âœ… å¸¸è§„æ¨¡å‹åŠ è½½å®Œæˆ")
        
            # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªGPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                device = torch.device("cuda:0")
                self.model = self.model.to(device)
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è®¾ç½®åŠ è½½å®Œæˆæ ‡å¿—
            self._model_loaded = True
            
            # ä¿å­˜åˆ°å…¨å±€ç¼“å­˜
            _MODEL_CACHE[cache_key] = {
                'model': self.model,
                'tokenizer': self.tokenizer
            }
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {self.model_path}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"âŒ è¯¦ç»†é”™è¯¯:")
            traceback.print_exc()
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {self.model_path}: {str(e)}")

    def test_step(self, batch, batch_idx):
        """å•ä¸ªæµ‹è¯•æ­¥éª¤"""
        try:
            # è®¡ç®—æŒ‡æ ‡
            loss = self._compute_loss(batch)
            accuracy = self._compute_accuracy(batch)
            perplexity = torch.exp(loss)
            batch_size = len(batch) if batch else 0
            
            # ç´¯ç§¯å‡†ç¡®ç‡è®¡ç®—
            if not hasattr(self, '_total_correct'):
                self._total_correct = 0
                self._total_samples = 0
            
            # æ›´æ–°ç´¯ç§¯ç»Ÿè®¡
            if batch_size > 0:
                correct_count = int(accuracy.item() * batch_size)
                self._total_correct += correct_count
                self._total_samples += batch_size
                
                # è®¡ç®—ç´¯ç§¯å‡†ç¡®ç‡
                cumulative_accuracy = self._total_correct / self._total_samples if self._total_samples > 0 else 0.0
                
                # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦å’Œç´¯ç§¯å‡†ç¡®ç‡
                if batch_idx % 50 == 0:
                    sample_info = ""
                    if batch and len(batch) > 0:
                        sample = batch[0]
                        if 'input' in sample:
                            sample_info = f" | æ ·æœ¬: {sample['input'][:50]}..."
                    print(f"ğŸ“Š Step {batch_idx:4d} | ç´¯ç§¯å‡†ç¡®ç‡: {cumulative_accuracy:.4f} ({self._total_correct}/{self._total_samples}){sample_info}")
                    log_memory_usage(f"step_{batch_idx}", verbose=False)
            
            # éªŒè¯batchå†…å®¹
            if not batch:
                return {
                    'loss': torch.tensor(0.0),
                    'accuracy': torch.tensor(0.0),
                    'perplexity': torch.tensor(1.0),
                    'batch_size': 0
                }
            
            # è®°å½•æŒ‡æ ‡
            self.log('test/loss', loss, batch_size=batch_size)
            self.log('test/accuracy', accuracy, batch_size=batch_size)
            self.log('test/perplexity', perplexity, batch_size=batch_size)
            
            return {
                'loss': loss,
                'accuracy': accuracy,
                'perplexity': perplexity,
                'batch_size': batch_size
            }
        except Exception as e:
            print(f"âŒ Step {batch_idx} å¤±è´¥: {type(e).__name__}: {str(e)}")
            if batch_idx % 100 == 0:  # åªåœ¨å…³é”®æ­¥éª¤æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                traceback.print_exc()
            
            return {
                'loss': torch.tensor(float('inf')),
                'accuracy': torch.tensor(0.0),
                'perplexity': torch.tensor(float('inf')),
                'batch_size': len(batch) if batch else 1
            }
        
    def _compute_loss(self, batch):
        """è®¡ç®—æŸå¤±"""
        try:
            inputs = []
            labels = []
            
            for item in batch:
                # å¤„ç†å¤šé€‰é¢˜æ ¼å¼
                if 'input' in item and 'options' in item:
                    question = item['input']
                    options = item['options']
                    target = item.get('target', 'A')
                    
                    # æ ¼å¼åŒ–é—®é¢˜ã€é€‰é¡¹å’Œç­”æ¡ˆ
                    text = f"Question: {question}\n"
                    for option in options:
                        text += f"{option}\n"
                    text += f"Answer: {target}"
                else:
                    # å¤‡é€‰ï¼šä½¿ç”¨ä»»ä½•æ–‡æœ¬å­—æ®µ
                    text = item.get('text', str(item))
                
                # Tokenize
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                inputs.append(encoding['input_ids'].squeeze())
                labels.append(encoding['input_ids'].squeeze())

            if inputs:
                # ç¡®ä¿æ‰€æœ‰tensoréƒ½åœ¨æ¨¡å‹çš„ä¸»è®¾å¤‡ä¸Š
                model_device = next(self.model.parameters()).device
                input_ids = torch.stack(inputs).to(model_device)
                attention_mask = torch.ones_like(input_ids).to(model_device)
                labels = torch.stack(labels).to(model_device)
            else:
                return torch.tensor(0.0)
            
            # è®¡ç®—æŸå¤±
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            return outputs.loss
            
        except Exception as e:
            print(f"âŒ _compute_losså¤±è´¥: {e}")
            print(f"âŒ batchå¤§å°: {len(batch) if batch else 'None'}")
            if batch:
                print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬: {batch[0] if len(batch) > 0 else 'Empty'}")
            traceback.print_exc()
            return torch.tensor(float('inf'))

    def _compute_accuracy(self, batch):
        """è®¡ç®—å‡†ç¡®ç‡ - æ”¯æŒæ‰€æœ‰7ä¸ªæ•°æ®é›†æ ¼å¼"""
        if not isinstance(batch, list):
            return torch.tensor(0.5)
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for item in batch:
                try:
                    # è§£ææ•°æ®é¡¹
                    question = item.get('input', '')
                    options = item.get('options', [])
                    correct_answer = item.get('target', '')
                    correct_idx = item.get('target_idx', 0)
                    dataset = item.get('dataset', '')
                    
                    if not options:
                        total += 1
                        continue
                    
                    # æ ¹æ®æ•°æ®é›†ç±»å‹æ ¼å¼åŒ–æç¤ºè¯
                    if dataset in ['arc-challenge', 'arc-easy', 'openbookqa']:
                        # ARCå’ŒOpenBookQAï¼š4é€‰é¡¹ï¼Œä½¿ç”¨A/B/C/Dæ ¼å¼
                        prompt = f"Question: {question}\n"
                        option_labels = ['A', 'B', 'C', 'D']
                        for i, option in enumerate(options):
                            if i < len(option_labels):
                                prompt += f"{option_labels[i]}. {option}\n"
                        prompt += "Answer:"
                        valid_answers = option_labels[:len(options)]
                        
                    elif dataset == 'boolq':
                        # BoolQï¼šTrue/Falseæ ¼å¼ï¼Œæ˜ å°„åˆ°A/B
                        prompt = f"Question: {question}\n"
                        prompt += "A. False\n"
                        prompt += "B. True\n"
                        prompt += "Answer:"
                        valid_answers = ['A', 'B']  # A=False, B=True
                        
                    elif dataset == 'hellaswag':
                        # HellaSwagï¼š4é€‰é¡¹ï¼Œä½¿ç”¨A/B/C/Dæ ¼å¼
                        prompt = f"Question: {question}\n"
                        option_labels = ['A', 'B', 'C', 'D']
                        for i, option in enumerate(options):
                            if i < len(option_labels):
                                prompt += f"{option_labels[i]}. {option}\n"
                        prompt += "Answer:"
                        valid_answers = option_labels[:len(options)]
                        
                    elif dataset in ['piqa', 'winogrande']:
                        # PIQAå’ŒWinoGrandeï¼š2é€‰é¡¹ï¼Œä½¿ç”¨A/Bæ ¼å¼
                        prompt = f"Question: {question}\n"
                        for i, option in enumerate(options[:2]):
                            prompt += f"{'A' if i == 0 else 'B'}. {option}\n"
                        prompt += "Answer:"
                        valid_answers = ['A', 'B']
                        
                    else:
                        # é»˜è®¤æ ¼å¼ï¼šä½¿ç”¨A/Bæ ¼å¼
                        prompt = f"Question: {question}\n"
                        option_labels = ['A', 'B', 'C', 'D']
                        for i, option in enumerate(options):
                            if i < len(option_labels):
                                prompt += f"{option_labels[i]}. {option}\n"
                        prompt += "Answer:"
                        valid_answers = option_labels[:len(options)]
                    
                    # Tokenize
                    model_device = next(self.model.parameters()).device
                    inputs = self.tokenizer(
                        prompt,
                        return_tensors='pt',
                        truncation=True,
                        max_length=self.max_length,
                        padding=True
                    )
                    inputs = {k: v.to(model_device) for k, v in inputs.items()}
                    
                    # ç”Ÿæˆå‚æ•° - åŠ é€Ÿç‰ˆæœ¬ï¼šåªç”Ÿæˆ1ä¸ªtoken
                    generation_kwargs = {
                        "max_new_tokens": 1,  # åªç”Ÿæˆ1ä¸ªtokenï¼Œå¤§å¹…åŠ é€Ÿ
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False,
                        "output_attentions": False,
                        "output_hidden_states": False,
                    }
                    
                    if "gemma" in self.model_path.lower():
                        generation_kwargs.update({
                            "temperature": 1.0,
                            "top_p": 1.0,
                            "repetition_penalty": 1.0,
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    outputs = self.model.generate(**inputs, **generation_kwargs)
                    
                    # è§£ç å¹¶æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬
                    generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                    # ç§»é™¤å¼€å¤´çš„å•å¼•å·å’Œç©ºæ ¼ï¼Œåªå–å‰2ä¸ªå­—ç¬¦è¿›è¡Œè§£æ
                    generated_text = generated_text.lstrip("' ").strip().upper()[:2]
                    
                    # æå–é¢„æµ‹ç­”æ¡ˆ
                    predicted_answer = None
                    for char in generated_text:
                        if char in valid_answers:
                            predicted_answer = valid_answers.index(char)
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
                    if predicted_answer is None:
                        predicted_answer = 0  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªé€‰é¡¹
                    
                    # è°ƒè¯•ä¿¡æ¯è¾“å‡º
                    is_correct = predicted_answer == correct_idx
                    # if total < 10:
                        # print(f"\nğŸ” æ ·æœ¬ {total + 1} è°ƒè¯•ä¿¡æ¯:")
                        # print(f"ğŸ“ æ•°æ®é›†: {dataset}")
                        # print(f"ğŸ“ é—®é¢˜: {question[:100]}...")
                        # print(f"ğŸ“ é€‰é¡¹æ•°é‡: {len(options)}")
                        # for i, opt in enumerate(options):
                        #     label = valid_answers[i] if i < len(valid_answers) else f"é€‰é¡¹{i+1}"
                        #     print(f"ğŸ“ {label}: {opt[:50]}...")
                        # print(f"ğŸ“ æ­£ç¡®ç­”æ¡ˆ: {correct_answer} (ç´¢å¼•: {correct_idx})")
                        # print(f"ğŸ“ ç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
                        # print(f"ğŸ“ é¢„æµ‹ç­”æ¡ˆ: {predicted_answer} ({valid_answers[predicted_answer] if predicted_answer < len(valid_answers) else 'N/A'})")
                        # print(f"ğŸ“ æ˜¯å¦æ­£ç¡®: {'âœ…' if is_correct else 'âŒ'}")
                        # print("-" * 50)
                    
                    # ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ
                    if is_correct:
                        correct += 1
                    
                    total += 1
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ ·æœ¬é”™è¯¯: {e}")
                    total += 1
                    continue
        
        if total == 0:
            return torch.tensor(0.0)
        
        accuracy = correct / total
        return torch.tensor(accuracy)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨ - è¯„ä¼°æ¨¡å¼ä¸éœ€è¦ï¼Œä½†Lightningéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return None
