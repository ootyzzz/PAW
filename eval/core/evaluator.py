"""
Lightningè¯„ä¼°æ¨¡å—
åŒ…å«LightningModelEvaluatorç±»å’Œç›¸å…³è¯„ä¼°é€»è¾‘
"""

from .config import *


class LightningModelEvaluator(pl.LightningModule):
    """Lightningæ¨¡å‹è¯„ä¼°æ¨¡å—"""
    
    def __init__(self, model_path: str, base_model_path: str = None, max_length: int = 512):
        super().__init__()
        self.save_hyperparameters()
        
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.max_length = max_length
        
        # åˆ›å»ºæ¨¡å‹åç§°ç”¨äºæŠ¥å‘Š
        self.model_name = Path(model_path).name
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯Hugging Faceæ¨¡å‹ID
        is_local_path = os.path.exists(self.model_path)
        
        print(f"ğŸ” æ¨¡å‹è·¯å¾„æ£€æŸ¥: {self.model_path}")
        print(f"ğŸ” æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„: {is_local_path}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not is_local_path:
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        try:
            # æ¨¡å‹åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "trust_remote_code": True,
                "use_cache": True,
                "device_map": "auto" if torch.cuda.is_available() else None,
            }
            
            print(f"ğŸ” æ¨¡å‹åŠ è½½å‚æ•°: {load_kwargs}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            config_path = Path(self.model_path) / "adapter_config.json"
            print(f"ğŸ” æ£€æŸ¥LoRAé…ç½®æ–‡ä»¶: {config_path} (å­˜åœ¨: {config_path.exists()})")
            
            if config_path.exists():
                # LoRAæ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ”§ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼Œä½¿ç”¨PEFTåŠ è½½...")
                try:
                    # åŠ è½½PEFTé…ç½®è·å–åŸºç¡€æ¨¡å‹ä¿¡æ¯
                    peft_config = PeftConfig.from_pretrained(self.model_path)
                    detected_base_model = peft_config.base_model_name_or_path
                    
                    # ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–æ£€æµ‹åˆ°çš„è·¯å¾„
                    actual_base_model = self.base_model_path or detected_base_model
                    
                    # ç¡®è®¤åŸºç¡€æ¨¡å‹è·¯å¾„
                    if not os.path.exists(actual_base_model) and "/" not in actual_base_model:
                        # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•autodl-tmpä¸­çš„å¸¸è§ä½ç½®
                        for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                            test_path = f"{prefix}{actual_base_model}"
                            if os.path.exists(test_path):
                                actual_base_model = test_path
                                break
                    
                    print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {actual_base_model}")
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹çš„tokenizer (ç§»é™¤local_files_onlyé™åˆ¶)
                    tokenizer_kwargs = {"trust_remote_code": True}
                    
                    self.tokenizer = AutoTokenizer.from_pretrained(actual_base_model, **tokenizer_kwargs)
                    
                    # ç‰¹æ®Šå¤„ç†Gemmaæ¨¡å‹
                    if "gemma" in actual_base_model.lower():
                        print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                        load_kwargs.update({
                            "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                            "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                            "_attn_implementation_internal": "eager"
                        })
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹
                    base_model = AutoModelForCausalLM.from_pretrained(
                        actual_base_model,
                        **load_kwargs
                    )
                    
                    print(f"ğŸ”§ åŠ è½½LoRAé€‚é…å™¨: {self.model_path}")
                    # åŠ è½½PEFTæ¨¡å‹
                    self.model = PeftModel.from_pretrained(base_model, self.model_path)
                    
                except Exception as e:
                    print(f"âŒ ä½œä¸ºPEFTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    raise RuntimeError(f"æ— æ³•åŠ è½½LoRAæ¨¡å‹: {self.model_path}ï¼ŒLoRAæ¨¡å‹å¿…é¡»ä¸æ­£ç¡®çš„åŸºç¡€æ¨¡å‹åŒ¹é…")
            else:
                # å¸¸è§„æ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ“¦ åŠ è½½ä¸ºå¸¸è§„æ¨¡å‹...")
                
                # å¤„ç†tokenizer (ç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶)
                tokenizer_kwargs = {"trust_remote_code": True}
                    
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                except Exception as e:
                    print(f"âš ï¸ æ ‡å‡†tokenizeråŠ è½½å¤±è´¥: {e}")
                    print("å°è¯•ä½¿ç”¨å¤‡ç”¨tokenizeré€‰é¡¹...")
                    tokenizer_kwargs["use_fast"] = False
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                
                # é’ˆå¯¹Gemmaæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
                model_name_lower = self.model_path.lower()
                special_kwargs = load_kwargs.copy()
                
                if "gemma" in model_name_lower:
                    print("ğŸ” æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                    special_kwargs.update({
                        "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                        "_attn_implementation_internal": "eager"
                    })
                    
                # åŠ è½½æ¨¡å‹ï¼Œç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    **special_kwargs
                )
        
            # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {self.model_path}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"âŒ è¯¦ç»†é”™è¯¯:")
            traceback.print_exc()
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {self.model_path}: {str(e)}")

    def test_step(self, batch, batch_idx):
        """å•ä¸ªæµ‹è¯•æ­¥éª¤"""
        try:
            # è®¡ç®—æŸå¤±
            loss = self._compute_loss(batch)
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = self._compute_accuracy(batch)
            perplexity = torch.exp(loss)
            
            batch_size = len(batch)
            
            # è®°å½•æŒ‡æ ‡
            self.log('test/loss', loss, batch_size=batch_size)
            self.log('test/accuracy', accuracy, batch_size=batch_size)
            self.log('test/perplexity', perplexity, batch_size=batch_size)
            
            return {
                'loss': loss,
                'accuracy': accuracy,
                'perplexity': perplexity,
                'batch_size': batch_size
            }
        except Exception as e:
            print(f"âŒ test_stepå¤±è´¥ (batch_idx={batch_idx}): {e}")
            print(f"âŒ batchå†…å®¹: {batch}")
            traceback.print_exc()
            # è¿”å›é»˜è®¤å€¼é¿å…è®­ç»ƒä¸­æ–­
            return {
                'loss': torch.tensor(float('inf')),
                'accuracy': torch.tensor(0.0),
                'perplexity': torch.tensor(float('inf')),
                'batch_size': len(batch) if batch else 1
            }
        
    def _compute_loss(self, batch):
        """è®¡ç®—æŸå¤±"""
        try:
            inputs = []
            labels = []
            
            for item in batch:
                # å¤„ç†å¤šé€‰é¢˜æ ¼å¼
                if 'input' in item and 'options' in item:
                    question = item['input']
                    options = item['options']
                    target = item.get('target', 'A')
                    
                    # æ ¼å¼åŒ–é—®é¢˜ã€é€‰é¡¹å’Œç­”æ¡ˆ
                    text = f"Question: {question}\n"
                    for option in options:
                        text += f"{option}\n"
                    text += f"Answer: {target}"
                else:
                    # å¤‡é€‰ï¼šä½¿ç”¨ä»»ä½•æ–‡æœ¬å­—æ®µ
                    text = item.get('text', str(item))
                
                # Tokenize
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                inputs.append(encoding['input_ids'].squeeze())
                labels.append(encoding['input_ids'].squeeze())

            if inputs:
                input_ids = torch.stack(inputs).to(self.device)
                attention_mask = torch.ones_like(input_ids).to(self.device)
                labels = torch.stack(labels).to(self.device)
            else:
                return torch.tensor(0.0)
            
            # è®¡ç®—æŸå¤±
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            return outputs.loss
            
        except Exception as e:
            print(f"âŒ _compute_losså¤±è´¥: {e}")
            print(f"âŒ batchå¤§å°: {len(batch) if batch else 'None'}")
            if batch:
                print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬: {batch[0] if len(batch) > 0 else 'Empty'}")
            traceback.print_exc()
            return torch.tensor(float('inf'))

    def _compute_accuracy(self, batch):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if not isinstance(batch, list):
            return torch.tensor(0.25)  # 4é€‰1é¢˜çš„éšæœºåŸºçº¿
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for item in batch:
                try:
                    # è§£ææ•°æ®é¡¹
                    question = item.get('input', '')
                    options = item.get('options', [])
                    correct_answer = item.get('target', 'A')
                    
                    if not options:
                        total += 1
                        continue
                    
                    # æ ¼å¼åŒ–å¸¦é€‰é¡¹çš„é—®é¢˜
                    prompt = f"Question: {question}\n"
                    for option in options:
                        prompt += f"{option}\n"
                    prompt += "Answer:"
                    
                    # Tokenize
                    inputs = self.tokenizer(
                        prompt,
                        return_tensors='pt',
                        truncation=True,
                        max_length=self.max_length,
                        padding=True
                    ).to(self.device)
                    
                    # Gemmaæ¨¡å‹ç‰¹æ®Šå¤„ç†
                    model_name_lower = self.model_path.lower()
                    generation_kwargs = {
                        "max_new_tokens": 3,  # å‡å°‘ç”Ÿæˆé•¿åº¦
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜
                        "output_attentions": False,
                        "output_hidden_states": False,
                    }
                    
                    if "gemma" in model_name_lower:
                        # Gemmaæ¨¡å‹ç‰¹æ®Šé€‚é…
                        generation_kwargs.update({
                            "temperature": 1.0,
                            "top_p": 1.0,
                            "repetition_penalty": 1.0,
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    outputs = self.model.generate(
                        **inputs,
                        **generation_kwargs
                    )
                    
                    # è§£ç ç”Ÿæˆçš„ç­”æ¡ˆ
                    generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                    generated_answer = generated_text.strip().upper()
                    
                    # æå–ç¬¬ä¸€ä¸ªå­—æ¯ (A, B, C, æˆ– D)
                    predicted_answer = None
                    for char in generated_answer:
                        if char in ['A', 'B', 'C', 'D']:
                            predicted_answer = char
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå°è¯•åŒ¹é…é€‰é¡¹å‰ç¼€
                    if predicted_answer is None:
                        for option in options:
                            if option.startswith('A:') and 'A' in generated_answer:
                                predicted_answer = 'A'
                            elif option.startswith('B:') and 'B' in generated_answer:
                                predicted_answer = 'B'
                            elif option.startswith('C:') and 'C' in generated_answer:
                                predicted_answer = 'C'
                            elif option.startswith('D:') and 'D' in generated_answer:
                                predicted_answer = 'D'
                            if predicted_answer:
                                break
                    
                    # ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ
                    if predicted_answer == correct_answer:
                        correct += 1
                    
                    total += 1
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ ·æœ¬é”™è¯¯: {e}")
                    total += 1
                    continue
        
        if total == 0:
            return torch.tensor(0.0)
        
        accuracy = correct / total
        return torch.tensor(accuracy)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨ - è¯„ä¼°æ¨¡å¼ä¸éœ€è¦ï¼Œä½†Lightningéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return None
