"""
Lightningè¯„ä¼°æ¨¡å—
åŒ…å«LightningModelEvaluatorç±»å’Œç›¸å…³è¯„ä¼°é€»è¾‘
"""

from .config import *
import psutil
import gc


# å…¨å±€æ¨¡å‹ç¼“å­˜ï¼Œé˜²æ­¢é‡å¤åŠ è½½
_MODEL_CACHE = {}


def log_memory_usage(stage=""):
    """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        if torch.cuda.is_available():
            gpu_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_reserved = torch.cuda.memory_reserved() / 1024**3
            gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ” [{stage}] GPUå†…å­˜: {gpu_allocated:.2f}GB / {gpu_reserved:.2f}GB / {gpu_total:.2f}GB")
        
        ram_usage = psutil.virtual_memory()
        print(f"ğŸ” [{stage}] RAMä½¿ç”¨: {ram_usage.used/1024**3:.2f}GB / {ram_usage.total/1024**3:.2f}GB ({ram_usage.percent:.1f}%)")
    except Exception as e:
        print(f"âš ï¸ å†…å­˜ç›‘æ§å¤±è´¥: {e}")


def detailed_exception_handler(func):
    """è¯¦ç»†å¼‚å¸¸å¤„ç†è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            print(f"âŒ å‡½æ•° {func.__name__} å‘ç”Ÿå¼‚å¸¸:")
            print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"âŒ å¼‚å¸¸ä¿¡æ¯: {str(e)}")
            print(f"âŒ è¯¦ç»†traceback:")
            traceback.print_exc()
            print(f"âŒ ç³»ç»Ÿä¿¡æ¯:")
            log_memory_usage("å¼‚å¸¸å‘ç”Ÿæ—¶")
            raise
    return wrapper


class LightningModelEvaluator(pl.LightningModule):
    """Lightningæ¨¡å‹è¯„ä¼°æ¨¡å—"""
    
    def __init__(self, model_path: str, base_model_path: str = None, max_length: int = 512):
        super().__init__()
        self.save_hyperparameters()
        
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.max_length = max_length
        
        # åˆ›å»ºæ¨¡å‹åç§°ç”¨äºæŠ¥å‘Š
        self.model_name = Path(model_path).name
        
        # æ¨¡å‹åŠ è½½çŠ¶æ€æ ‡å¿—
        self._model_loaded = False
        self.model = None
        self.tokenizer = None
        
        # åˆ›å»ºç¼“å­˜é”®
        self.cache_key = f"{self.model_path}_{self.base_model_path or 'none'}"
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()
    
    def setup(self, stage=None):
        """Lightningç”Ÿå‘½å‘¨æœŸæ–¹æ³• - ç¡®ä¿æ¨¡å‹å·²åŠ è½½"""
        if not self._model_loaded:
            print(f"ğŸ”„ Lightning setupé˜¶æ®µé‡æ–°åŠ è½½æ¨¡å‹...")
            self._load_model()
    
    def on_test_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„é’©å­"""
        if not self._model_loaded:
            print(f"ğŸ”„ æµ‹è¯•å¼€å§‹æ—¶é‡æ–°åŠ è½½æ¨¡å‹...")
            self._load_model()
        
    @detailed_exception_handler
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        # åˆ›å»ºç¼“å­˜é”®
        cache_key = f"{self.model_path}_{self.base_model_path or 'none'}"
        
        # æ£€æŸ¥å…¨å±€ç¼“å­˜
        if cache_key in _MODEL_CACHE:
            print(f"âœ… ä»ç¼“å­˜åŠ è½½æ¨¡å‹: {self.model_path}")
            cached_data = _MODEL_CACHE[cache_key]
            self.model = cached_data['model']
            self.tokenizer = cached_data['tokenizer']
            self._model_loaded = True
            log_memory_usage("ç¼“å­˜åŠ è½½å")
            return
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½è¿‡æ¨¡å‹
        if self._model_loaded and self.model is not None and self.tokenizer is not None:
            print(f"âœ… æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½: {self.model_path}")
            return
        
        print(f"ğŸ“¦ å¼€å§‹åŠ è½½æ¨¡å‹: {self.model_path}")
        log_memory_usage("æ¨¡å‹åŠ è½½å‰")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯Hugging Faceæ¨¡å‹ID
        is_local_path = os.path.exists(self.model_path)
        
        print(f"ğŸ” æ¨¡å‹è·¯å¾„æ£€æŸ¥: {self.model_path}")
        print(f"ğŸ” ç»å¯¹è·¯å¾„: {os.path.abspath(self.model_path)}")
        print(f"ğŸ” æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„: {is_local_path}")
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œæ£€æŸ¥ç›®å½•å†…å®¹
        if is_local_path:
            try:
                files = os.listdir(self.model_path)
                print(f"ğŸ” æ¨¡å‹ç›®å½•å†…å®¹: {files[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                
                # æ£€æŸ¥å…³é”®æ–‡ä»¶
                key_files = ['config.json', 'pytorch_model.bin', 'model.safetensors', 'tokenizer.json']
                for key_file in key_files:
                    if key_file in files:
                        print(f"âœ… æ‰¾åˆ°å…³é”®æ–‡ä»¶: {key_file}")
                    else:
                        print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ä»¶: {key_file}")
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è¯»å–æ¨¡å‹ç›®å½•: {e}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not is_local_path:
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            print(f"âŒ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            print(f"âŒ å°è¯•çš„ç»å¯¹è·¯å¾„: {os.path.abspath(self.model_path)}")
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        try:
            # æ¨¡å‹åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "trust_remote_code": True,
                "use_cache": True,
                "device_map": "auto" if torch.cuda.is_available() else None,
            }
            
            print(f"ğŸ” æ¨¡å‹åŠ è½½å‚æ•°: {load_kwargs}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            config_path = Path(self.model_path) / "adapter_config.json"
            print(f"ğŸ” æ£€æŸ¥LoRAé…ç½®æ–‡ä»¶: {config_path} (å­˜åœ¨: {config_path.exists()})")
            
            if config_path.exists():
                # LoRAæ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ”§ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼Œä½¿ç”¨PEFTåŠ è½½...")
                try:
                    print("ğŸ” æ­¥éª¤1: åŠ è½½PEFTé…ç½®...")
                    # åŠ è½½PEFTé…ç½®è·å–åŸºç¡€æ¨¡å‹ä¿¡æ¯
                    peft_config = PeftConfig.from_pretrained(self.model_path)
                    detected_base_model = peft_config.base_model_name_or_path
                    print(f"ğŸ” æ£€æµ‹åˆ°çš„åŸºç¡€æ¨¡å‹: {detected_base_model}")
                    
                    # ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–æ£€æµ‹åˆ°çš„è·¯å¾„
                    actual_base_model = self.base_model_path or detected_base_model
                    print(f"ğŸ” å®é™…ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹è·¯å¾„: {actual_base_model}")
                    
                    # ç¡®è®¤åŸºç¡€æ¨¡å‹è·¯å¾„
                    if not os.path.exists(actual_base_model) and "/" not in actual_base_model:
                        # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•autodl-tmpä¸­çš„å¸¸è§ä½ç½®
                        for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                            test_path = f"{prefix}{actual_base_model}"
                            if os.path.exists(test_path):
                                actual_base_model = test_path
                                print(f"ğŸ” æ‰¾åˆ°åŸºç¡€æ¨¡å‹: {actual_base_model}")
                                break
                    
                    print(f"ğŸ” æ­¥éª¤2: åŠ è½½tokenizer...")
                    log_memory_usage("tokenizeråŠ è½½å‰")
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹çš„tokenizer (ç§»é™¤local_files_onlyé™åˆ¶)
                    tokenizer_kwargs = {"trust_remote_code": True}
                    
                    self.tokenizer = AutoTokenizer.from_pretrained(actual_base_model, **tokenizer_kwargs)
                    print(f"âœ… tokenizeråŠ è½½æˆåŠŸ")
                    log_memory_usage("tokenizeråŠ è½½å")
                    
                    # ç‰¹æ®Šå¤„ç†Gemmaæ¨¡å‹
                    if "gemma" in actual_base_model.lower():
                        print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                        load_kwargs.update({
                            "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                            "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                            "_attn_implementation_internal": "eager"
                        })
                    
                    print(f"ğŸ” æ­¥éª¤3: åŠ è½½åŸºç¡€æ¨¡å‹...")
                    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹è·¯å¾„: {actual_base_model}")
                    print(f"ğŸ“¦ åŠ è½½å‚æ•°: {load_kwargs}")
                    log_memory_usage("åŸºç¡€æ¨¡å‹åŠ è½½å‰")
                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹
                    base_model = AutoModelForCausalLM.from_pretrained(
                        actual_base_model,
                        **load_kwargs
                    )
                    print(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
                    log_memory_usage("åŸºç¡€æ¨¡å‹åŠ è½½å")
                    
                    print(f"ğŸ” æ­¥éª¤4: åŠ è½½LoRAé€‚é…å™¨...")
                    print(f"ğŸ”§ LoRAè·¯å¾„: {self.model_path}")
                    
                    # åŠ è½½PEFTæ¨¡å‹
                    self.model = PeftModel.from_pretrained(base_model, self.model_path)
                    print(f"âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
                    log_memory_usage("LoRAåŠ è½½å")
                    
                except Exception as e:
                    print(f"âŒ ä½œä¸ºPEFTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    raise RuntimeError(f"æ— æ³•åŠ è½½LoRAæ¨¡å‹: {self.model_path}ï¼ŒLoRAæ¨¡å‹å¿…é¡»ä¸æ­£ç¡®çš„åŸºç¡€æ¨¡å‹åŒ¹é…")
            else:
                # å¸¸è§„æ¨¡å‹åŠ è½½æµç¨‹
                print("ğŸ“¦ åŠ è½½ä¸ºå¸¸è§„æ¨¡å‹...")
                
                print("ğŸ” æ­¥éª¤1: åŠ è½½tokenizer...")
                log_memory_usage("tokenizeråŠ è½½å‰")
                
                # å¤„ç†tokenizer (ç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶)
                tokenizer_kwargs = {"trust_remote_code": True}
                    
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                    print("âœ… tokenizeråŠ è½½æˆåŠŸ")
                except Exception as e:
                    print(f"âš ï¸ æ ‡å‡†tokenizeråŠ è½½å¤±è´¥: {e}")
                    print("ğŸ” å°è¯•ä½¿ç”¨å¤‡ç”¨tokenizeré€‰é¡¹...")
                    tokenizer_kwargs["use_fast"] = False
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
                    print("âœ… å¤‡ç”¨tokenizeråŠ è½½æˆåŠŸ")
                
                log_memory_usage("tokenizeråŠ è½½å")
                
                print("ğŸ” æ­¥éª¤2: å‡†å¤‡æ¨¡å‹åŠ è½½å‚æ•°...")
                # é’ˆå¯¹ç‰¹æ®Šæ¨¡å‹çš„å¤„ç†
                model_name_lower = self.model_path.lower()
                special_kwargs = load_kwargs.copy()
                
                if "gemma" in model_name_lower:
                    print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                    special_kwargs.update({
                        "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                        "_attn_implementation_internal": "eager"
                    })
                elif "llama" in model_name_lower:
                    print("ğŸ¦™ æ£€æµ‹åˆ°Llamaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                    # Llamaæ¨¡å‹å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    special_kwargs.update({
                        "use_cache": True,  # Llamaé€šå¸¸å¯ä»¥ä½¿ç”¨ç¼“å­˜
                    })
                
                print(f"ğŸ” æœ€ç»ˆåŠ è½½å‚æ•°: {special_kwargs}")
                
                print("ğŸ” æ­¥éª¤3: åŠ è½½æ¨¡å‹...")
                log_memory_usage("æ¨¡å‹åŠ è½½å‰")
                    
                # åŠ è½½æ¨¡å‹ï¼Œç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    **special_kwargs
                )
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                log_memory_usage("æ¨¡å‹åŠ è½½å")
        
            # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # è®¾ç½®pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è®¾ç½®åŠ è½½å®Œæˆæ ‡å¿—
            self._model_loaded = True
            
            # ä¿å­˜åˆ°å…¨å±€ç¼“å­˜
            _MODEL_CACHE[cache_key] = {
                'model': self.model,
                'tokenizer': self.tokenizer
            }
            print(f"ğŸ’¾ æ¨¡å‹å·²ç¼“å­˜: {cache_key}")
                
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {self.model_path}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"âŒ è¯¦ç»†é”™è¯¯:")
            traceback.print_exc()
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {self.model_path}: {str(e)}")

    def test_step(self, batch, batch_idx):
        """å•ä¸ªæµ‹è¯•æ­¥éª¤"""
        try:
            print(f"ğŸ” test_stepå¼€å§‹ - batch_idx: {batch_idx}, batch_size: {len(batch) if batch else 0}")
            log_memory_usage(f"test_step_{batch_idx}_å¼€å§‹")
            
            # éªŒè¯batchå†…å®¹
            if not batch:
                print(f"âš ï¸ ç©ºbatchï¼Œè·³è¿‡å¤„ç†")
                return {
                    'loss': torch.tensor(0.0),
                    'accuracy': torch.tensor(0.0),
                    'perplexity': torch.tensor(1.0),
                    'batch_size': 0
                }
            
            print(f"ğŸ” batchæ ·æœ¬ç¤ºä¾‹: {batch[0] if len(batch) > 0 else 'None'}")
            
            # è®¡ç®—æŸå¤±
            print(f"ğŸ” å¼€å§‹è®¡ç®—æŸå¤±...")
            loss = self._compute_loss(batch)
            print(f"ğŸ” æŸå¤±è®¡ç®—å®Œæˆ: {loss}")
            
            # è®¡ç®—å‡†ç¡®ç‡
            print(f"ğŸ” å¼€å§‹è®¡ç®—å‡†ç¡®ç‡...")
            accuracy = self._compute_accuracy(batch)
            print(f"ğŸ” å‡†ç¡®ç‡è®¡ç®—å®Œæˆ: {accuracy}")
            
            # è®¡ç®—å›°æƒ‘åº¦
            perplexity = torch.exp(loss)
            print(f"ğŸ” å›°æƒ‘åº¦è®¡ç®—å®Œæˆ: {perplexity}")
            
            batch_size = len(batch)
            
            # è®°å½•æŒ‡æ ‡
            self.log('test/loss', loss, batch_size=batch_size)
            self.log('test/accuracy', accuracy, batch_size=batch_size)
            self.log('test/perplexity', perplexity, batch_size=batch_size)
            
            log_memory_usage(f"test_step_{batch_idx}_å®Œæˆ")
            print(f"âœ… test_stepå®Œæˆ - batch_idx: {batch_idx}")
            
            return {
                'loss': loss,
                'accuracy': accuracy,
                'perplexity': perplexity,
                'batch_size': batch_size
            }
        except Exception as e:
            print(f"âŒ test_stepå¤±è´¥ (batch_idx={batch_idx}): {e}")
            print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"âŒ batchå¤§å°: {len(batch) if batch else 'None'}")
            if batch and len(batch) > 0:
                print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬: {batch[0]}")
            print(f"âŒ è¯¦ç»†traceback:")
            traceback.print_exc()
            log_memory_usage(f"test_step_{batch_idx}_å¼‚å¸¸")
            
            # è¿”å›é»˜è®¤å€¼é¿å…è®­ç»ƒä¸­æ–­
            return {
                'loss': torch.tensor(float('inf')),
                'accuracy': torch.tensor(0.0),
                'perplexity': torch.tensor(float('inf')),
                'batch_size': len(batch) if batch else 1
            }
        
    def _compute_loss(self, batch):
        """è®¡ç®—æŸå¤±"""
        try:
            inputs = []
            labels = []
            
            for item in batch:
                # å¤„ç†å¤šé€‰é¢˜æ ¼å¼
                if 'input' in item and 'options' in item:
                    question = item['input']
                    options = item['options']
                    target = item.get('target', 'A')
                    
                    # æ ¼å¼åŒ–é—®é¢˜ã€é€‰é¡¹å’Œç­”æ¡ˆ
                    text = f"Question: {question}\n"
                    for option in options:
                        text += f"{option}\n"
                    text += f"Answer: {target}"
                else:
                    # å¤‡é€‰ï¼šä½¿ç”¨ä»»ä½•æ–‡æœ¬å­—æ®µ
                    text = item.get('text', str(item))
                
                # Tokenize
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                inputs.append(encoding['input_ids'].squeeze())
                labels.append(encoding['input_ids'].squeeze())

            if inputs:
                input_ids = torch.stack(inputs).to(self.device)
                attention_mask = torch.ones_like(input_ids).to(self.device)
                labels = torch.stack(labels).to(self.device)
            else:
                return torch.tensor(0.0)
            
            # è®¡ç®—æŸå¤±
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            return outputs.loss
            
        except Exception as e:
            print(f"âŒ _compute_losså¤±è´¥: {e}")
            print(f"âŒ batchå¤§å°: {len(batch) if batch else 'None'}")
            if batch:
                print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬: {batch[0] if len(batch) > 0 else 'Empty'}")
            traceback.print_exc()
            return torch.tensor(float('inf'))

    def _compute_accuracy(self, batch):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if not isinstance(batch, list):
            return torch.tensor(0.25)  # 4é€‰1é¢˜çš„éšæœºåŸºçº¿
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for item in batch:
                try:
                    # è§£ææ•°æ®é¡¹
                    question = item.get('input', '')
                    options = item.get('options', [])
                    correct_answer = item.get('target', 'A')
                    
                    if not options:
                        total += 1
                        continue
                    
                    # æ ¼å¼åŒ–å¸¦é€‰é¡¹çš„é—®é¢˜
                    prompt = f"Question: {question}\n"
                    for option in options:
                        prompt += f"{option}\n"
                    prompt += "Answer:"
                    
                    # Tokenize
                    inputs = self.tokenizer(
                        prompt,
                        return_tensors='pt',
                        truncation=True,
                        max_length=self.max_length,
                        padding=True
                    ).to(self.device)
                    
                    # Gemmaæ¨¡å‹ç‰¹æ®Šå¤„ç†
                    model_name_lower = self.model_path.lower()
                    generation_kwargs = {
                        "max_new_tokens": 3,  # å‡å°‘ç”Ÿæˆé•¿åº¦
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜
                        "output_attentions": False,
                        "output_hidden_states": False,
                    }
                    
                    if "gemma" in model_name_lower:
                        # Gemmaæ¨¡å‹ç‰¹æ®Šé€‚é…
                        generation_kwargs.update({
                            "temperature": 1.0,
                            "top_p": 1.0,
                            "repetition_penalty": 1.0,
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    outputs = self.model.generate(
                        **inputs,
                        **generation_kwargs
                    )
                    
                    # è§£ç ç”Ÿæˆçš„ç­”æ¡ˆ
                    generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                    generated_answer = generated_text.strip().upper()
                    
                    # æå–ç¬¬ä¸€ä¸ªå­—æ¯ (A, B, C, æˆ– D)
                    predicted_answer = None
                    for char in generated_answer:
                        if char in ['A', 'B', 'C', 'D']:
                            predicted_answer = char
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå°è¯•åŒ¹é…é€‰é¡¹å‰ç¼€
                    if predicted_answer is None:
                        for option in options:
                            if option.startswith('A:') and 'A' in generated_answer:
                                predicted_answer = 'A'
                            elif option.startswith('B:') and 'B' in generated_answer:
                                predicted_answer = 'B'
                            elif option.startswith('C:') and 'C' in generated_answer:
                                predicted_answer = 'C'
                            elif option.startswith('D:') and 'D' in generated_answer:
                                predicted_answer = 'D'
                            if predicted_answer:
                                break
                    
                    # ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ
                    if predicted_answer == correct_answer:
                        correct += 1
                    
                    total += 1
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ ·æœ¬é”™è¯¯: {e}")
                    total += 1
                    continue
        
        if total == 0:
            return torch.tensor(0.0)
        
        accuracy = correct / total
        return torch.tensor(accuracy)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨ - è¯„ä¼°æ¨¡å¼ä¸éœ€è¦ï¼Œä½†Lightningéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return None
