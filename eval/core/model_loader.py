"""
æ¨¡å‹åŠ è½½æ¨¡å—
åŒ…å«æ¨¡å‹åŠ è½½ç›¸å…³çš„å·¥å…·å‡½æ•°
"""

from .config import *


def load_model_for_eval(model_path, device="auto", **kwargs):
    """åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡å¥½è¯„ä¼°"""
    print(f"â³ åŠ è½½æ¨¡å‹ {model_path}...")
    
    # ç¡®å®šè®¾å¤‡
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    is_local_path = os.path.exists(model_path)
    
    try:
        # ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œé¿å…tokenizerè­¦å‘Šå’Œä¼˜åŒ–
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # åŠ è½½tokenizer (ç‰¹åˆ«å¤„ç†æœ¬åœ°æ¨¡å‹è·¯å¾„)
        tokenizer_kwargs = {"trust_remote_code": True}
        if is_local_path:
            tokenizer_kwargs["local_files_only"] = True
            
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_kwargs)
        except Exception as e:
            print(f"è­¦å‘Š: æ ‡å‡†tokenizeråŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é¢„è®­ç»ƒtokenizer: {e}")
            tokenizer_kwargs["use_fast"] = False
            tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_kwargs)
        
        # æ¨¡å‹åç§°è½¬å°å†™ç”¨äºåˆ¤æ–­
        model_name_lower = model_path.lower() if isinstance(model_path, str) else ""
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ä¸åŒçš„åŠ è½½å‚æ•°
        model_kwargs = {
            "torch_dtype": torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            "device_map": device,
            "trust_remote_code": True,
        }
        
        # æ·»åŠ æœ¬åœ°æ–‡ä»¶å‚æ•°
        if is_local_path:
            model_kwargs["local_files_only"] = True
        
        # Gemmaæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
        if "gemma" in model_name_lower:
            print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®ŠåŠ è½½è®¾ç½®")
            model_kwargs.update({
                "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
            })
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRAå¾®è°ƒæ¨¡å‹
        adapter_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_path):
            print("ğŸ” æ£€æµ‹åˆ°LoRAé€‚é…å™¨é…ç½®")
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model_path = kwargs.get('base_model_path')
            if not base_model_path:
                # å°è¯•ä»adapter_config.jsonä¸­æ‰¾åˆ°åŸºç¡€æ¨¡å‹
                try:
                    with open(adapter_path, 'r') as f:
                        adapter_config = json.load(f)
                    base_model_path = adapter_config.get('base_model_name_or_path')
                    print(f"ğŸ“„ ä»adapter_config.jsonè·å–åˆ°åŸºç¡€æ¨¡å‹: {base_model_path}")
                except Exception as e:
                    raise ValueError(f"LoRAæ¨¡å‹éœ€è¦æŒ‡å®šbase_model_pathå‚æ•°: {e}")
            
            # æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„
            if not os.path.exists(base_model_path) and "/" not in base_model_path:
                # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•autodl-tmpä¸­çš„å¸¸è§ä½ç½®
                for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                    test_path = f"{prefix}{base_model_path}"
                    if os.path.exists(test_path):
                        base_model_path = test_path
                        print(f"ğŸ” å®šä½åˆ°åŸºç¡€æ¨¡å‹: {base_model_path}")
                        break
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print(f"ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            
            # ç‰¹æ®Šå¤„ç†æœ¬åœ°åŸºç¡€æ¨¡å‹
            base_kwargs = model_kwargs.copy()
            if os.path.exists(base_model_path):
                base_kwargs["local_files_only"] = True
                
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                **base_kwargs
            )
            
            # åŠ è½½LoRAæƒé‡
            print(f"ğŸ”„ åŠ è½½LoRAæƒé‡: {model_path}")
            model = PeftModel.from_pretrained(model, model_path)
        else:
            # ç›´æ¥åŠ è½½æ¨¡å‹
            print(f"ğŸ”„ åŠ è½½æ ‡å‡†æ¨¡å‹: {model_path}")
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs
            )
        
        # å°†æ¨¡å‹ç½®äºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        print(f"âœ… æ¨¡å‹ {model_path} åŠ è½½å®Œæˆ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        raise
