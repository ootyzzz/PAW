"""
æ‰¹é‡è¯„ä¼°æ¨¡å—
åŒ…å«æ‰¹é‡è¯„ä¼°é€»è¾‘å’Œç»“æœå¤„ç†åŠŸèƒ½
"""

from .config import *
from .data import get_test_file_path, SimpleDataset
from .evaluator import LightningModelEvaluator


def evaluate_models(
    models_list: List[str],
    dataset_name: str,
    output_dir: str = "eval/results",
    base_model_path: str = None,
    sample_ratio: float = 1.0,
    batch_size: int = 8
):
    """è¯„ä¼°å¤šä¸ªæ¨¡å‹å¹¶ä¿å­˜ç»“æœ"""
    print("\n" + "=" * 70)
    print(f"ğŸš€ Lightning æ‰¹é‡æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = get_test_file_path(dataset_name)
    test_dataset = SimpleDataset(test_file, sample_ratio=sample_ratio)
    
    print(f"ğŸ“ æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ“Š é‡‡æ ·æ¯”ä¾‹: {sample_ratio*100:.1f}%")
    
    results = {}
    start_time = time.time()
    
    # å‡†å¤‡å…±äº«æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä»¥ç¡®ä¿å¯æ¯”æ€§
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,  # Lightningæµ‹è¯•æ¨èæ‰“ä¹±é¡ºåº
        num_workers=2,  # å‡å°‘workeræ•°é‡ï¼Œé™ä½forkå¸¦æ¥çš„è­¦å‘Š
        pin_memory=True,
        collate_fn=lambda batch: batch,  # ä¿æŒæ‰¹æ¬¡æ ¼å¼ä¸å˜
        generator=torch.Generator().manual_seed(42),  # å›ºå®šéšæœºç§å­
        persistent_workers=True  # ä¿æŒworkeræŒç»­è¿è¡Œï¼Œé¿å…é¢‘ç¹fork
    )
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for i, model_path in enumerate(models_list):
        print(f"\n{'='*70}")
        print(f"ğŸ“Š [{i+1}/{len(models_list)}] è¯„ä¼°æ¨¡å‹: {model_path}")
        
        model_name = Path(model_path).name
        if not model_name:  # å¤„ç†è·¯å¾„æœ«å°¾çš„æ–œæ 
            model_name = Path(model_path).parent.name
            
        try:
            # åˆå§‹åŒ–Lightningè¯„ä¼°æ¨¡å—
            evaluator = LightningModelEvaluator(model_path, base_model_path)
            
            # åˆ›å»ºTrainer (æ— éœ€checkpoint) - é’ˆå¯¹Gemmaæ¨¡å‹ä¼˜åŒ–
            trainer_kwargs = {
                "accelerator": 'auto',
                "devices": 'auto',
                "precision": '16-mixed' if torch.cuda.is_available() else 32,
                "logger": False,
                "enable_checkpointing": False,  # è¯„ä¼°ä¸éœ€è¦æ£€æŸ¥ç‚¹
                "enable_model_summary": False,  # å…³é—­æ¨¡å‹æ‘˜è¦
                "enable_progress_bar": True,
                "deterministic": False,  # å¯¹Gemmaæ¨¡å‹ç¦ç”¨deterministic
                "num_sanity_val_steps": 0,  # é¿å…sanityæ£€æŸ¥
                "inference_mode": True,  # ä½¿ç”¨æ¨ç†æ¨¡å¼
                "benchmark": False,  # å…³é—­åŸºå‡†æµ‹è¯•
            }
            
            # å¦‚æœæ˜¯Gemmaæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
            if "gemma" in model_path.lower():
                trainer_kwargs.update({
                    "precision": 32,  # ä½¿ç”¨32ä½ç²¾åº¦é¿å…æ•°å€¼é—®é¢˜
                    "deterministic": False,  # å®Œå…¨ç¦ç”¨deterministic
                })
            
            trainer = Trainer(**trainer_kwargs)
            
            # æ‰§è¡Œæµ‹è¯•
            eval_start = time.time()
            test_results = trainer.test(evaluator, dataloaders=test_loader)
            eval_time = time.time() - eval_start
            
            # æ•´ç†ç»“æœ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡è€Œä¸æ˜¯Tensor
            model_results = {}
            if test_results and len(test_results) > 0:
                raw_results = test_results[0]
                # è½¬æ¢æ‰€æœ‰çš„tensorå€¼ä¸ºPythonæ ‡é‡
                for key, value in raw_results.items():
                    if hasattr(value, 'item'):
                        model_results[key] = value.item()
                    else:
                        model_results[key] = value
            
            # æ·»åŠ æ—¶é—´æŒ‡æ ‡
            model_results['eval_time_seconds'] = eval_time
            model_results['samples_per_second'] = len(test_dataset) / eval_time
            
            # æ·»åŠ åˆ°ç»“æœé›†
            results[model_name] = {
                dataset_name: model_results
            }
            
            # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
            result_file = output_path / f"{model_name}_{dataset_name}_evaluation_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(model_results, f, indent=4, ensure_ascii=False)
                
            print(f"âœ… è¯„ä¼°å®Œæˆ (ç”¨æ—¶: {eval_time:.1f}ç§’, {model_results['samples_per_second']:.1f} æ ·æœ¬/ç§’)")
            print(f"ğŸ“Š ç»“æœ:")
            print(f"  - Loss: {model_results.get('test/loss', 0):.4f}")
            print(f"  - Accuracy: {model_results.get('test/accuracy', 0):.4f}") 
            print(f"  - Perplexity: {model_results.get('test/perplexity', 0):.4f}")
            
            # æ¸…ç†å†…å­˜
            del evaluator
            del trainer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            print(f"âŒ æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"âŒ æ¨¡å‹åç§°: {model_name}")
            print(f"âŒ æ•°æ®é›†: {dataset_name}")
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            results[model_name] = {
                dataset_name: {"error": str(e)}
            }
    
    # è®¡ç®—æ€»ç”¨æ—¶
    total_time = time.time() - start_time
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = output_path / f"lightning_evaluation_summary_{timestamp}.json"
    
    summary_data = {
        "evaluation_summary": {
            "dataset": dataset_name,
            "total_models": len(models_list),
            "sample_ratio": sample_ratio,
            "batch_size": batch_size,
            "total_samples": len(test_dataset),
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat()
        },
        "results": results
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=4, ensure_ascii=False)
    
    # ä¿å­˜CSVæ ¼å¼ç»“æœ
    rows = []
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡
                loss_val = dataset_results.get('test/loss', 0)
                acc_val = dataset_results.get('test/accuracy', 0)
                ppl_val = dataset_results.get('test/perplexity', 0)
                time_val = dataset_results.get('eval_time_seconds', 0)
                samples_val = dataset_results.get('samples_per_second', 0)
                
                # è½¬æ¢tensor/numpyå€¼ä¸ºPythonæ ‡é‡
                if hasattr(loss_val, 'item'):
                    loss_val = float(loss_val.item())
                if hasattr(acc_val, 'item'):
                    acc_val = float(acc_val.item())
                if hasattr(ppl_val, 'item'):
                    ppl_val = float(ppl_val.item())
                if hasattr(time_val, 'item'):
                    time_val = float(time_val.item())
                if hasattr(samples_val, 'item'):
                    samples_val = float(samples_val.item())
                
                rows.append({
                    'Model': str(model_name),
                    'Dataset': str(dataset_name),
                    'Loss': round(float(loss_val), 4),
                    'Accuracy': round(float(acc_val), 4),
                    'Perplexity': round(float(ppl_val), 4),
                    'Eval_Time(s)': round(float(time_val), 1),
                    'Samples/Sec': round(float(samples_val), 1),
                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
    
    csv_file = None  # åˆå§‹åŒ–å˜é‡
    if rows:
        try:
            # æ‰‹åŠ¨å†™CSVæ–‡ä»¶ï¼Œé¿å…pandasé—®é¢˜
            csv_file = output_path / f"lightning_evaluation_results_{timestamp}.csv"
            
            # è·å–åˆ—å
            headers = ['Model', 'Dataset', 'Loss', 'Accuracy', 'Perplexity', 'Eval_Time(s)', 'Samples/Sec', 'Timestamp']
            
            with open(csv_file, 'w', encoding='utf-8', newline='') as f:
                f.write(','.join(headers) + '\n')
                for row in rows:
                    values = []
                    for header in headers:
                        val = row.get(header, '')
                        # ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²ï¼Œå¹¶å¤„ç†å¯èƒ½çš„é€—å·
                        if isinstance(val, str) and ',' in val:
                            val = f'"{val}"'
                        values.append(str(val))
                    f.write(','.join(values) + '\n')
            
            print(f" ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
            
            # å°è¯•pandasæ–¹å¼ä½œä¸ºå¤‡é€‰
            try:
                # åªæœ‰åœ¨æ‰‹åŠ¨æ–¹å¼æˆåŠŸåæ‰å°è¯•pandas
                import pandas as pd
                df = pd.DataFrame(rows)
                cumulative_csv = output_path / "all_evaluation_results.csv"
                if cumulative_csv.exists():
                    existing_df = pd.read_csv(cumulative_csv, encoding='utf-8-sig')
                    # ç§»é™¤é‡å¤é¡¹
                    for _, row_data in df.iterrows():
                        mask = (existing_df['Model'] == row_data['Model']) & (existing_df['Dataset'] == row_data['Dataset'])
                        existing_df = existing_df[~mask]
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df.to_csv(cumulative_csv, index=False, encoding='utf-8-sig')
                else:
                    df.to_csv(cumulative_csv, index=False, encoding='utf-8-sig')
                print(f"ğŸ“ ç´¯ç§¯ç»“æœ: {cumulative_csv}")
            except Exception as pandas_error:
                print(f"âš ï¸ pandasç´¯ç§¯CSVæ›´æ–°å¤±è´¥: {pandas_error}")
                # ä¸å½±å“ä¸»è¦CSVæ–‡ä»¶çš„ä¿å­˜
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜CSVç»“æœå¤±è´¥: {e}")
            traceback.print_exc()
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"{'Model':<40} {'Dataset':<15} {'Loss':<8} {'Accuracy':<10} {'Perplexity':<12} {'Time(s)':<8} {'Samples/s':<10}")
    print("-" * 110)
    
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                print(f"{model_name:<40} {dataset_name:<15} "
                      f"{dataset_results.get('test/loss', 0):<8.4f} "
                      f"{dataset_results.get('test/accuracy', 0):<10.4f} "
                      f"{dataset_results.get('test/perplexity', 0):<12.4f} "
                      f"{dataset_results.get('eval_time_seconds', 0):<8.1f} "
                      f"{dataset_results.get('samples_per_second', 0):<10.1f}")
            else:
                print(f"{model_name:<40} {dataset_name:<15} {'ERROR':<8} {'ERROR':<10} {'ERROR':<12} {'ERROR':<8} {'ERROR':<10}")
    
    print("\n" + "=" * 80)
    print(f"â±ï¸  æ€»è¯„ä¼°æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ æ±‡æ€»ç»“æœ: {summary_file}")
    if csv_file:
        print(f"ğŸ“Š CSVç»“æœ: {csv_file}")
    
    return results
