"""
æ‰¹é‡è¯„ä¼°æ¨¡å—
åŒ…å«æ‰¹é‡è¯„ä¼°é€»è¾‘å’Œç»“æœå¤„ç†åŠŸèƒ½
"""

from .config import *
from .data import get_test_file_path, SimpleDataset
from .evaluator import LightningModelEvaluator


def evaluate_models(
    models_list: List[str],
    dataset_name: str,
    output_dir: str = "eval/results",
    base_model_path: str = None,
    sample_ratio: float = 1.0,
    batch_size: int = 1
):
    """è¯„ä¼°å¤šä¸ªæ¨¡å‹å¹¶ä¿å­˜ç»“æœ"""
    print("\n" + "=" * 70)
    print(f"ğŸš€ Lightning æ‰¹é‡æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = get_test_file_path(dataset_name)
    test_dataset = SimpleDataset(test_file, sample_ratio=sample_ratio)
    
    print(f"ğŸ“ æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ“Š é‡‡æ ·æ¯”ä¾‹: {sample_ratio*100:.1f}%")
    
    results = {}
    start_time = time.time()
    
    # å‡†å¤‡å…±äº«æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä»¥ç¡®ä¿å¯æ¯”æ€§
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,  # Lightningæµ‹è¯•æ¨èæ‰“ä¹±é¡ºåº
        num_workers=2,  # å‡å°‘workeræ•°é‡ï¼Œé™ä½forkå¸¦æ¥çš„è­¦å‘Š
        pin_memory=True,
        collate_fn=lambda batch: batch,  # ä¿æŒæ‰¹æ¬¡æ ¼å¼ä¸å˜
        generator=torch.Generator().manual_seed(42),  # å›ºå®šéšæœºç§å­
        persistent_workers=True  # ä¿æŒworkeræŒç»­è¿è¡Œï¼Œé¿å…é¢‘ç¹fork
    )
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for i, model_path in enumerate(models_list):
        print(f"\n{'='*70}")
        print(f"ğŸ“Š [{i+1}/{len(models_list)}] è¯„ä¼°æ¨¡å‹: {model_path}")
        
        model_name = Path(model_path).name
        if not model_name:  # å¤„ç†è·¯å¾„æœ«å°¾çš„æ–œæ 
            model_name = Path(model_path).parent.name
            
        try:
            print(f"ğŸ” å¼€å§‹åˆå§‹åŒ–è¯„ä¼°å™¨...")
            print(f"ğŸ” æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"ğŸ” åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
            print(f"ğŸ” æ¨¡å‹åç§°: {model_name}")
            
            # åˆå§‹åŒ–Lightningè¯„ä¼°æ¨¡å—
            evaluator = LightningModelEvaluator(model_path, base_model_path)
            print(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ— éœ€è°ƒæ•´batch size
            
            # åˆ›å»ºTrainer (æ— éœ€checkpoint) - é’ˆå¯¹Gemmaæ¨¡å‹ä¼˜åŒ–
            trainer_kwargs = {
                "accelerator": 'auto',
                "devices": 1,  # å¼ºåˆ¶ä½¿ç”¨å•GPUï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
                "precision": '16-mixed' if torch.cuda.is_available() else 32,
                "logger": False,
                "enable_checkpointing": False,  # è¯„ä¼°ä¸éœ€è¦æ£€æŸ¥ç‚¹
                "enable_model_summary": False,  # å…³é—­æ¨¡å‹æ‘˜è¦
                "enable_progress_bar": True,  # å¯ç”¨è¿›åº¦æ¡æ˜¾ç¤ºç´¯ç§¯å‡†ç¡®ç‡
                "deterministic": False,  # å¯¹Gemmaæ¨¡å‹ç¦ç”¨deterministic
                "num_sanity_val_steps": 0,  # é¿å…sanityæ£€æŸ¥
                "inference_mode": True,  # ä½¿ç”¨æ¨ç†æ¨¡å¼
                "benchmark": False,  # å…³é—­åŸºå‡†æµ‹è¯•
                "strategy": "auto",  # ä½¿ç”¨è‡ªåŠ¨ç­–ç•¥ï¼Œä½†é™åˆ¶ä¸ºå•è®¾å¤‡
            }
            
            # å¦‚æœæ˜¯Gemmaæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
            if "gemma" in model_path.lower():
                trainer_kwargs.update({
                    "precision": 32,  # ä½¿ç”¨32ä½ç²¾åº¦é¿å…æ•°å€¼é—®é¢˜
                    "deterministic": False,  # å®Œå…¨ç¦ç”¨deterministic
                })
            
            trainer = Trainer(**trainer_kwargs)
            
            # æ‰§è¡Œæµ‹è¯•
            eval_start = time.time()
            test_results = trainer.test(evaluator, dataloaders=test_loader)
            eval_time = time.time() - eval_start
            
            # æ•´ç†ç»“æœ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡è€Œä¸æ˜¯Tensor
            model_results = {}
            if test_results and len(test_results) > 0:
                raw_results = test_results[0]
                # è½¬æ¢æ‰€æœ‰çš„tensorå€¼ä¸ºPythonæ ‡é‡
                for key, value in raw_results.items():
                    if hasattr(value, 'item'):
                        model_results[key] = value.item()
                    else:
                        model_results[key] = value
            
            # æ·»åŠ æ—¶é—´æŒ‡æ ‡
            model_results['eval_time_seconds'] = eval_time
            model_results['samples_per_second'] = len(test_dataset) / eval_time
            
            # æ·»åŠ åˆ°ç»“æœé›†
            results[model_name] = {
                dataset_name: model_results
            }
            
            # ä¸å†ä¿å­˜å•ä¸ªæ¨¡å‹çš„JSONç»“æœæ–‡ä»¶ï¼Œå‡å°‘æ–‡ä»¶è¾“å‡º
            # result_file = output_path / f"{model_name}_{dataset_name}_evaluation_results.json"
            # with open(result_file, 'w', encoding='utf-8') as f:
            #     json.dump(model_results, f, indent=4, ensure_ascii=False)
                
            print(f"âœ… è¯„ä¼°å®Œæˆ (ç”¨æ—¶: {eval_time:.1f}ç§’, {model_results['samples_per_second']:.1f} æ ·æœ¬/ç§’)")
            
            # æ¸…ç†å†…å­˜
            del evaluator
            del trainer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"âŒ æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"âŒ æ¨¡å‹åç§°: {model_name}")
            print(f"âŒ æ•°æ®é›†: {dataset_name}")
            print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
            print(f"âŒ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if os.path.exists(model_path):
                print(f"âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨")
                try:
                    files = os.listdir(model_path)
                    print(f"ğŸ” æ¨¡å‹ç›®å½•æ–‡ä»¶: {files[:5]}...")
                except Exception as list_error:
                    print(f"âš ï¸ æ— æ³•åˆ—å‡ºæ¨¡å‹ç›®å½•: {list_error}")
            else:
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
            
            # å†…å­˜çŠ¶æ€
            try:
                if torch.cuda.is_available():
                    gpu_allocated = torch.cuda.memory_allocated() / 1024**3
                    gpu_reserved = torch.cuda.memory_reserved() / 1024**3
                    print(f"ğŸ” GPUå†…å­˜: {gpu_allocated:.2f}GB / {gpu_reserved:.2f}GB")
            except Exception as mem_error:
                print(f"âš ï¸ å†…å­˜æ£€æŸ¥å¤±è´¥: {mem_error}")
            
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            results[model_name] = {
                dataset_name: {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "model_path": model_path,
                    "model_exists": os.path.exists(model_path)
                }
            }
    
    # è®¡ç®—æ€»ç”¨æ—¶
    total_time = time.time() - start_time
    
    # ç®€åŒ–æ±‡æ€»ç»“æœè¾“å‡º - ä¸å†ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„JSONæ–‡ä»¶
    # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # summary_file = output_path / f"lightning_evaluation_summary_{timestamp}.json"
    #
    # summary_data = {
    #     "evaluation_summary": {
    #         "dataset": dataset_name,
    #         "total_models": len(models_list),
    #         "sample_ratio": sample_ratio,
    #         "batch_size": batch_size,
    #         "total_samples": len(test_dataset),
    #         "total_time_seconds": total_time,
    #         "timestamp": datetime.now().isoformat()
    #     },
    #     "results": results
    # }
    #
    # with open(summary_file, 'w', encoding='utf-8') as f:
    #     json.dump(summary_data, f, indent=4, ensure_ascii=False)
    
    # ä¿å­˜CSVæ ¼å¼ç»“æœ
    rows = []
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡
                loss_val = dataset_results.get('test/loss', 0)
                acc_val = dataset_results.get('test/accuracy', 0)
                ppl_val = dataset_results.get('test/perplexity', 0)
                time_val = dataset_results.get('eval_time_seconds', 0)
                samples_val = dataset_results.get('samples_per_second', 0)
                
                # å¼ºåˆ¶è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                try:
                    if hasattr(loss_val, 'item'):
                        loss_val = float(loss_val.item())
                    else:
                        loss_val = float(loss_val)
                except:
                    loss_val = 0.0
                    
                try:
                    if hasattr(acc_val, 'item'):
                        acc_val = float(acc_val.item())
                    else:
                        acc_val = float(acc_val)
                except:
                    acc_val = 0.0
                    
                try:
                    if hasattr(ppl_val, 'item'):
                        ppl_val = float(ppl_val.item())
                    else:
                        ppl_val = float(ppl_val)
                except:
                    ppl_val = 0.0
                    
                try:
                    if hasattr(time_val, 'item'):
                        time_val = float(time_val.item())
                    else:
                        time_val = float(time_val)
                except:
                    time_val = 0.0
                    
                try:
                    if hasattr(samples_val, 'item'):
                        samples_val = float(samples_val.item())
                    else:
                        samples_val = float(samples_val)
                except:
                    samples_val = 0.0
                
                row_data = {
                    'Model': str(model_name),
                    'Dataset': str(dataset_name),
                    'Loss': round(loss_val, 4),
                    'Accuracy': round(acc_val, 4),
                    'Perplexity': round(ppl_val, 4),
                    'Eval_Time(s)': round(time_val, 1),
                    'Samples/Sec': round(samples_val, 1),
                    'Batch_Size': batch_size,
                    'Timestamp': str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
                }
                rows.append(row_data)
    
    # ä¿å­˜CSVæ ¼å¼ç»“æœ - ä½¿ç”¨åŸç”ŸCSVå†™å…¥é¿å…pandasé—®é¢˜
    if rows:
        try:
            import csv
            
            # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„CSVæ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = output_path / f"lightning_evaluation_results_{timestamp}.csv"
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                if rows:
                    fieldnames = rows[0].keys()
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(rows)
            
            print(f"ğŸ“ CSVç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
            
            # å¦‚æœæ˜¯å®Œæ•´æ•°æ®é›†è¯„ä¼°ï¼Œä¹Ÿè¿½åŠ åˆ°æ€»çš„å®éªŒç»“æœæ–‡ä»¶
            if sample_ratio == 1.0:
                experiment_csv = Path("results/experiment_results.csv")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                experiment_csv.parent.mkdir(parents=True, exist_ok=True)
                
                # ç®€å•è¿½åŠ åˆ°æ–‡ä»¶
                file_exists = experiment_csv.exists()
                with open(experiment_csv, 'a', newline='', encoding='utf-8') as f:
                    if rows:
                        fieldnames = rows[0].keys()
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        if not file_exists:
                            writer.writeheader()
                        writer.writerows(rows)
                
                print(f"ğŸ“ æ€»ç»“æœå·²è¿½åŠ åˆ°: {experiment_csv}")
        except Exception as csv_error:
            print(f"âš ï¸ ä¿å­˜CSVç»“æœå¤±è´¥: {csv_error}")
            import traceback
            traceback.print_exc()
    
    print(f"â±ï¸  æ€»è¯„ä¼°æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    
    return results
