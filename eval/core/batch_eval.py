"""
æ‰¹é‡è¯„ä¼°æ¨¡å—
åŒ…å«æ‰¹é‡è¯„ä¼°é€»è¾‘å’Œç»“æœå¤„ç†åŠŸèƒ½
"""

from .config import *
from .data import get_test_file_path, SimpleDataset
from .evaluator import LightningModelEvaluator


def evaluate_models(
    models_list: List[str],
    dataset_name: str,
    output_dir: str = "eval/results",
    base_model_path: str = None,
    sample_ratio: float = 1.0,
    batch_size: int = 1
):
    """è¯„ä¼°å¤šä¸ªæ¨¡å‹å¹¶ä¿å­˜ç»“æœ"""
    print("\n" + "=" * 70)
    print(f"ğŸš€ Lightning æ‰¹é‡æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = get_test_file_path(dataset_name)
    test_dataset = SimpleDataset(test_file, sample_ratio=sample_ratio)
    
    print(f"ğŸ“ æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ“Š é‡‡æ ·æ¯”ä¾‹: {sample_ratio*100:.1f}%")
    
    results = {}
    start_time = time.time()
    
    # å‡†å¤‡å…±äº«æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä»¥ç¡®ä¿å¯æ¯”æ€§
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,  # Lightningæµ‹è¯•æ¨èæ‰“ä¹±é¡ºåº
        num_workers=2,  # å‡å°‘workeræ•°é‡ï¼Œé™ä½forkå¸¦æ¥çš„è­¦å‘Š
        pin_memory=True,
        collate_fn=lambda batch: batch,  # ä¿æŒæ‰¹æ¬¡æ ¼å¼ä¸å˜
        generator=torch.Generator().manual_seed(42),  # å›ºå®šéšæœºç§å­
        persistent_workers=True  # ä¿æŒworkeræŒç»­è¿è¡Œï¼Œé¿å…é¢‘ç¹fork
    )
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for i, model_path in enumerate(models_list):
        print(f"\n{'='*70}")
        print(f"ğŸ“Š [{i+1}/{len(models_list)}] è¯„ä¼°æ¨¡å‹: {model_path}")
        
        model_name = Path(model_path).name
        if not model_name:  # å¤„ç†è·¯å¾„æœ«å°¾çš„æ–œæ 
            model_name = Path(model_path).parent.name
            
        try:
            print(f"ğŸ” å¼€å§‹åˆå§‹åŒ–è¯„ä¼°å™¨...")
            print(f"ğŸ” æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"ğŸ” åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
            print(f"ğŸ” æ¨¡å‹åç§°: {model_name}")
            
            # åˆå§‹åŒ–Lightningè¯„ä¼°æ¨¡å—
            evaluator = LightningModelEvaluator(model_path, base_model_path)
            print(f"âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # æ¨¡å‹åŠ è½½å®Œæˆï¼Œæ— éœ€è°ƒæ•´batch size
            
            # åˆ›å»ºTrainer (æ— éœ€checkpoint) - é’ˆå¯¹Gemmaæ¨¡å‹ä¼˜åŒ–
            trainer_kwargs = {
                "accelerator": 'auto',
                "devices": 1,  # å¼ºåˆ¶ä½¿ç”¨å•GPUï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
                "precision": '16-mixed' if torch.cuda.is_available() else 32,
                "logger": False,
                "enable_checkpointing": False,  # è¯„ä¼°ä¸éœ€è¦æ£€æŸ¥ç‚¹
                "enable_model_summary": False,  # å…³é—­æ¨¡å‹æ‘˜è¦
                "enable_progress_bar": True,
                "deterministic": False,  # å¯¹Gemmaæ¨¡å‹ç¦ç”¨deterministic
                "num_sanity_val_steps": 0,  # é¿å…sanityæ£€æŸ¥
                "inference_mode": True,  # ä½¿ç”¨æ¨ç†æ¨¡å¼
                "benchmark": False,  # å…³é—­åŸºå‡†æµ‹è¯•
                "strategy": "auto",  # ä½¿ç”¨è‡ªåŠ¨ç­–ç•¥ï¼Œä½†é™åˆ¶ä¸ºå•è®¾å¤‡
            }
            
            # å¦‚æœæ˜¯Gemmaæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
            if "gemma" in model_path.lower():
                trainer_kwargs.update({
                    "precision": 32,  # ä½¿ç”¨32ä½ç²¾åº¦é¿å…æ•°å€¼é—®é¢˜
                    "deterministic": False,  # å®Œå…¨ç¦ç”¨deterministic
                })
            
            trainer = Trainer(**trainer_kwargs)
            
            # æ‰§è¡Œæµ‹è¯•
            eval_start = time.time()
            test_results = trainer.test(evaluator, dataloaders=test_loader)
            eval_time = time.time() - eval_start
            
            # æ•´ç†ç»“æœ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡è€Œä¸æ˜¯Tensor
            model_results = {}
            if test_results and len(test_results) > 0:
                raw_results = test_results[0]
                # è½¬æ¢æ‰€æœ‰çš„tensorå€¼ä¸ºPythonæ ‡é‡
                for key, value in raw_results.items():
                    if hasattr(value, 'item'):
                        model_results[key] = value.item()
                    else:
                        model_results[key] = value
            
            # æ·»åŠ æ—¶é—´æŒ‡æ ‡
            model_results['eval_time_seconds'] = eval_time
            model_results['samples_per_second'] = len(test_dataset) / eval_time
            
            # æ·»åŠ åˆ°ç»“æœé›†
            results[model_name] = {
                dataset_name: model_results
            }
            
            # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
            result_file = output_path / f"{model_name}_{dataset_name}_evaluation_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(model_results, f, indent=4, ensure_ascii=False)
                
            print(f"âœ… è¯„ä¼°å®Œæˆ (ç”¨æ—¶: {eval_time:.1f}ç§’, {model_results['samples_per_second']:.1f} æ ·æœ¬/ç§’)")
            
            # æ¸…ç†å†…å­˜
            del evaluator
            del trainer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            print(f"âŒ å¼‚å¸¸ç±»å‹: {type(e).__name__}")
            print(f"âŒ æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"âŒ æ¨¡å‹åç§°: {model_name}")
            print(f"âŒ æ•°æ®é›†: {dataset_name}")
            print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
            print(f"âŒ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            if os.path.exists(model_path):
                print(f"âœ… æ¨¡å‹è·¯å¾„å­˜åœ¨")
                try:
                    files = os.listdir(model_path)
                    print(f"ğŸ” æ¨¡å‹ç›®å½•æ–‡ä»¶: {files[:5]}...")
                except Exception as list_error:
                    print(f"âš ï¸ æ— æ³•åˆ—å‡ºæ¨¡å‹ç›®å½•: {list_error}")
            else:
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
            
            # å†…å­˜çŠ¶æ€
            try:
                if torch.cuda.is_available():
                    gpu_allocated = torch.cuda.memory_allocated() / 1024**3
                    gpu_reserved = torch.cuda.memory_reserved() / 1024**3
                    print(f"ğŸ” GPUå†…å­˜: {gpu_allocated:.2f}GB / {gpu_reserved:.2f}GB")
            except Exception as mem_error:
                print(f"âš ï¸ å†…å­˜æ£€æŸ¥å¤±è´¥: {mem_error}")
            
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            
            results[model_name] = {
                dataset_name: {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "model_path": model_path,
                    "model_exists": os.path.exists(model_path)
                }
            }
    
    # è®¡ç®—æ€»ç”¨æ—¶
    total_time = time.time() - start_time
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = output_path / f"lightning_evaluation_summary_{timestamp}.json"
    
    summary_data = {
        "evaluation_summary": {
            "dataset": dataset_name,
            "total_models": len(models_list),
            "sample_ratio": sample_ratio,
            "batch_size": batch_size,
            "total_samples": len(test_dataset),
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat()
        },
        "results": results
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=4, ensure_ascii=False)
    
    # ä¿å­˜CSVæ ¼å¼ç»“æœ
    rows = []
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡
                loss_val = dataset_results.get('test/loss', 0)
                acc_val = dataset_results.get('test/accuracy', 0)
                ppl_val = dataset_results.get('test/perplexity', 0)
                time_val = dataset_results.get('eval_time_seconds', 0)
                samples_val = dataset_results.get('samples_per_second', 0)
                
                # è½¬æ¢tensor/numpyå€¼ä¸ºPythonæ ‡é‡
                if hasattr(loss_val, 'item'):
                    loss_val = float(loss_val.item())
                if hasattr(acc_val, 'item'):
                    acc_val = float(acc_val.item())
                if hasattr(ppl_val, 'item'):
                    ppl_val = float(ppl_val.item())
                if hasattr(time_val, 'item'):
                    time_val = float(time_val.item())
                if hasattr(samples_val, 'item'):
                    samples_val = float(samples_val.item())
                
                rows.append({
                    'Model': str(model_name),
                    'Dataset': str(dataset_name),
                    'Loss': round(float(loss_val), 4),
                    'Accuracy': round(float(acc_val), 4),
                    'Perplexity': round(float(ppl_val), 4),
                    'Eval_Time(s)': round(float(time_val), 1),
                    'Samples/Sec': round(float(samples_val), 1),
                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
    
    # å¦‚æœæ˜¯å®Œæ•´æ•°æ®é›†è¯„ä¼° (sample_ratio = 1.0)ï¼Œè¿½åŠ åˆ°å®éªŒç»“æœæ–‡ä»¶
    if rows and sample_ratio == 1.0:
        try:
            import pandas as pd
            df = pd.DataFrame(rows)
            experiment_csv = Path("results/experiment_results.csv")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            experiment_csv.parent.mkdir(parents=True, exist_ok=True)
            
            if experiment_csv.exists():
                existing_df = pd.read_csv(experiment_csv, encoding='utf-8-sig')
                # ç§»é™¤é‡å¤é¡¹
                for _, row_data in df.iterrows():
                    mask = (existing_df['Model'] == row_data['Model']) & (existing_df['Dataset'] == row_data['Dataset'])
                    existing_df = existing_df[~mask]
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                combined_df.to_csv(experiment_csv, index=False, encoding='utf-8-sig')
            else:
                df.to_csv(experiment_csv, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {experiment_csv}")
        except Exception as pandas_error:
            print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {pandas_error}")
    
    print(f"â±ï¸  æ€»è¯„ä¼°æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    
    return results
