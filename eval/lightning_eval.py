#!/usr/bin/env python3
"""
Lightningé£æ ¼çš„å¿«é€Ÿè¯„ä¼°è„šæœ¬ - åŸºäºPyTorch Lightningä¼˜åŒ–çš„è¯„ä¼°æ–¹æ³•
æ”¯æŒåŒæ—¶è¯„ä¼°å¤šä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’ŒLoRAæ¨¡å‹
ä½¿ç”¨Lightningçš„æ•°æ®åŠ è½½å’Œå¹¶è¡Œå¤„ç†æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜è¯„ä¼°é€Ÿåº¦

ä½¿ç”¨ç¤ºä¾‹:
python eval/lightning_eval.py --models_list \
    /root/autodl-tmp/models/Qwen_Qwen2.5-1.5B \
    /root/autodl-tmp/models/gemma-2-2b-it \
    /root/PAW/runs/Qwen_Qwen2.5-1.5B/arc-challenge_lora_20250723_191421/final_model \
    --dataset arc-challenge
"""

import os
import sys
import json
import argparse
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
import random

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è§£å†³tokenizerså¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# ç¦ç”¨CUDAå›¾ä¼˜åŒ–ï¼Œè§£å†³Gemmaæ¨¡å‹çš„deterministic index puté—®é¢˜
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512,expandable_segments:True"
# è®¾ç½®transformersæ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è­¦å‘Šä¿¡æ¯
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

import torch
torch.set_float32_matmul_precision('medium')  # ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
# ç¦ç”¨CUDAå›¾ä»¥é¿å…deterministicé—®é¢˜
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
torch.backends.cuda.enable_flash_sdp(False)  # ç¦ç”¨Flash Attention
torch.backends.cuda.enable_mem_efficient_sdp(False)  # ç¦ç”¨å†…å­˜é«˜æ•ˆAttention

import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TensorBoardLogger

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

import pandas as pd
from tqdm import tqdm


def get_test_file_path(dataset_name: str) -> str:
    """è·å–æµ‹è¯•æ–‡ä»¶è·¯å¾„"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®ç›®å½•è·¯å¾„
    possible_paths = [
        f"data_to_lora/cs/{dataset_name}",  # ä»PAWæ ¹ç›®å½•è¿è¡Œ
        f"../data_to_lora/cs/{dataset_name}",  # ä»pipelineç›®å½•è¿è¡Œ
        f"../../data_to_lora/cs/{dataset_name}",  # ä»å­ç›®å½•è¿è¡Œ
    ]
    
    for data_dir in possible_paths:
        test_file = f"{data_dir}/{dataset_name}_test_formatted.jsonl"
        validation_file = f"{data_dir}/{dataset_name}_validation_formatted.jsonl"
        
        if os.path.exists(test_file):
            return test_file
        elif os.path.exists(validation_file):
            print(f"ğŸ“ ä½¿ç”¨validationæ–‡ä»¶ä½œä¸ºtest: {validation_file}")
            return validation_file
    
    # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œç»™å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
    raise FileNotFoundError(f"æ•°æ®é›† {dataset_name} æ‰¾ä¸åˆ°testæˆ–validationæ–‡ä»¶ã€‚å°è¯•è¿‡çš„è·¯å¾„: {possible_paths}")


class SimpleDataset(Dataset):
    """ç®€å•çš„æ•°æ®é›†ç±»ï¼Œé€‚ç”¨äºè¯„ä¼°"""
    def __init__(self, data_file: str, sample_ratio: float = 1.0):
        self.data = self._load_data(data_file)
        
        # å¦‚æœéœ€è¦é‡‡æ ·åŠ é€Ÿè¯„ä¼°
        if sample_ratio < 1.0:
            original_size = len(self.data)
            sample_size = max(1, int(original_size * sample_ratio))
            # ä½¿ç”¨å›ºå®šç§å­ä¿è¯é‡‡æ ·å¯é‡å¤
            random.seed(42)
            self.data = random.sample(self.data, sample_size)
            print(f"  ğŸ“Š é‡‡æ ·æ•°æ®: {sample_size}/{original_size} ({sample_ratio*100:.1f}%)")
        else:
            print(f"  ğŸ“Š ä½¿ç”¨å®Œæ•´æ•°æ®: {len(self.data)}æ ·æœ¬")
    
    def _load_data(self, data_file: str) -> List[Dict[str, Any]]:
        """ä»JSONLæ–‡ä»¶åŠ è½½æ•°æ®"""
        data = []
        if os.path.exists(data_file):
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx].copy()


class LightningModelEvaluator(pl.LightningModule):
    """Lightningæ¨¡å‹è¯„ä¼°æ¨¡å—"""
    
    def __init__(self, model_path: str, base_model_path: str = None, max_length: int = 512):
        super().__init__()
        self.save_hyperparameters()
        
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.max_length = max_length
        
        # åˆ›å»ºæ¨¡å‹åç§°ç”¨äºæŠ¥å‘Š
        self.model_name = Path(model_path).name
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„è¿˜æ˜¯Hugging Faceæ¨¡å‹ID
        is_local_path = os.path.exists(self.model_path)
        
        print(f"ğŸ” æ¨¡å‹è·¯å¾„æ£€æŸ¥: {self.model_path}")
        print(f"ğŸ” æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„: {is_local_path}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not is_local_path:
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        
        try:
            # æ¨¡å‹åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "trust_remote_code": True,
                "use_cache": True,
                "device_map": "auto" if torch.cuda.is_available() else None,
            }
            
            print(f"ğŸ” æ¨¡å‹åŠ è½½å‚æ•°: {load_kwargs}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            config_path = Path(self.model_path) / "adapter_config.json"
            print(f"ğŸ” æ£€æŸ¥LoRAé…ç½®æ–‡ä»¶: {config_path} (å­˜åœ¨: {config_path.exists()})")
            
            if config_path.exists():
            # LoRAæ¨¡å‹åŠ è½½æµç¨‹
            print("ğŸ”§ æ£€æµ‹åˆ°LoRAæ¨¡å‹ï¼Œä½¿ç”¨PEFTåŠ è½½...")
            try:
                # åŠ è½½PEFTé…ç½®è·å–åŸºç¡€æ¨¡å‹ä¿¡æ¯
                peft_config = PeftConfig.from_pretrained(self.model_path)
                detected_base_model = peft_config.base_model_name_or_path
                
                # ä½¿ç”¨æä¾›çš„åŸºç¡€æ¨¡å‹è·¯å¾„æˆ–æ£€æµ‹åˆ°çš„è·¯å¾„
                actual_base_model = self.base_model_path or detected_base_model
                
                # ç¡®è®¤åŸºç¡€æ¨¡å‹è·¯å¾„
                if not os.path.exists(actual_base_model) and "/" not in actual_base_model:
                    # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•autodl-tmpä¸­çš„å¸¸è§ä½ç½®
                    for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                        test_path = f"{prefix}{actual_base_model}"
                        if os.path.exists(test_path):
                            actual_base_model = test_path
                            break
                
                print(f"ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹: {actual_base_model}")
                
                # åŠ è½½åŸºç¡€æ¨¡å‹çš„tokenizer (ç§»é™¤local_files_onlyé™åˆ¶)
                tokenizer_kwargs = {"trust_remote_code": True}
                
                self.tokenizer = AutoTokenizer.from_pretrained(actual_base_model, **tokenizer_kwargs)
                
                # ç‰¹æ®Šå¤„ç†Gemmaæ¨¡å‹
                if "gemma" in actual_base_model.lower():
                    print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                    load_kwargs.update({
                        "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                        "_attn_implementation_internal": "eager"
                    })
                
                # åŠ è½½åŸºç¡€æ¨¡å‹
                base_model = AutoModelForCausalLM.from_pretrained(
                    actual_base_model,
                    **load_kwargs
                )
                
                print(f"ğŸ”§ åŠ è½½LoRAé€‚é…å™¨: {self.model_path}")
                # åŠ è½½PEFTæ¨¡å‹
                self.model = PeftModel.from_pretrained(base_model, self.model_path)
                
            except Exception as e:
                print(f"âŒ ä½œä¸ºPEFTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                raise RuntimeError(f"æ— æ³•åŠ è½½LoRAæ¨¡å‹: {self.model_path}ï¼ŒLoRAæ¨¡å‹å¿…é¡»ä¸æ­£ç¡®çš„åŸºç¡€æ¨¡å‹åŒ¹é…")
        else:
            # å¸¸è§„æ¨¡å‹åŠ è½½æµç¨‹
            print("ğŸ“¦ åŠ è½½ä¸ºå¸¸è§„æ¨¡å‹...")
            
            # å¤„ç†tokenizer (ç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶)
            tokenizer_kwargs = {"trust_remote_code": True}
                
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
            except Exception as e:
                print(f"âš ï¸ æ ‡å‡†tokenizeråŠ è½½å¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨å¤‡ç”¨tokenizeré€‰é¡¹...")
                tokenizer_kwargs["use_fast"] = False
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_kwargs)
            
            # é’ˆå¯¹Gemmaæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†
            model_name_lower = self.model_path.lower()
            special_kwargs = load_kwargs.copy()
            
            if "gemma" in model_name_lower:
                print("ğŸ” æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œåº”ç”¨ç‰¹æ®Šé…ç½®...")
                special_kwargs.update({
                    "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
                    "use_cache": False,  # ç¦ç”¨ç¼“å­˜æœºåˆ¶
                    "_attn_implementation_internal": "eager"
                })
                
            # åŠ è½½æ¨¡å‹ï¼Œç§»é™¤ä¸¥æ ¼çš„local_files_onlyé™åˆ¶
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **special_kwargs
            )
        
        # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {self.model_path}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"âŒ è¯¦ç»†é”™è¯¯:")
        traceback.print_exc()
        raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {self.model_path}: {str(e)}")

    def test_step(self, batch, batch_idx):
        """å•ä¸ªæµ‹è¯•æ­¥éª¤"""
        try:
            # è®¡ç®—æŸå¤±
            loss = self._compute_loss(batch)
            # è®¡ç®—å‡†ç¡®ç‡
            accuracy = self._compute_accuracy(batch)
            perplexity = torch.exp(loss)
            
            batch_size = len(batch)
            
            # è®°å½•æŒ‡æ ‡
            self.log('test/loss', loss, batch_size=batch_size)
            self.log('test/accuracy', accuracy, batch_size=batch_size)
            self.log('test/perplexity', perplexity, batch_size=batch_size)
            
            return {
                'loss': loss,
                'accuracy': accuracy,
                'perplexity': perplexity,
                'batch_size': batch_size
            }
        except Exception as e:
            print(f"âŒ test_stepå¤±è´¥ (batch_idx={batch_idx}): {e}")
            print(f"âŒ batchå†…å®¹: {batch}")
            traceback.print_exc()
            # è¿”å›é»˜è®¤å€¼é¿å…è®­ç»ƒä¸­æ–­
            return {
                'loss': torch.tensor(float('inf')),
                'accuracy': torch.tensor(0.0),
                'perplexity': torch.tensor(float('inf')),
                'batch_size': len(batch) if batch else 1
            }
        
    def _compute_loss(self, batch):
        """è®¡ç®—æŸå¤±"""
        try:
            inputs = []
            labels = []
            
            for item in batch:
                # å¤„ç†å¤šé€‰é¢˜æ ¼å¼
                if 'input' in item and 'options' in item:
                    question = item['input']
                    options = item['options']
                    target = item.get('target', 'A')
                    
                    # æ ¼å¼åŒ–é—®é¢˜ã€é€‰é¡¹å’Œç­”æ¡ˆ
                    text = f"Question: {question}\n"
                    for option in options:
                        text += f"{option}\n"
                    text += f"Answer: {target}"
                else:
                    # å¤‡é€‰ï¼šä½¿ç”¨ä»»ä½•æ–‡æœ¬å­—æ®µ
                    text = item.get('text', str(item))
                
                # Tokenize
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                inputs.append(encoding['input_ids'].squeeze())
                labels.append(encoding['input_ids'].squeeze())

            if inputs:
                input_ids = torch.stack(inputs).to(self.device)
                attention_mask = torch.ones_like(input_ids).to(self.device)
                labels = torch.stack(labels).to(self.device)
            else:
                return torch.tensor(0.0)
            
            # è®¡ç®—æŸå¤±
            with torch.no_grad():
                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            return outputs.loss
            
        except Exception as e:
            print(f"âŒ _compute_losså¤±è´¥: {e}")
            print(f"âŒ batchå¤§å°: {len(batch) if batch else 'None'}")
            if batch:
                print(f"âŒ ç¬¬ä¸€ä¸ªæ ·æœ¬: {batch[0] if len(batch) > 0 else 'Empty'}")
            traceback.print_exc()
            return torch.tensor(float('inf'))

    def _compute_accuracy(self, batch):
        """è®¡ç®—å‡†ç¡®ç‡"""
        if not isinstance(batch, list):
            return torch.tensor(0.25)  # 4é€‰1é¢˜çš„éšæœºåŸºçº¿
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for item in batch:
                try:
                    # è§£ææ•°æ®é¡¹
                    question = item.get('input', '')
                    options = item.get('options', [])
                    correct_answer = item.get('target', 'A')
                    
                    if not options:
                        total += 1
                        continue
                    
                    # æ ¼å¼åŒ–å¸¦é€‰é¡¹çš„é—®é¢˜
                    prompt = f"Question: {question}\n"
                    for option in options:
                        prompt += f"{option}\n"
                    prompt += "Answer:"
                    
                    # Tokenize
                    inputs = self.tokenizer(
                        prompt,
                        return_tensors='pt',
                        truncation=True,
                        max_length=self.max_length,
                        padding=True
                    ).to(self.device)
                    
                    # Gemmaæ¨¡å‹ç‰¹æ®Šå¤„ç†
                    model_name_lower = self.model_path.lower()
                    generation_kwargs = {
                        "max_new_tokens": 3,  # å‡å°‘ç”Ÿæˆé•¿åº¦
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False,  # ç¦ç”¨ç¼“å­˜
                        "output_attentions": False,
                        "output_hidden_states": False,
                    }
                    
                    if "gemma" in model_name_lower:
                        # Gemmaæ¨¡å‹ç‰¹æ®Šé€‚é…
                        generation_kwargs.update({
                            "temperature": 1.0,
                            "top_p": 1.0,
                            "repetition_penalty": 1.0,
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    outputs = self.model.generate(
                        **inputs,
                        **generation_kwargs
                    )
                    
                    # è§£ç ç”Ÿæˆçš„ç­”æ¡ˆ
                    generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                    generated_answer = generated_text.strip().upper()
                    
                    # æå–ç¬¬ä¸€ä¸ªå­—æ¯ (A, B, C, æˆ– D)
                    predicted_answer = None
                    for char in generated_answer:
                        if char in ['A', 'B', 'C', 'D']:
                            predicted_answer = char
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå°è¯•åŒ¹é…é€‰é¡¹å‰ç¼€
                    if predicted_answer is None:
                        for option in options:
                            if option.startswith('A:') and 'A' in generated_answer:
                                predicted_answer = 'A'
                            elif option.startswith('B:') and 'B' in generated_answer:
                                predicted_answer = 'B'
                            elif option.startswith('C:') and 'C' in generated_answer:
                                predicted_answer = 'C'
                            elif option.startswith('D:') and 'D' in generated_answer:
                                predicted_answer = 'D'
                            if predicted_answer:
                                break
                    
                    # ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ
                    if predicted_answer == correct_answer:
                        correct += 1
                    
                    total += 1
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†æ ·æœ¬é”™è¯¯: {e}")
                    total += 1
                    continue
        
        if total == 0:
            return torch.tensor(0.0)
        
        accuracy = correct / total
        return torch.tensor(accuracy)

    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨ - è¯„ä¼°æ¨¡å¼ä¸éœ€è¦ï¼Œä½†Lightningéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return None


def load_model_for_eval(model_path, device="auto", **kwargs):
    """åŠ è½½æ¨¡å‹å¹¶å‡†å¤‡å¥½è¯„ä¼°"""
    print(f"â³ åŠ è½½æ¨¡å‹ {model_path}...")
    
    # ç¡®å®šè®¾å¤‡
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    is_local_path = os.path.exists(model_path)
    
    try:
        # ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œé¿å…tokenizerè­¦å‘Šå’Œä¼˜åŒ–
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        
        # åŠ è½½tokenizer (ç‰¹åˆ«å¤„ç†æœ¬åœ°æ¨¡å‹è·¯å¾„)
        tokenizer_kwargs = {"trust_remote_code": True}
        if is_local_path:
            tokenizer_kwargs["local_files_only"] = True
            
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_kwargs)
        except Exception as e:
            print(f"è­¦å‘Š: æ ‡å‡†tokenizeråŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é¢„è®­ç»ƒtokenizer: {e}")
            tokenizer_kwargs["use_fast"] = False
            tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_kwargs)
        
        # æ¨¡å‹åç§°è½¬å°å†™ç”¨äºåˆ¤æ–­
        model_name_lower = model_path.lower() if isinstance(model_path, str) else ""
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®ä¸åŒçš„åŠ è½½å‚æ•°
        model_kwargs = {
            "torch_dtype": torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            "device_map": device,
            "trust_remote_code": True,
        }
        
        # æ·»åŠ æœ¬åœ°æ–‡ä»¶å‚æ•°
        if is_local_path:
            model_kwargs["local_files_only"] = True
        
        # Gemmaæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
        if "gemma" in model_name_lower:
            print("ğŸ¦™ æ£€æµ‹åˆ°Gemmaæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®ŠåŠ è½½è®¾ç½®")
            model_kwargs.update({
                "attn_implementation": "eager",  # é¿å…ä½¿ç”¨flash attention
            })
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºLoRAå¾®è°ƒæ¨¡å‹
        adapter_path = os.path.join(model_path, "adapter_config.json")
        if os.path.exists(adapter_path):
            print("ğŸ” æ£€æµ‹åˆ°LoRAé€‚é…å™¨é…ç½®")
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model_path = kwargs.get('base_model_path')
            if not base_model_path:
                # å°è¯•ä»adapter_config.jsonä¸­æ‰¾åˆ°åŸºç¡€æ¨¡å‹
                try:
                    with open(adapter_path, 'r') as f:
                        adapter_config = json.load(f)
                    base_model_path = adapter_config.get('base_model_name_or_path')
                    print(f"ğŸ“„ ä»adapter_config.jsonè·å–åˆ°åŸºç¡€æ¨¡å‹: {base_model_path}")
                except Exception as e:
                    raise ValueError(f"LoRAæ¨¡å‹éœ€è¦æŒ‡å®šbase_model_pathå‚æ•°: {e}")
            
            # æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„
            if not os.path.exists(base_model_path) and "/" not in base_model_path:
                # å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•autodl-tmpä¸­çš„å¸¸è§ä½ç½®
                for prefix in ["/root/autodl-tmp/models/", "/root/autodl-tmp/"]:
                    test_path = f"{prefix}{base_model_path}"
                    if os.path.exists(test_path):
                        base_model_path = test_path
                        print(f"ğŸ” å®šä½åˆ°åŸºç¡€æ¨¡å‹: {base_model_path}")
                        break
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print(f"ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            
            # ç‰¹æ®Šå¤„ç†æœ¬åœ°åŸºç¡€æ¨¡å‹
            base_kwargs = model_kwargs.copy()
            if os.path.exists(base_model_path):
                base_kwargs["local_files_only"] = True
                
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                **base_kwargs
            )
            
            # åŠ è½½LoRAæƒé‡
            print(f"ğŸ”„ åŠ è½½LoRAæƒé‡: {model_path}")
            model = PeftModel.from_pretrained(model, model_path)
        else:
            # ç›´æ¥åŠ è½½æ¨¡å‹
            print(f"ğŸ”„ åŠ è½½æ ‡å‡†æ¨¡å‹: {model_path}")
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs
            )
        
        # å°†æ¨¡å‹ç½®äºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        print(f"âœ… æ¨¡å‹ {model_path} åŠ è½½å®Œæˆ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        raise


def evaluate_models(
    models_list: List[str],
    dataset_name: str,
    output_dir: str = "eval/results",
    base_model_path: str = None,
    sample_ratio: float = 1.0,
    batch_size: int = 8
):
    """è¯„ä¼°å¤šä¸ªæ¨¡å‹å¹¶ä¿å­˜ç»“æœ"""
    print("\n" + "=" * 70)
    print(f"ğŸš€ Lightning æ‰¹é‡æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    
    # å‡†å¤‡è¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = get_test_file_path(dataset_name)
    test_dataset = SimpleDataset(test_file, sample_ratio=sample_ratio)
    
    print(f"ğŸ“ æ•°æ®é›†: {dataset_name}")
    print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"ğŸ“Š é‡‡æ ·æ¯”ä¾‹: {sample_ratio*100:.1f}%")
    
    results = {}
    start_time = time.time()
    
    # å‡†å¤‡å…±äº«æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ä»¥ç¡®ä¿å¯æ¯”æ€§
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False,  # Lightningæµ‹è¯•æ¨èæ‰“ä¹±é¡ºåº
        num_workers=2,  # å‡å°‘workeræ•°é‡ï¼Œé™ä½forkå¸¦æ¥çš„è­¦å‘Š
        pin_memory=True,
        collate_fn=lambda batch: batch,  # ä¿æŒæ‰¹æ¬¡æ ¼å¼ä¸å˜
        generator=torch.Generator().manual_seed(42),  # å›ºå®šéšæœºç§å­
        persistent_workers=True  # ä¿æŒworkeræŒç»­è¿è¡Œï¼Œé¿å…é¢‘ç¹fork
    )
    
    # è¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for i, model_path in enumerate(models_list):
        print(f"\n{'='*70}")
        print(f"ğŸ“Š [{i+1}/{len(models_list)}] è¯„ä¼°æ¨¡å‹: {model_path}")
        
        model_name = Path(model_path).name
        if not model_name:  # å¤„ç†è·¯å¾„æœ«å°¾çš„æ–œæ 
            model_name = Path(model_path).parent.name
            
        try:
            # åˆå§‹åŒ–Lightningè¯„ä¼°æ¨¡å—
            evaluator = LightningModelEvaluator(model_path, base_model_path)
            
            # åˆ›å»ºTrainer (æ— éœ€checkpoint) - é’ˆå¯¹Gemmaæ¨¡å‹ä¼˜åŒ–
            trainer_kwargs = {
                "accelerator": 'auto',
                "devices": 'auto',
                "precision": '16-mixed' if torch.cuda.is_available() else 32,
                "logger": False,
                "enable_checkpointing": False,  # è¯„ä¼°ä¸éœ€è¦æ£€æŸ¥ç‚¹
                "enable_model_summary": False,  # å…³é—­æ¨¡å‹æ‘˜è¦
                "enable_progress_bar": True,
                "deterministic": False,  # å¯¹Gemmaæ¨¡å‹ç¦ç”¨deterministic
                "num_sanity_val_steps": 0,  # é¿å…sanityæ£€æŸ¥
                "inference_mode": True,  # ä½¿ç”¨æ¨ç†æ¨¡å¼
                "benchmark": False,  # å…³é—­åŸºå‡†æµ‹è¯•
            }
            
            # å¦‚æœæ˜¯Gemmaæ¨¡å‹ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®
            if "gemma" in model_path.lower():
                trainer_kwargs.update({
                    "precision": 32,  # ä½¿ç”¨32ä½ç²¾åº¦é¿å…æ•°å€¼é—®é¢˜
                    "deterministic": False,  # å®Œå…¨ç¦ç”¨deterministic
                })
            
            trainer = Trainer(**trainer_kwargs)
            
            # æ‰§è¡Œæµ‹è¯•
            eval_start = time.time()
            test_results = trainer.test(evaluator, dataloaders=test_loader)
            eval_time = time.time() - eval_start
            
            # æ•´ç†ç»“æœ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡è€Œä¸æ˜¯Tensor
            model_results = {}
            if test_results and len(test_results) > 0:
                raw_results = test_results[0]
                # è½¬æ¢æ‰€æœ‰çš„tensorå€¼ä¸ºPythonæ ‡é‡
                for key, value in raw_results.items():
                    if hasattr(value, 'item'):
                        model_results[key] = value.item()
                    else:
                        model_results[key] = value
            
            # æ·»åŠ æ—¶é—´æŒ‡æ ‡
            model_results['eval_time_seconds'] = eval_time
            model_results['samples_per_second'] = len(test_dataset) / eval_time
            
            # æ·»åŠ åˆ°ç»“æœé›†
            results[model_name] = {
                dataset_name: model_results
            }
            
            # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
            result_file = output_path / f"{model_name}_{dataset_name}_evaluation_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(model_results, f, indent=4, ensure_ascii=False)
                
            print(f"âœ… è¯„ä¼°å®Œæˆ (ç”¨æ—¶: {eval_time:.1f}ç§’, {model_results['samples_per_second']:.1f} æ ·æœ¬/ç§’)")
            print(f"ğŸ“Š ç»“æœ:")
            print(f"  - Loss: {model_results.get('test/loss', 0):.4f}")
            print(f"  - Accuracy: {model_results.get('test/accuracy', 0):.4f}") 
            print(f"  - Perplexity: {model_results.get('test/perplexity', 0):.4f}")
            
            # æ¸…ç†å†…å­˜
            del evaluator
            del trainer
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            print(f"âŒ æ¨¡å‹è·¯å¾„: {model_path}")
            print(f"âŒ æ¨¡å‹åç§°: {model_name}")
            print(f"âŒ æ•°æ®é›†: {dataset_name}")
            print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            results[model_name] = {
                dataset_name: {"error": str(e)}
            }
    
    # è®¡ç®—æ€»ç”¨æ—¶
    total_time = time.time() - start_time
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_file = output_path / f"lightning_evaluation_summary_{timestamp}.json"
    
    summary_data = {
        "evaluation_summary": {
            "dataset": dataset_name,
            "total_models": len(models_list),
            "sample_ratio": sample_ratio,
            "batch_size": batch_size,
            "total_samples": len(test_dataset),
            "total_time_seconds": total_time,
            "timestamp": datetime.now().isoformat()
        },
        "results": results
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=4, ensure_ascii=False)
    
    # ä¿å­˜CSVæ ¼å¼ç»“æœ
    rows = []
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯Pythonæ ‡é‡
                loss_val = dataset_results.get('test/loss', 0)
                acc_val = dataset_results.get('test/accuracy', 0)
                ppl_val = dataset_results.get('test/perplexity', 0)
                time_val = dataset_results.get('eval_time_seconds', 0)
                samples_val = dataset_results.get('samples_per_second', 0)
                
                # è½¬æ¢tensor/numpyå€¼ä¸ºPythonæ ‡é‡
                if hasattr(loss_val, 'item'):
                    loss_val = float(loss_val.item())
                if hasattr(acc_val, 'item'):
                    acc_val = float(acc_val.item())
                if hasattr(ppl_val, 'item'):
                    ppl_val = float(ppl_val.item())
                if hasattr(time_val, 'item'):
                    time_val = float(time_val.item())
                if hasattr(samples_val, 'item'):
                    samples_val = float(samples_val.item())
                
                rows.append({
                    'Model': str(model_name),
                    'Dataset': str(dataset_name),
                    'Loss': round(float(loss_val), 4),
                    'Accuracy': round(float(acc_val), 4),
                    'Perplexity': round(float(ppl_val), 4),
                    'Eval_Time(s)': round(float(time_val), 1),
                    'Samples/Sec': round(float(samples_val), 1),
                    'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
    
    csv_file = None  # åˆå§‹åŒ–å˜é‡
    if rows:
        try:
            # æ‰‹åŠ¨å†™CSVæ–‡ä»¶ï¼Œé¿å…pandasé—®é¢˜
            csv_file = output_path / f"lightning_evaluation_results_{timestamp}.csv"
            
            # è·å–åˆ—å
            headers = ['Model', 'Dataset', 'Loss', 'Accuracy', 'Perplexity', 'Eval_Time(s)', 'Samples/Sec', 'Timestamp']
            
            with open(csv_file, 'w', encoding='utf-8', newline='') as f:
                f.write(','.join(headers) + '\n')
                for row in rows:
                    values = []
                    for header in headers:
                        val = row.get(header, '')
                        # ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²ï¼Œå¹¶å¤„ç†å¯èƒ½çš„é€—å·
                        if isinstance(val, str) and ',' in val:
                            val = f'"{val}"'
                        values.append(str(val))
                    f.write(','.join(values) + '\n')
            
            print(f"ï¿½ ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")
            
            # å°è¯•pandasæ–¹å¼ä½œä¸ºå¤‡é€‰
            try:
                # åªæœ‰åœ¨æ‰‹åŠ¨æ–¹å¼æˆåŠŸåæ‰å°è¯•pandas
                import pandas as pd
                df = pd.DataFrame(rows)
                cumulative_csv = output_path / "all_evaluation_results.csv"
                if cumulative_csv.exists():
                    existing_df = pd.read_csv(cumulative_csv, encoding='utf-8-sig')
                    # ç§»é™¤é‡å¤é¡¹
                    for _, row_data in df.iterrows():
                        mask = (existing_df['Model'] == row_data['Model']) & (existing_df['Dataset'] == row_data['Dataset'])
                        existing_df = existing_df[~mask]
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    combined_df.to_csv(cumulative_csv, index=False, encoding='utf-8-sig')
                else:
                    df.to_csv(cumulative_csv, index=False, encoding='utf-8-sig')
                print(f"ğŸ“ ç´¯ç§¯ç»“æœ: {cumulative_csv}")
            except Exception as pandas_error:
                print(f"âš ï¸ pandasç´¯ç§¯CSVæ›´æ–°å¤±è´¥: {pandas_error}")
                # ä¸å½±å“ä¸»è¦CSVæ–‡ä»¶çš„ä¿å­˜
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜CSVç»“æœå¤±è´¥: {e}")
            traceback.print_exc()
    
    # æ‰“å°æ±‡æ€»è¡¨æ ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"{'Model':<40} {'Dataset':<15} {'Loss':<8} {'Accuracy':<10} {'Perplexity':<12} {'Time(s)':<8} {'Samples/s':<10}")
    print("-" * 110)
    
    for model_name, model_results in results.items():
        for dataset_name, dataset_results in model_results.items():
            if 'error' not in dataset_results:
                print(f"{model_name:<40} {dataset_name:<15} "
                      f"{dataset_results.get('test/loss', 0):<8.4f} "
                      f"{dataset_results.get('test/accuracy', 0):<10.4f} "
                      f"{dataset_results.get('test/perplexity', 0):<12.4f} "
                      f"{dataset_results.get('eval_time_seconds', 0):<8.1f} "
                      f"{dataset_results.get('samples_per_second', 0):<10.1f}")
            else:
                print(f"{model_name:<40} {dataset_name:<15} {'ERROR':<8} {'ERROR':<10} {'ERROR':<12} {'ERROR':<8} {'ERROR':<10}")
    
    print("\n" + "=" * 80)
    print(f"â±ï¸  æ€»è¯„ä¼°æ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ æ±‡æ€»ç»“æœ: {summary_file}")
    if csv_file:
        print(f"ğŸ“Š CSVç»“æœ: {csv_file}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Lightningé£æ ¼çš„å¿«é€Ÿæ¨¡å‹è¯„ä¼°å·¥å…·")
    parser.add_argument("--models_list", type=str, nargs="+", required=True,
                       help="è¦è¯„ä¼°çš„æ¨¡å‹è·¯å¾„åˆ—è¡¨")
    parser.add_argument("--dataset", type=str, default="arc-challenge",
                       help="æ•°æ®é›†åç§° (é»˜è®¤: arc-challenge)")
    parser.add_argument("--output_dir", type=str, default="eval/results",
                       help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: eval/results)")
    parser.add_argument("--base_model", type=str, default=None,
                       help="æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œç”¨äºåŠ è½½LoRAæ¨¡å‹ (å¯é€‰)")
    parser.add_argument("--sample_ratio", type=float, default=1.0,
                       help="æ•°æ®é‡‡æ ·æ¯”ä¾‹ï¼ŒåŠ é€Ÿè¯„ä¼° (é»˜è®¤: 1.0 = 100%)")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 8)")
    
    args = parser.parse_args()
    
    print("ğŸ”¬ Lightningæ¨¡å‹è¯„ä¼°å·¥å…·")
    print("=" * 50)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    valid_models = []
    for model_path in args.models_list:
        if os.path.exists(model_path):
            valid_models.append(model_path)
        else:
            print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            print(f"å°†å°è¯•ä½œä¸ºHuggingFaceæ¨¡å‹åç§°åŠ è½½")
            valid_models.append(model_path)  # ä»ç„¶æ·»åŠ ï¼Œè®©ä¸‹æ¸¸å¤„ç†
    
    if len(valid_models) == 0:
        print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹è·¯å¾„")
        return False
    
    # æ£€æµ‹LoRAæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹
    lora_models = []
    for model_path in valid_models:
        if os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "adapter_config.json")):
            lora_models.append(model_path)
    
    if lora_models and not args.base_model:
        print(f"â„¹ï¸ æ£€æµ‹åˆ°{len(lora_models)}ä¸ªLoRAæ¨¡å‹:")
        for lora in lora_models:
            print(f"  - {lora}")
        print(f"ğŸ’¡ å¦‚æœåŠ è½½å¤±è´¥ï¼Œè¯·ä½¿ç”¨ --base_model å‚æ•°æŒ‡å®šåŸºç¡€æ¨¡å‹")
    
    try:
        # è¿è¡Œè¯„ä¼°
        results = evaluate_models(
            models_list=valid_models,
            dataset_name=args.dataset,
            output_dir=args.output_dir,
            base_model_path=args.base_model,
            sample_ratio=args.sample_ratio,
            batch_size=args.batch_size
        )
        
        print("âœ… è¯„ä¼°å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
