#!/usr/bin/env python3
"""
æ•°æ®é›†Checkpointç®¡ç†å™¨
ç”¨äºæŒ‰ç…§æ•°æ®é›†ç®¡ç†å’ŒæŸ¥æ‰¾æœ€æ–°çš„checkpoint

è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå¸®åŠ©ä½ ç®¡ç†æŒ‰æ•°æ®é›†ç»„ç»‡çš„è®­ç»ƒå®éªŒç»“æœã€‚
æ–°çš„æ–‡ä»¶ç»“æ„: experiments/cs/{dataset_name}/{timestamp_lora}/

ä¸»è¦åŠŸèƒ½:
1. æŸ¥çœ‹æ‰€æœ‰æ•°æ®é›†çš„è®­ç»ƒçŠ¶æ€æ¦‚è§ˆ
2. æŸ¥çœ‹ç‰¹å®šæ•°æ®é›†çš„è¯¦ç»†å®éªŒä¿¡æ¯
3. å¿«é€Ÿè·å–æœ€æ–°checkpointè·¯å¾„(ç”¨äºç»§ç»­è®­ç»ƒæˆ–æ¨ç†)
4. å¿«é€Ÿè·å–æœ€ç»ˆæ¨¡å‹è·¯å¾„(ç”¨äºéƒ¨ç½²æˆ–è¯„ä¼°)

ä½¿ç”¨ç¤ºä¾‹:
# æŸ¥çœ‹æ‰€æœ‰æ•°æ®é›†çš„æ¦‚è§ˆ
python scripts/dataset_checkpoint_manager.py

# æŸ¥çœ‹arc-challengeæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
python scripts/dataset_checkpoint_manager.py --dataset arc-challenge

# è·å–arc-challengeçš„æœ€æ–°checkpointè·¯å¾„
python scripts/dataset_checkpoint_manager.py --latest-checkpoint arc-challenge

# è·å–arc-challengeçš„æœ€ç»ˆè®­ç»ƒæ¨¡å‹è·¯å¾„  
python scripts/dataset_checkpoint_manager.py --latest-model arc-challenge

# åœ¨è„šæœ¬ä¸­ä½¿ç”¨(è·å–æ¨¡å‹è·¯å¾„ç”¨äºæ¨ç†)
MODEL_PATH=$(python scripts/dataset_checkpoint_manager.py --latest-model arc-challenge)
python inference.py --model_path $MODEL_PATH --input test.jsonl

åº”ç”¨åœºæ™¯:
- å½“ä½ è®­ç»ƒäº†å¤šä¸ªæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†æœ‰å¤šæ¬¡å®éªŒæ—¶
- éœ€è¦å¿«é€Ÿæ‰¾åˆ°æŸä¸ªæ•°æ®é›†çš„æœ€ä½³/æœ€æ–°æ¨¡å‹
- éœ€è¦ç»§ç»­ä¹‹å‰ä¸­æ–­çš„è®­ç»ƒ(ä½¿ç”¨æœ€æ–°checkpoint)
- éœ€è¦æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†çš„è®­ç»ƒç»“æœ
"""

import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import argparse


class DatasetCheckpointManager:
    """æŒ‰æ•°æ®é›†ç®¡ç†checkpointçš„å·¥å…·ç±»"""
    
    def __init__(self, experiments_root: str = "./experiments"):
        self.experiments_root = Path(experiments_root)
        self.cs_dir = self.experiments_root / "cs"
        
    def list_available_datasets(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        if not self.cs_dir.exists():
            return []
            
        datasets = []
        for item in self.cs_dir.iterdir():
            if item.is_dir():
                datasets.append(item.name)
        return sorted(datasets)
    
    def list_experiments_for_dataset(self, dataset_name: str) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰å®éªŒ"""
        dataset_dir = self.cs_dir / dataset_name
        if not dataset_dir.exists():
            return []
        
        experiments = []
        for exp_dir in dataset_dir.iterdir():
            if exp_dir.is_dir():
                metadata_file = exp_dir / "metadata.json"
                if metadata_file.exists():
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        # æ£€æŸ¥checkpointå’Œmodelæ–‡ä»¶
                        checkpoints_dir = exp_dir / "checkpoints"
                        models_dir = exp_dir / "models"
                        
                        checkpoint_count = 0
                        if checkpoints_dir.exists():
                            checkpoint_count = len(list(checkpoints_dir.glob("checkpoint_*")))
                        
                        has_final_model = False
                        if models_dir.exists():
                            has_final_model = any(models_dir.glob("*.safetensors")) or any(models_dir.glob("pytorch_model.bin"))
                        
                        experiments.append({
                            "name": exp_dir.name,
                            "path": str(exp_dir),
                            "created_at": metadata.get("created_at"),
                            "status": metadata.get("status", "unknown"),
                            "description": metadata.get("description", ""),
                            "checkpoint_count": checkpoint_count,
                            "has_final_model": has_final_model,
                            "checkpoints_dir": str(checkpoints_dir) if checkpoints_dir.exists() else None,
                            "models_dir": str(models_dir) if models_dir.exists() else None
                        })
                    except Exception as e:
                        print(f"âš ï¸ è¯»å–å®éªŒå…ƒæ•°æ®å¤±è´¥ {exp_dir}: {e}")
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        experiments.sort(key=lambda x: x["created_at"] if x["created_at"] else "", reverse=True)
        return experiments
    
    def get_latest_experiment(self, dataset_name: str) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ•°æ®é›†çš„æœ€æ–°å®éªŒ"""
        experiments = self.list_experiments_for_dataset(dataset_name)
        if experiments:
            return experiments[0]  # å·²æŒ‰æ—¶é—´æ’åºï¼Œç¬¬ä¸€ä¸ªæ˜¯æœ€æ–°çš„
        return None
    
    def get_latest_checkpoint_path(self, dataset_name: str) -> Optional[str]:
        """è·å–æŒ‡å®šæ•°æ®é›†æœ€æ–°å®éªŒçš„æœ€æ–°checkpointè·¯å¾„"""
        latest_exp = self.get_latest_experiment(dataset_name)
        if not latest_exp or not latest_exp["checkpoints_dir"]:
            return None
        
        checkpoints_dir = Path(latest_exp["checkpoints_dir"])
        if not checkpoints_dir.exists():
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
        checkpoints = list(checkpoints_dir.glob("checkpoint_*"))
        if not checkpoints:
            return None
        
        # æŒ‰æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
        checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return str(checkpoints[0])
    
    def get_final_model_path(self, dataset_name: str) -> Optional[str]:
        """è·å–æŒ‡å®šæ•°æ®é›†æœ€æ–°å®éªŒçš„æœ€ç»ˆæ¨¡å‹è·¯å¾„"""
        latest_exp = self.get_latest_experiment(dataset_name)
        if not latest_exp or not latest_exp["models_dir"]:
            return None
        
        models_dir = Path(latest_exp["models_dir"])
        if not models_dir.exists():
            return None
        
        # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_files = list(models_dir.glob("*.safetensors")) + list(models_dir.glob("pytorch_model.bin"))
        if model_files:
            return str(models_dir)  # è¿”å›æ¨¡å‹ç›®å½•è·¯å¾„
        
        return None
    
    def print_dataset_summary(self, dataset_name: str):
        """æ‰“å°æŒ‡å®šæ•°æ®é›†çš„æ‘˜è¦ä¿¡æ¯"""
        experiments = self.list_experiments_for_dataset(dataset_name)
        
        print(f"\nğŸ“Š æ•°æ®é›†: {dataset_name}")
        print(f"{'=' * 50}")
        
        if not experiments:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å®éªŒ")
            return
        
        print(f"ğŸ“ˆ å®éªŒæ€»æ•°: {len(experiments)}")
        
        latest = experiments[0]
        print(f"\nğŸ† æœ€æ–°å®éªŒ:")
        print(f"  åç§°: {latest['name']}")
        print(f"  çŠ¶æ€: {latest['status']}")
        print(f"  åˆ›å»ºæ—¶é—´: {latest['created_at']}")
        print(f"  Checkpointæ•°: {latest['checkpoint_count']}")
        print(f"  æœ‰æœ€ç»ˆæ¨¡å‹: {'âœ…' if latest['has_final_model'] else 'âŒ'}")
        
        # æœ€æ–°checkpointè·¯å¾„
        latest_ckpt = self.get_latest_checkpoint_path(dataset_name)
        if latest_ckpt:
            print(f"  æœ€æ–°Checkpoint: {latest_ckpt}")
        
        # æœ€ç»ˆæ¨¡å‹è·¯å¾„
        final_model = self.get_final_model_path(dataset_name)
        if final_model:
            print(f"  æœ€ç»ˆæ¨¡å‹ç›®å½•: {final_model}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰å®éªŒ:")
        for exp in experiments:
            status_emoji = {"completed": "âœ…", "failed": "âŒ", "running": "ğŸ”„"}.get(exp["status"], "â“")
            print(f"  {status_emoji} {exp['name']} ({exp['status']}) - {exp['checkpoint_count']}ä¸ªcheckpoints")
    
    def print_all_datasets_summary(self):
        """æ‰“å°æ‰€æœ‰æ•°æ®é›†çš„æ‘˜è¦"""
        datasets = self.list_available_datasets()
        
        print(f"\nğŸ—‚ï¸ æ‰€æœ‰æ•°æ®é›†æ‘˜è¦")
        print(f"{'=' * 70}")
        
        if not datasets:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®é›†å®éªŒ")
            return
        
        print(f"ğŸ“ˆ æ•°æ®é›†æ€»æ•°: {len(datasets)}")
        print()
        
        for dataset in datasets:
            experiments = self.list_experiments_for_dataset(dataset)
            if experiments:
                latest = experiments[0]
                status_emoji = {"completed": "âœ…", "failed": "âŒ", "running": "ğŸ”„"}.get(latest["status"], "â“")
                print(f"{status_emoji} {dataset:<15} | {len(experiments)}ä¸ªå®éªŒ | æœ€æ–°: {latest['name']} ({latest['status']})")
            else:
                print(f"â“ {dataset:<15} | æ— æœ‰æ•ˆå®éªŒ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®é›†Checkpointç®¡ç†å™¨")
    parser.add_argument("--dataset", type=str, help="æŸ¥çœ‹æŒ‡å®šæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--latest-checkpoint", type=str, help="è·å–æŒ‡å®šæ•°æ®é›†çš„æœ€æ–°checkpointè·¯å¾„")
    parser.add_argument("--latest-model", type=str, help="è·å–æŒ‡å®šæ•°æ®é›†çš„æœ€ç»ˆæ¨¡å‹è·¯å¾„")
    parser.add_argument("--experiments-root", type=str, default="./experiments", help="å®éªŒæ ¹ç›®å½•")
    
    args = parser.parse_args()
    
    manager = DatasetCheckpointManager(args.experiments_root)
    
    if args.list:
        manager.print_all_datasets_summary()
    elif args.dataset:
        manager.print_dataset_summary(args.dataset)
    elif args.latest_checkpoint:
        path = manager.get_latest_checkpoint_path(args.latest_checkpoint)
        if path:
            print(path)
        else:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›† {args.latest_checkpoint} çš„checkpoint")
    elif args.latest_model:
        path = manager.get_final_model_path(args.latest_model)
        if path:
            print(path)
        else:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›† {args.latest_model} çš„æœ€ç»ˆæ¨¡å‹")
    else:
        # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æ•°æ®é›†æ‘˜è¦
        manager.print_all_datasets_summary()


if __name__ == "__main__":
    main()
