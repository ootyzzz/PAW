#!/usr/bin/env python3
"""
æè‡´åŠ é€Ÿç‰ˆLoRAè¿ç§»è„šæœ¬ - Turbo Mode
ä¼˜åŒ–ç­–ç•¥:
1. æ‰¹é‡SVDè®¡ç®—ï¼Œå‡å°‘GPUå†…å­˜ä¼ è¾“
2. ç®€åŒ–ç›¸ä¼¼åº¦è®¡ç®—ï¼Œä½¿ç”¨å¿«é€Ÿè¿‘ä¼¼æ–¹æ³•
3. ç§»é™¤è¯¦ç»†æ—¥å¿—ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
4. å¹¶è¡Œå¤„ç†å¤šä¸ªå±‚
5. å†…å­˜ä¼˜åŒ–ï¼ŒåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡

python /root/PAW/lora_adapter/lora_shift_turbo.py --source_lora /root/autodl-tmp/loraed/Qwen2.5-7B-Instruct/250719_004518/final_model --target_model /root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct --batch_size 8

"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch
import torch.nn.functional as F
from concurrent.futures import ThreadPoolExecutor
import gc

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from lora_x_core import LoRAXCore
from model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®ç®€åŒ–æ—¥å¿—
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")


def infer_source_model_path(lora_path: str) -> str:
    """æ ¹æ®LoRAè·¯å¾„æ¨æ–­æºæ¨¡å‹è·¯å¾„"""
    lora_path = Path(lora_path)
    
    if lora_path.name == "final_model":
        model_name = lora_path.parent.parent.name  
    else:
        model_name = lora_path.parent.name  
    
    source_model_path = f"/root/autodl-tmp/models/{model_name}"
    return source_model_path


class TurboLoRAXCore(LoRAXCore):
    """æè‡´åŠ é€Ÿç‰ˆLoRA-Xæ ¸å¿ƒç±»"""
    
    def __init__(self, rank=128, similarity_threshold=0.002, batch_size=16):
        super().__init__(rank, similarity_threshold)
        self.batch_size = batch_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.cpu_device = torch.device('cpu')
        
    def stream_similarity_compute(self, valid_layers: list, source_base_weights: dict, target_base_weights: dict) -> dict:
        """æ‰¹é‡å¹¶è¡Œè®¡ç®—ç›¸ä¼¼åº¦"""
        similarities = {}
        total_layers = len(valid_layers)
        start_time = datetime.now()
        
        print(f"âš¡ è®¡ç®—{total_layers}å±‚ç›¸ä¼¼åº¦ (batch_size={self.batch_size})")
        
        # æŒ‰batch_sizeåˆ†æ‰¹å¤„ç†
        for batch_start in range(0, total_layers, self.batch_size):
            batch_start_time = datetime.now()
            batch_end = min(batch_start + self.batch_size, total_layers)
            batch_layers = valid_layers[batch_start:batch_end]
            
            # é¢„åŠ è½½batchæƒé‡åˆ°GPU
            batch_data = []
            for lora_key, base_key in batch_layers:
                try:
                    source_weight = source_base_weights[base_key].to(self.device).float()
                    target_weight = target_base_weights[base_key].to(self.device).float()
                    batch_data.append({
                        'source_weight': source_weight,
                        'target_weight': target_weight,
                        'base_key': base_key
                    })
                except Exception as e:
                    similarities[base_key] = 0.0
            
            # å¹¶è¡ŒSVDè®¡ç®—
            if batch_data:
                try:
                    from concurrent.futures import ThreadPoolExecutor
                    
                    def compute_similarity_on_gpu(data_item):
                        try:
                            source_weight = data_item['source_weight']
                            target_weight = data_item['target_weight']
                            
                            U_s, _, _ = self.compute_svd_subspace(source_weight)
                            U_t, _, _ = self.compute_svd_subspace(target_weight)
                            similarity = self.compute_subspace_similarity(U_s, U_t)
                            
                            del U_s, U_t
                            return similarity
                        except Exception as e:
                            return 0.0
                    
                    max_workers = min(len(batch_data), 8)
                    with ThreadPoolExecutor(max_workers=max_workers) as executor:
                        futures = [executor.submit(compute_similarity_on_gpu, data_item) for data_item in batch_data]
                        
                        for i, future in enumerate(futures):
                            try:
                                similarity = future.result(timeout=60)
                                similarities[batch_data[i]['base_key']] = similarity
                            except Exception as e:
                                similarities[batch_data[i]['base_key']] = 0.0
                
                except Exception as e:
                    # å›é€€åˆ°ä¸²è¡Œè®¡ç®—
                    for data_item in batch_data:
                        try:
                            source_weight = data_item['source_weight']
                            target_weight = data_item['target_weight']
                            base_key = data_item['base_key']
                            
                            U_s, _, _ = self.compute_svd_subspace(source_weight)
                            U_t, _, _ = self.compute_svd_subspace(target_weight)
                            similarity = self.compute_subspace_similarity(U_s, U_t)
                            similarities[base_key] = similarity
                            
                            del U_s, U_t
                        except:
                            similarities[base_key] = 0.0
            
            # æ¸…ç†batchæ•°æ®
            del batch_data
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
            gc.collect()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            batch_end_time = datetime.now()
            batch_duration = (batch_end_time - batch_start_time).total_seconds()
            batch_size_actual = len(batch_layers)
            items_per_sec = batch_size_actual / batch_duration if batch_duration > 0 else 0
            
            # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
            completed = batch_end
            progress_pct = (completed / total_layers) * 100
            elapsed_total = (batch_end_time - start_time).total_seconds()
            
            if completed > 0:
                avg_time_per_item = elapsed_total / completed
                remaining_items = total_layers - completed
                eta_seconds = remaining_items * avg_time_per_item
                eta_minutes = eta_seconds / 60
            else:
                eta_minutes = 0
            
            print(f"   [{completed:3d}/{total_layers}] {progress_pct:5.1f}% | {items_per_sec:.1f} items/s | å·²ç”¨æ—¶ {elapsed_total/60:.1f}m | é¢„è®¡å‰©ä½™ {eta_minutes:.1f}m")
        
        total_time = (datetime.now() - start_time).total_seconds()
        avg_speed = total_layers / total_time if total_time > 0 else 0
        print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ | æ€»ç”¨æ—¶ {total_time/60:.1f}m | å¹³å‡é€Ÿåº¦ {avg_speed:.1f} items/s")
        
        return similarities

    def transfer_lora_weights(self,
                            source_lora: dict,
                            target_base_weights: dict,
                            source_base_weights: dict) -> dict:
        """æè‡´åŠ é€Ÿçš„LoRAæƒé‡è¿ç§»"""
        transferred_lora = {}
        stats = {
            'total_layers': 0,
            'transferred_layers': 0,
            'skipped_layers': 0,
            'transferred_list': [],
            'skipped_list': [],
            'skipped_reasons': {},
            'similarity_stats': [],
            'layer_types': {},
            'processing_times': []
        }
        
        print(f"ğŸš€ Turboæ¨¡å¼å¯åŠ¨ - å¤„ç†{len(source_lora)}ä¸ªLoRAæƒé‡")
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„å±‚
        valid_layers = []
        for lora_key in source_lora.keys():
            if not lora_key.endswith('.weight'):
                continue
            
            base_key = self._map_lora_to_base_key(lora_key)
            if base_key in source_base_weights and base_key in target_base_weights:
                valid_layers.append((lora_key, base_key))
                stats['total_layers'] += 1
                
                # ç»Ÿè®¡å±‚ç±»å‹
                layer_type = self._classify_layer_type(lora_key)
                if layer_type not in stats['layer_types']:
                    stats['layer_types'][layer_type] = {'total': 0, 'transferred': 0}
                stats['layer_types'][layer_type]['total'] += 1
        
        print(f"ğŸ“Š æœ‰æ•ˆå±‚æ•°: {len(valid_layers)}")
        
        # æµå¼è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self.stream_similarity_compute(valid_layers, source_base_weights, target_base_weights)
        
        # æµå¼å¤„ç†è¿ç§»
        print("âš¡ æµå¼è¿ç§»æƒé‡...")
        
        for i, (lora_key, base_key) in enumerate(valid_layers):
            if i % 50 == 0:
                print(f"   è¿ç§»è¿›åº¦: {i}/{len(valid_layers)}")
            
            similarity = similarities.get(base_key, 0.0)
            layer_type = self._classify_layer_type(lora_key)
            
            stats['similarity_stats'].append({
                'layer': lora_key,
                'similarity': similarity,
                'layer_type': layer_type
            })
            
            # ç›¸ä¼¼åº¦è¿‡æ»¤
            if similarity < self.similarity_threshold:
                reason = f"ç›¸ä¼¼åº¦è¿‡ä½ ({similarity:.6f} < {self.similarity_threshold})"
                stats['skipped_layers'] += 1
                stats['skipped_list'].append(lora_key)
                stats['skipped_reasons'][lora_key] = reason
                continue
            
            try:
                start_time = datetime.now()
                
                # åªåœ¨GPUä¸Šè¿›è¡Œè¿ç§»è®¡ç®—
                source_base = source_base_weights[base_key].to(self.device)
                target_base = target_base_weights[base_key].to(self.device)
                lora_weight = source_lora[lora_key].to(self.device)
                
                # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
                if not self._check_dimension_compatibility(source_base, target_base, lora_weight):
                    transferred_weight = self._frobenius_projection(lora_weight, source_base, target_base)
                else:
                    transferred_weight = self._transfer_single_layer(lora_weight, source_base, target_base)
                
                transferred_lora[lora_key] = transferred_weight.cpu()
                stats['transferred_layers'] += 1
                stats['transferred_list'].append(lora_key)
                stats['layer_types'][layer_type]['transferred'] += 1
                
                # è®°å½•å¤„ç†æ—¶é—´
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                stats['processing_times'].append(processing_time)
                
                # åŠæ—¶æ¸…ç†GPUå†…å­˜
                del source_base, target_base, lora_weight, transferred_weight
                
            except Exception as e:
                reason = f"è¿ç§»å¤±è´¥: {str(e)}"
                stats['skipped_layers'] += 1
                stats['skipped_list'].append(lora_key)
                stats['skipped_reasons'][lora_key] = reason
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if i % 5 == 0:
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                gc.collect()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if stats['processing_times']:
            stats['avg_processing_time'] = sum(stats['processing_times']) / len(stats['processing_times'])
            stats['total_processing_time'] = sum(stats['processing_times'])
        
        self._print_detailed_stats(stats)
        return transferred_lora, stats

    def _print_detailed_stats(self, stats: dict):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'ğŸ‰'*20} Turboè¿ç§»å®Œæˆç»Ÿè®¡ {'ğŸ‰'*20}")
        print(f"{'='*80}")
        print(f"ğŸ“Š æ€»å±‚æ•°: {stats['total_layers']}")
        print(f"âœ… æˆåŠŸè¿ç§»: {stats['transferred_layers']}")
        print(f"âŒ è·³è¿‡å±‚æ•°: {stats['skipped_layers']}")
        
        if stats['transferred_layers'] > 0:
            success_rate = (stats['transferred_layers'] / stats['total_layers']) * 100
            print(f"ğŸ“ˆ è¿ç§»æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æŒ‰å±‚ç±»å‹ç»Ÿè®¡
        if stats['layer_types']:
            print(f"\nğŸ“‹ æŒ‰å±‚ç±»å‹ç»Ÿè®¡:")
            for layer_type, type_stats in stats['layer_types'].items():
                total = type_stats['total']
                transferred = type_stats['transferred']
                rate = (transferred / total * 100) if total > 0 else 0
                print(f"  {layer_type:12s}: {transferred:2d}/{total:2d} ({rate:5.1f}%)")
        
        # ç›¸ä¼¼åº¦ç»Ÿè®¡
        if stats['similarity_stats']:
            similarities = [s['similarity'] for s in stats['similarity_stats']]
            print(f"\nğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡ç›¸ä¼¼åº¦: {sum(similarities)/len(similarities):.6f}")
            print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.6f}")
            print(f"  æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.6f}")
        
        # æ€§èƒ½ç»Ÿè®¡
        if 'avg_processing_time' in stats:
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}ç§’/å±‚")
            print(f"  æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.1f}ç§’")
        
        print(f"{'='*80}")

    def _fast_svd_subspace(self, weight_matrix: torch.Tensor) -> torch.Tensor:
        """å¿«é€ŸSVDå­ç©ºé—´è®¡ç®— - ä½¿ç”¨æˆªæ–­SVD"""
        # ä½¿ç”¨æ›´å°çš„rankè¿›è¡Œå¿«é€Ÿè®¡ç®—
        fast_rank = min(self.rank // 2, 64)
        try:
            U, _, _ = torch.svd_lowrank(weight_matrix.float(), q=fast_rank)
            return U[:, :fast_rank]
        except:
            # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
            return super().compute_svd_subspace(weight_matrix)[0]


def main():
    parser = argparse.ArgumentParser(description="æè‡´åŠ é€Ÿç‰ˆLoRAè¿ç§»è„šæœ¬")
    parser.add_argument("--source_lora", type=str, required=True,
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, required=True,
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str, 
                       default="/root/autodl-tmp/shifted/turbo",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--rank", type=int, default=64,
                       help="SVDæˆªæ–­ç§© (é™ä½ä»¥æé€Ÿ)")
    parser.add_argument("--similarity_threshold", type=float, default=0.002,
                       help="ç›¸ä¼¼æ€§é˜ˆå€¼")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œè¾“å‡ºè·¯å¾„
    timestamp = generate_timestamp()
    output_path = os.path.join(args.output_base, timestamp)
    
    # æ¨æ–­æºæ¨¡å‹è·¯å¾„
    source_model_path = infer_source_model_path(args.source_lora)
    
    print(f"ğŸš€ TURBO LoRAè¿ç§» - æè‡´åŠ é€Ÿæ¨¡å¼")
    print(f"ğŸ“‚ æºLoRA: {args.source_lora}")
    print(f"ğŸ“‚ æºæ¨¡å‹: {source_model_path}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡º: {output_path}")
    print(f"âš™ï¸ å‚æ•°: rank={args.rank}, threshold={args.similarity_threshold}, batch={args.batch_size}")
    
    try:
        # æ£€æŸ¥è·¯å¾„
        for path in [args.source_lora, source_model_path, args.target_model]:
            if not os.path.exists(path):
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
                return False
        
        # åˆå§‹åŒ–ç»„ä»¶
        lora_x = TurboLoRAXCore(
            rank=args.rank, 
            similarity_threshold=args.similarity_threshold,
            batch_size=args.batch_size
        )
        loader = ModelWeightLoader()
        
        # åŠ è½½æƒé‡
        print("ğŸ“¥ åŠ è½½LoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)
        
        print("ğŸ“¥ åŠ è½½æºæ¨¡å‹æƒé‡...")
        source_base_weights = loader.load_base_model_weights(source_model_path)
        
        print("ğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)
        
        # æ‰§è¡Œè¿ç§»
        start_time = datetime.now()
        transferred_lora, stats = lora_x.transfer_lora_weights(
            source_lora=source_lora_weights,
            target_base_weights=target_base_weights,
            source_base_weights=source_base_weights
        )
        end_time = datetime.now()
        
        if not transferred_lora:
            print("âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚")
            return False
        
        # ä¿å­˜ç»“æœ
        os.makedirs(output_path, exist_ok=True)
        save_transferred_lora(transferred_lora, lora_config, output_path)
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡æ—¥å¿—
        duration = (end_time - start_time).total_seconds()
        stats_file = os.path.join(output_path, "transfer_stats.json")
        
        # å‡†å¤‡ç»Ÿè®¡æ•°æ®
        detailed_stats = {
            "timestamp": timestamp,
            "total_duration_seconds": duration,
            "source_lora_path": args.source_lora,
            "target_model_path": args.target_model,
            "output_path": output_path,
            "parameters": {
                "rank": args.rank,
                "similarity_threshold": args.similarity_threshold,
                "batch_size": args.batch_size
            },
            "results": {
                "total_layers": stats['total_layers'],
                "transferred_layers": stats['transferred_layers'],
                "skipped_layers": stats['skipped_layers'],
                "success_rate": (stats['transferred_layers'] / stats['total_layers'] * 100) if stats['total_layers'] > 0 else 0
            },
            "layer_types": stats['layer_types'],
            "similarity_statistics": {
                "similarities": [s['similarity'] for s in stats['similarity_stats']],
                "avg_similarity": sum(s['similarity'] for s in stats['similarity_stats']) / len(stats['similarity_stats']) if stats['similarity_stats'] else 0,
                "max_similarity": max(s['similarity'] for s in stats['similarity_stats']) if stats['similarity_stats'] else 0,
                "min_similarity": min(s['similarity'] for s in stats['similarity_stats']) if stats['similarity_stats'] else 0
            },
            "transferred_layers_list": stats['transferred_list'],
            "skipped_layers_details": {
                "layers": stats['skipped_list'],
                "reasons": stats['skipped_reasons']
            }
        }
        
        # ä¿å­˜JSONç»Ÿè®¡æ–‡ä»¶
        import json
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ‰ Turboè¿ç§»å®Œæˆï¼ç”¨æ—¶: {duration:.1f}ç§’")
        print(f"ğŸ“‚ ç»“æœ: {output_path}")
        print(f"ğŸ“Š è¯¦ç»†ç»Ÿè®¡: {stats_file}")
        return True
        
    except Exception as e:
        print(f"âŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)