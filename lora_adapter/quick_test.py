#!/usr/bin/env python3
"""
LoRA-Xå®é™…åŠŸèƒ½æµ‹è¯•
"""

import sys
import logging
from pathlib import Path

# è®¾ç½®è·¯å¾„
lora_adapter_dir = Path(__file__).parent
src_dir = lora_adapter_dir / "src"
sys.path.insert(0, str(src_dir))

import torch
from model_utils import ModelWeightLoader

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

def test_lora_loading():
    """æµ‹è¯•LoRAæƒé‡åŠ è½½"""
    print("ğŸ” æµ‹è¯•LoRAæƒé‡åŠ è½½...")
    
    loader = ModelWeightLoader()
    source_lora_path = "../train_lora/runs/Qwen_Qwen2.5-1.5B/arc-challenge_lora_20250723_191421/final_model"
    
    try:
        lora_weights, config = loader.load_lora_weights(source_lora_path)
        
        print(f"âœ… æˆåŠŸåŠ è½½LoRA: {len(lora_weights)}ä¸ªå‚æ•°")
        print(f"âœ… é…ç½®: rank={config.get('r')}, alpha={config.get('lora_alpha')}")
        
        # æ˜¾ç¤ºæƒé‡ä¿¡æ¯
        print("\\nğŸ“Š æƒé‡è¯¦æƒ…:")
        for i, (key, weight) in enumerate(lora_weights.items()):
            print(f"  {key}: {weight.shape} ({weight.dtype})")
            if i >= 3:
                print(f"  ... (+{len(lora_weights)-4}ä¸ªæ›´å¤šæƒé‡)")
                break
        
        return lora_weights, config
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_simple_transfer():
    """æµ‹è¯•ç®€å•çš„è¿ç§»é€»è¾‘"""
    print("\\nğŸ”¬ æµ‹è¯•æ ¸å¿ƒè¿ç§»é€»è¾‘...")
    
    from lora_x_core import LoRAXCore
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("åˆ›å»ºæµ‹è¯•æƒé‡çŸ©é˜µ...")
    source_weight = torch.randn(64, 128, dtype=torch.float32)
    target_weight = torch.randn(64, 128, dtype=torch.float32)
    lora_delta = torch.randn(64, 128, dtype=torch.float32) * 0.1  # å°çš„å˜åŒ–
    
    lora_x = LoRAXCore(rank=32, similarity_threshold=0.1)
    
    try:
        # æµ‹è¯•SVDåˆ†è§£
        U_s, S_s, Vh_s = lora_x.compute_svd_subspace(source_weight)
        U_t, S_t, Vh_t = lora_x.compute_svd_subspace(target_weight)
        
        print(f"âœ… SVDåˆ†è§£: U_s={U_s.shape}, U_t={U_t.shape}")
        
        # æµ‹è¯•ç›¸ä¼¼æ€§è®¡ç®—
        similarity = lora_x.compute_subspace_similarity(U_s, U_t)
        print(f"âœ… å­ç©ºé—´ç›¸ä¼¼æ€§: {similarity:.3f}")
        
        # æµ‹è¯•å•å±‚è¿ç§»
        transferred = lora_x._transfer_single_layer(lora_delta, source_weight, target_weight)
        print(f"âœ… å±‚è¿ç§»: {lora_delta.shape} -> {transferred.shape}")
        
        return True
    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸš€ LoRA-Xæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•1: LoRAåŠ è½½
    lora_weights, config = test_lora_loading()
    
    # æµ‹è¯•2: æ ¸å¿ƒç®—æ³•
    test_simple_transfer()
    
    print("\\n" + "=" * 60)
    
    if lora_weights is not None:
        print("âœ… åŸºç¡€åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥å°è¯•å®é™…è¿ç§»ï¼")
        print("\\nğŸ“‹ ä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡Œå®Œæ•´è¿ç§»: python scripts/transfer_lora_x.py")
        print("  2. è¯„ä¼°è¿ç§»ç»“æœ")
        print("  3. è®°å½•å®éªŒæ•°æ®")
    else:
        print("âŒ åŸºç¡€åŠŸèƒ½æœ‰é—®é¢˜ï¼Œéœ€è¦ä¿®å¤")

if __name__ == "__main__":
    main()
