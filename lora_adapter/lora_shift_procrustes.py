#!/usr/bin/env python3
"""
åŸºäºProcrustesæœ€ä¼˜å¯¹é½çš„LoRAè¿ç§»ç®—æ³•
å®ç°ç­–ç•¥Aï¼šç»“æ„ä¿æŒçš„LoRAè¿ç§» + Procrustesæœ€ä¼˜å¯¹é½

æ ¸å¿ƒæ€æƒ³ï¼š
1. é‡æ„é˜¶æ®µï¼šÎ”W_source = B_source @ A_source
2. å¯¹é½é˜¶æ®µï¼šä½¿ç”¨Procrustesæ‰¾åˆ°æœ€ä¼˜å˜æ¢ Q*
3. å˜æ¢é˜¶æ®µï¼šÎ”W_aligned = Q* @ Î”W_source @ Q*^T
4. åˆ†è§£é˜¶æ®µï¼šÎ”W_aligned = B_target @ A_target (SVD + æˆªæ–­)
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch
import torch.nn.functional as F
import numpy as np
import gc

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from lora_x_core import LoRAXCore
from model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")


def infer_source_model_path(lora_path: str) -> str:
    """æ ¹æ®LoRAè·¯å¾„æ¨æ–­æºæ¨¡å‹è·¯å¾„"""
    lora_path = Path(lora_path)
    
    if lora_path.name == "final_model":
        model_name = lora_path.parent.parent.name  
    else:
        model_name = lora_path.parent.name  
    
    source_model_path = f"/root/autodl-tmp/models/{model_name}"
    return source_model_path


class ProcrustesLoRACore(LoRAXCore):
    """åŸºäºProcrusteså¯¹é½çš„LoRAè¿ç§»æ ¸å¿ƒç±»"""
    
    def __init__(self, rank=64, similarity_threshold=0.1, lora_rank=16):
        super().__init__(rank, similarity_threshold)
        self.lora_rank = lora_rank
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def procrustes_alignment(self, source_matrix: torch.Tensor, target_matrix: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—Procrustesæœ€ä¼˜å¯¹é½å˜æ¢çŸ©é˜µ
        
        Args:
            source_matrix: æºçŸ©é˜µ A
            target_matrix: ç›®æ ‡çŸ©é˜µ B
            
        Returns:
            Q: æœ€ä¼˜æ­£äº¤å˜æ¢çŸ©é˜µï¼Œä½¿å¾— ||AQ - B||_F^2 æœ€å°
        """
        # è®¡ç®— A^T B
        AtB = torch.mm(source_matrix.T, target_matrix)
        
        # SVDåˆ†è§£: A^T B = U Î£ V^T
        U, S, Vh = torch.linalg.svd(AtB, full_matrices=False)
        
        # æœ€ä¼˜æ­£äº¤å˜æ¢: Q* = V U^T
        Q_optimal = torch.mm(Vh.T, U.T)
        
        return Q_optimal
    
    def reconstruct_full_lora(self, lora_A: torch.Tensor, lora_B: torch.Tensor) -> torch.Tensor:
        """é‡æ„å®Œæ•´çš„LoRAæƒé‡çŸ©é˜µ"""
        return torch.mm(lora_B, lora_A)
    
    def decompose_to_lora(self, full_weight: torch.Tensor, rank: int = None) -> tuple:
        """å°†å®Œæ•´æƒé‡çŸ©é˜µåˆ†è§£ä¸ºLoRAå½¢å¼"""
        if rank is None:
            rank = self.lora_rank
            
        # SVDåˆ†è§£
        U, S, Vh = torch.linalg.svd(full_weight.float(), full_matrices=False)
        
        # æˆªæ–­åˆ°æŒ‡å®šrank
        effective_rank = min(rank, min(full_weight.shape), len(S))
        U_truncated = U[:, :effective_rank]
        S_truncated = S[:effective_rank]
        Vh_truncated = Vh[:effective_rank, :]
        
        # æ„é€ LoRAæƒé‡
        # LoRA_A = sqrt(S) * V^T
        # LoRA_B = U * sqrt(S)
        sqrt_S = torch.sqrt(S_truncated)
        lora_A = torch.mm(torch.diag(sqrt_S), Vh_truncated)
        lora_B = torch.mm(U_truncated, torch.diag(sqrt_S))
        
        return lora_A, lora_B
    
    def transfer_lora_pair(self, lora_A_source: torch.Tensor, lora_B_source: torch.Tensor,
                          source_base: torch.Tensor, target_base: torch.Tensor) -> tuple:
        """
        ä½¿ç”¨Procrusteså¯¹é½è¿ç§»ä¸€å¯¹LoRAæƒé‡
        
        Returns:
            (lora_A_target, lora_B_target): è¿ç§»åçš„LoRAæƒé‡å¯¹
        """
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = self.device
        lora_A_source = lora_A_source.to(device)
        lora_B_source = lora_B_source.to(device)
        source_base = source_base.to(device)
        target_base = target_base.to(device)
        
        # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
        if source_base.shape != target_base.shape:
            # ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ç®€åŒ–çš„æŠ•å½±æ–¹æ³•
            return self._handle_dimension_mismatch(lora_A_source, lora_B_source, source_base, target_base)
        
        # æ­¥éª¤1: é‡æ„å®Œæ•´çš„LoRAæƒé‡
        full_lora_source = self.reconstruct_full_lora(lora_A_source, lora_B_source)
        
        # æ­¥éª¤2: è®¡ç®—æºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹çš„å­ç©ºé—´
        U_source, _, Vh_source = self.compute_svd_subspace(source_base)
        U_target, _, Vh_target = self.compute_svd_subspace(target_base)
        
        # æ­¥éª¤3: Procrusteså¯¹é½
        try:
            # å¯¹è¡Œç©ºé—´è¿›è¡Œå¯¹é½
            Q_row = self.procrustes_alignment(U_source, U_target)
            # å¯¹åˆ—ç©ºé—´è¿›è¡Œå¯¹é½
            Q_col = self.procrustes_alignment(Vh_source.T, Vh_target.T)
            
            # æ­¥éª¤4: åº”ç”¨å¯¹é½å˜æ¢
            # Î”W_aligned = Q_row^T @ Î”W_source @ Q_col
            full_lora_aligned = torch.mm(torch.mm(Q_row.T, full_lora_source), Q_col)
            
            # æ­¥éª¤5: é‡æ–°åˆ†è§£ä¸ºLoRAå½¢å¼
            lora_A_target, lora_B_target = self.decompose_to_lora(full_lora_aligned, self.lora_rank)
            
            return lora_A_target, lora_B_target
            
        except Exception as e:
            # å¦‚æœProcrusteså¯¹é½å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ–¹æ³•
            logger.warning(f"Procrusteså¯¹é½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ³•: {e}")
            return self._handle_dimension_mismatch(lora_A_source, lora_B_source, source_base, target_base)
    
    def _handle_dimension_mismatch(self, lora_A_source: torch.Tensor, lora_B_source: torch.Tensor,
                                  source_base: torch.Tensor, target_base: torch.Tensor) -> tuple:
        """å¤„ç†ç»´åº¦ä¸åŒ¹é…çš„æƒ…å†µ"""
        # è·å–ç›®æ ‡ç»´åº¦
        target_shape = target_base.shape
        
        # é‡æ„æºLoRA
        full_lora_source = self.reconstruct_full_lora(lora_A_source, lora_B_source)
        
        # åˆ›å»ºç›®æ ‡å¤§å°çš„é›¶çŸ©é˜µ
        full_lora_target = torch.zeros(target_shape, device=self.device, dtype=full_lora_source.dtype)
        
        # å¤åˆ¶å¯ä»¥åŒ¹é…çš„éƒ¨åˆ†
        min_rows = min(full_lora_source.shape[0], target_shape[0])
        min_cols = min(full_lora_source.shape[1], target_shape[1])
        full_lora_target[:min_rows, :min_cols] = full_lora_source[:min_rows, :min_cols]
        
        # é‡æ–°åˆ†è§£ä¸ºLoRAå½¢å¼
        lora_A_target, lora_B_target = self.decompose_to_lora(full_lora_target, self.lora_rank)
        
        return lora_A_target, lora_B_target
    
    def transfer_lora_weights(self, source_lora: dict, target_base_weights: dict, 
                            source_base_weights: dict) -> dict:
        """æ‰§è¡Œå®Œæ•´çš„LoRAæƒé‡è¿ç§»"""
        transferred_lora = {}
        stats = {
            'total_pairs': 0,
            'transferred_pairs': 0,
            'skipped_pairs': 0,
            'transferred_list': [],
            'skipped_list': [],
            'skipped_reasons': {},
            'similarity_stats': [],
            'layer_types': {},
            'processing_times': []
        }
        
        print(f"ğŸš€ Procrustes LoRAè¿ç§»å¯åŠ¨ - å¤„ç†{len(source_lora)}ä¸ªæƒé‡")
        
        # å°†LoRAæƒé‡æŒ‰A/Bé…å¯¹
        lora_pairs = self._group_lora_pairs(source_lora)
        stats['total_pairs'] = len(lora_pairs)
        
        print(f"ğŸ“Š æ‰¾åˆ°{len(lora_pairs)}ä¸ªLoRAæƒé‡å¯¹")
        
        for i, (base_name, pair_info) in enumerate(lora_pairs.items()):
            if i % 10 == 0:
                print(f"   è¿ç§»è¿›åº¦: {i}/{len(lora_pairs)}")
            
            try:
                start_time = datetime.now()
                
                # è·å–æƒé‡
                lora_A = pair_info['lora_A']
                lora_B = pair_info['lora_B']
                base_key = self._map_lora_to_base_key(pair_info['lora_A_key'])
                
                if base_key not in source_base_weights or base_key not in target_base_weights:
                    reason = "æ‰¾ä¸åˆ°å¯¹åº”çš„åŸºç¡€æƒé‡"
                    stats['skipped_pairs'] += 1
                    stats['skipped_list'].append(base_name)
                    stats['skipped_reasons'][base_name] = reason
                    continue
                
                source_base = source_base_weights[base_key]
                target_base = target_base_weights[base_key]
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                U_s, _, _ = self.compute_svd_subspace(source_base)
                U_t, _, _ = self.compute_svd_subspace(target_base)
                similarity = self.compute_subspace_similarity(U_s, U_t)
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                layer_type = self._classify_layer_type(pair_info['lora_A_key'])
                stats['similarity_stats'].append({
                    'layer': base_name,
                    'similarity': similarity,
                    'layer_type': layer_type
                })
                
                if layer_type not in stats['layer_types']:
                    stats['layer_types'][layer_type] = {'total': 0, 'transferred': 0}
                stats['layer_types'][layer_type]['total'] += 1
                
                # ç›¸ä¼¼åº¦è¿‡æ»¤
                if similarity < self.similarity_threshold:
                    reason = f"ç›¸ä¼¼åº¦è¿‡ä½ ({similarity:.6f} < {self.similarity_threshold})"
                    stats['skipped_pairs'] += 1
                    stats['skipped_list'].append(base_name)
                    stats['skipped_reasons'][base_name] = reason
                    continue
                
                # æ‰§è¡ŒProcrustesè¿ç§»
                lora_A_target, lora_B_target = self.transfer_lora_pair(
                    lora_A, lora_B, source_base, target_base
                )
                
                # ä¿å­˜è¿ç§»ç»“æœ
                transferred_lora[pair_info['lora_A_key']] = lora_A_target.cpu()
                transferred_lora[pair_info['lora_B_key']] = lora_B_target.cpu()
                
                stats['transferred_pairs'] += 1
                stats['transferred_list'].append(base_name)
                stats['layer_types'][layer_type]['transferred'] += 1
                
                # è®°å½•å¤„ç†æ—¶é—´
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                stats['processing_times'].append(processing_time)
                
                # æ¸…ç†GPUå†…å­˜
                del lora_A_target, lora_B_target, source_base, target_base
                
            except Exception as e:
                reason = f"è¿ç§»å¤±è´¥: {str(e)}"
                stats['skipped_pairs'] += 1
                stats['skipped_list'].append(base_name)
                stats['skipped_reasons'][base_name] = reason
                logger.warning(f"è¿ç§»å¤±è´¥ {base_name}: {e}")
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if i % 5 == 0:
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                gc.collect()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if stats['processing_times']:
            stats['avg_processing_time'] = sum(stats['processing_times']) / len(stats['processing_times'])
            stats['total_processing_time'] = sum(stats['processing_times'])
        
        self._print_procrustes_stats(stats)
        return transferred_lora, stats
    
    def _group_lora_pairs(self, source_lora: dict) -> dict:
        """å°†LoRAæƒé‡æŒ‰A/Bé…å¯¹"""
        pairs = {}
        
        for key, weight in source_lora.items():
            if not key.endswith('.weight'):
                continue
                
            if '.lora_A.' in key:
                base_name = key.replace('.lora_A.weight', '')
                if base_name not in pairs:
                    pairs[base_name] = {}
                pairs[base_name]['lora_A'] = weight
                pairs[base_name]['lora_A_key'] = key
            elif '.lora_B.' in key:
                base_name = key.replace('.lora_B.weight', '')
                if base_name not in pairs:
                    pairs[base_name] = {}
                pairs[base_name]['lora_B'] = weight
                pairs[base_name]['lora_B_key'] = key
        
        # åªä¿ç•™å®Œæ•´çš„A/Bå¯¹
        complete_pairs = {}
        for base_name, pair_info in pairs.items():
            if 'lora_A' in pair_info and 'lora_B' in pair_info:
                complete_pairs[base_name] = pair_info
        
        return complete_pairs
    
    def _print_procrustes_stats(self, stats: dict):
        """æ‰“å°Procrustesè¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'ğŸ‰'*20} Procrustesè¿ç§»å®Œæˆç»Ÿè®¡ {'ğŸ‰'*20}")
        print(f"{'='*80}")
        print(f"ğŸ“Š LoRAæƒé‡å¯¹ç»Ÿè®¡:")
        print(f"  æ€»æƒé‡å¯¹æ•°: {stats['total_pairs']}")
        print(f"  æˆåŠŸè¿ç§»: {stats['transferred_pairs']}")
        print(f"  è·³è¿‡å¯¹æ•°: {stats['skipped_pairs']}")
        
        if stats['transferred_pairs'] > 0:
            success_rate = (stats['transferred_pairs'] / stats['total_pairs']) * 100
            print(f"  è¿ç§»æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æŒ‰å±‚ç±»å‹ç»Ÿè®¡
        if stats['layer_types']:
            print(f"\nğŸ“‹ æŒ‰å±‚ç±»å‹ç»Ÿè®¡:")
            for layer_type, type_stats in stats['layer_types'].items():
                total = type_stats['total']
                transferred = type_stats['transferred']
                rate = (transferred / total * 100) if total > 0 else 0
                print(f"  {layer_type:12s}: {transferred:2d}/{total:2d} ({rate:5.1f}%)")
        
        # ç›¸ä¼¼åº¦ç»Ÿè®¡
        if stats['similarity_stats']:
            similarities = [s['similarity'] for s in stats['similarity_stats']]
            print(f"\nğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:")
            print(f"  å¹³å‡ç›¸ä¼¼åº¦: {sum(similarities)/len(similarities):.6f}")
            print(f"  æœ€é«˜ç›¸ä¼¼åº¦: {max(similarities):.6f}")
            print(f"  æœ€ä½ç›¸ä¼¼åº¦: {min(similarities):.6f}")
        
        # æ€§èƒ½ç»Ÿè®¡
        if 'avg_processing_time' in stats:
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}ç§’/å¯¹")
            print(f"  æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.1f}ç§’")
        
        print(f"{'='*80}")


def main():
    parser = argparse.ArgumentParser(description="åŸºäºProcrusteså¯¹é½çš„LoRAè¿ç§»è„šæœ¬")
    parser.add_argument("--source_lora", type=str, required=True,
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, required=True,
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str, 
                       default="/root/autodl-tmp/shifted/procrustes",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--rank", type=int, default=64,
                       help="SVDæˆªæ–­ç§©")
    parser.add_argument("--similarity_threshold", type=float, default=0.1,
                       help="ç›¸ä¼¼æ€§é˜ˆå€¼ (æé«˜ä»¥ç¡®ä¿è´¨é‡)")
    parser.add_argument("--lora_rank", type=int, default=16,
                       help="LoRAç§©")
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œè¾“å‡ºè·¯å¾„
    timestamp = generate_timestamp()
    output_path = os.path.join(args.output_base, timestamp)
    
    # æ¨æ–­æºæ¨¡å‹è·¯å¾„
    source_model_path = infer_source_model_path(args.source_lora)
    
    print(f"ğŸš€ Procrustes LoRAè¿ç§» - ç»“æ„ä¿æŒå¯¹é½ç®—æ³•")
    print(f"ğŸ“‚ æºLoRA: {args.source_lora}")
    print(f"ğŸ“‚ æºæ¨¡å‹: {source_model_path}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡º: {output_path}")
    print(f"âš™ï¸ å‚æ•°: rank={args.rank}, threshold={args.similarity_threshold}, lora_rank={args.lora_rank}")
    
    try:
        # æ£€æŸ¥è·¯å¾„
        for path in [args.source_lora, source_model_path, args.target_model]:
            if not os.path.exists(path):
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
                return False
        
        # åˆå§‹åŒ–ç»„ä»¶
        lora_core = ProcrustesLoRACore(
            rank=args.rank, 
            similarity_threshold=args.similarity_threshold,
            lora_rank=args.lora_rank
        )
        loader = ModelWeightLoader()
        
        # åŠ è½½æƒé‡
        print("ğŸ“¥ åŠ è½½LoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)
        
        print("ğŸ“¥ åŠ è½½æºæ¨¡å‹æƒé‡...")
        source_base_weights = loader.load_base_model_weights(source_model_path)
        
        print("ğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)
        
        # æ‰§è¡Œè¿ç§»
        start_time = datetime.now()
        transferred_lora, stats = lora_core.transfer_lora_weights(
            source_lora=source_lora_weights,
            target_base_weights=target_base_weights,
            source_base_weights=source_base_weights
        )
        end_time = datetime.now()
        
        if not transferred_lora:
            print("âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•æƒé‡å¯¹")
            return False
        
        # ä¿å­˜ç»“æœ
        os.makedirs(output_path, exist_ok=True)
        save_transferred_lora(transferred_lora, lora_config, output_path)
        
        duration = (end_time - start_time).total_seconds()
        print(f"ğŸ‰ Procrustesè¿ç§»å®Œæˆï¼ç”¨æ—¶: {duration:.1f}ç§’")
        print(f"ğŸ“‚ ç»“æœ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)