#!/usr/bin/env python3
"""
åŸºäºç®€åŒ–ç­–ç•¥çš„LoRAè¿ç§»ç®—æ³• - æ— éœ€æºæ¨¡å‹ç‰ˆæœ¬
å®ç°ç­–ç•¥ï¼šç›´æ¥LoRAè¿ç§» + ç»´åº¦é€‚é…

æ ¸å¿ƒæ€æƒ³ï¼š
1. é‡æ„é˜¶æ®µï¼šÎ”W_source = B_source @ A_source
2. é€‚é…é˜¶æ®µï¼šæ ¹æ®ç›®æ ‡æ¨¡å‹ç»´åº¦è°ƒæ•´LoRAæƒé‡
3. åˆ†è§£é˜¶æ®µï¼šÎ”W_adapted = B_target @ A_target (SVD + æˆªæ–­)
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch
import torch.nn.functional as F
import numpy as np
import gc

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")


def extract_model_name(model_path: str) -> str:
    """ä»æ¨¡å‹è·¯å¾„ä¸­æå–æ¨¡å‹åç§°"""
    model_path = Path(model_path)
    return model_path.name


def extract_dataset_name(lora_path: str) -> str:
    """ä»LoRAè·¯å¾„ä¸­æå–æ•°æ®é›†åç§°"""
    lora_path = Path(lora_path)
    # ä»è·¯å¾„ä¸­æŸ¥æ‰¾æ•°æ®é›†åç§°ï¼Œé€šå¸¸åœ¨æ–‡ä»¶å¤¹åä¸­åŒ…å«
    for part in lora_path.parts:
        if 'arc-challenge' in part:
            return 'arc-challenge'
        elif 'hellaswag' in part:
            return 'hellaswag'
        elif 'mmlu' in part:
            return 'mmlu'
        elif 'truthfulqa' in part:
            return 'truthfulqa'
        elif 'winogrande' in part:
            return 'winogrande'
        elif 'gsm8k' in part:
            return 'gsm8k'
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·²çŸ¥æ•°æ®é›†ï¼Œå°è¯•ä»æ–‡ä»¶å¤¹åä¸­æå–
    for part in lora_path.parts:
        if '_lora_' in part:
            # æå– _lora_ å‰é¢çš„éƒ¨åˆ†ä½œä¸ºæ•°æ®é›†å
            dataset_part = part.split('_lora_')[0]
            if dataset_part:
                return dataset_part
    
    return 'unknown'


def generate_output_path(source_lora: str, target_model: str, output_base: str, timestamp: str) -> str:
    """ç”Ÿæˆè¾“å‡ºè·¯å¾„ï¼Œæ ¼å¼: output_base/dataset/source_model_to_target_model/timestamp"""
    dataset_name = extract_dataset_name(source_lora)
    
    # ä»LoRAè·¯å¾„ä¸­æå–æºæ¨¡å‹åç§°
    lora_path = Path(source_lora)
    source_model_name = None
    for part in lora_path.parts:
        if any(model in part for model in ['Llama', 'Qwen', 'gemma', 'mistral', 'phi']):
            source_model_name = part
            break
    
    if not source_model_name:
        source_model_name = "unknown"
    
    target_model_name = extract_model_name(target_model)
    
    # æ„å»ºè·¯å¾„: output_base/dataset/source_to_target/timestamp
    transfer_name = f"{source_model_name}_to_{target_model_name}"
    output_path = os.path.join(output_base, dataset_name, transfer_name, timestamp)
    
    return output_path


class SimpleLoRATransfer:
    """ç®€åŒ–çš„LoRAè¿ç§»æ ¸å¿ƒç±» - æ— éœ€æºæ¨¡å‹"""
    
    def __init__(self, lora_rank=16):
        self.lora_rank = lora_rank
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def reconstruct_full_lora(self, lora_A: torch.Tensor, lora_B: torch.Tensor) -> torch.Tensor:
        """é‡æ„å®Œæ•´çš„LoRAæƒé‡çŸ©é˜µ"""
        return torch.mm(lora_B, lora_A)
    
    def decompose_to_lora(self, full_weight: torch.Tensor, rank: int = None) -> tuple:
        """å°†å®Œæ•´æƒé‡çŸ©é˜µåˆ†è§£ä¸ºLoRAå½¢å¼"""
        if rank is None:
            rank = self.lora_rank
            
        # SVDåˆ†è§£
        U, S, Vh = torch.linalg.svd(full_weight.float(), full_matrices=False)
        
        # æˆªæ–­åˆ°æŒ‡å®šrank
        effective_rank = min(rank, min(full_weight.shape), len(S))
        U_truncated = U[:, :effective_rank]
        S_truncated = S[:effective_rank]
        Vh_truncated = Vh[:effective_rank, :]
        
        # æ„é€ LoRAæƒé‡
        # LoRA_A = sqrt(S) * V^T
        # LoRA_B = U * sqrt(S)
        sqrt_S = torch.sqrt(S_truncated)
        lora_A = torch.mm(torch.diag(sqrt_S), Vh_truncated)
        lora_B = torch.mm(U_truncated, torch.diag(sqrt_S))
        
        return lora_A, lora_B
    
    def adapt_lora_dimensions(self, lora_A: torch.Tensor, lora_B: torch.Tensor, 
                             target_shape: tuple) -> tuple:
        """
        é€‚é…LoRAæƒé‡åˆ°ç›®æ ‡ç»´åº¦
        
        Args:
            lora_A: æºLoRA_Aæƒé‡
            lora_B: æºLoRA_Bæƒé‡
            target_shape: ç›®æ ‡æƒé‡å½¢çŠ¶ (out_features, in_features)
            
        Returns:
            (adapted_lora_A, adapted_lora_B): é€‚é…åçš„LoRAæƒé‡å¯¹
        """
        # é‡æ„å®Œæ•´æƒé‡
        full_lora = self.reconstruct_full_lora(lora_A, lora_B)
        
        # è·å–å½“å‰å’Œç›®æ ‡ç»´åº¦
        current_shape = full_lora.shape
        target_out, target_in = target_shape
        
        # åˆ›å»ºç›®æ ‡å¤§å°çš„é›¶çŸ©é˜µ
        adapted_full = torch.zeros(target_shape, device=self.device, dtype=full_lora.dtype)
        
        # ç­–ç•¥1: ç›´æ¥å¤åˆ¶å¯åŒ¹é…çš„éƒ¨åˆ†
        min_out = min(current_shape[0], target_out)
        min_in = min(current_shape[1], target_in)
        adapted_full[:min_out, :min_in] = full_lora[:min_out, :min_in]
        
        # ç­–ç•¥2: å¦‚æœç›®æ ‡ç»´åº¦æ›´å¤§ï¼Œä½¿ç”¨æ’å€¼æˆ–é‡å¤å¡«å……
        if target_out > current_shape[0] or target_in > current_shape[1]:
            # å¯¹äºè¶…å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨è¾ƒå°çš„éšæœºåˆå§‹åŒ–
            if target_out > current_shape[0]:
                # è¡Œç»´åº¦æ‰©å±•ï¼šä½¿ç”¨æœ€åå‡ è¡Œçš„å¹³å‡å€¼
                if current_shape[0] > 0:
                    avg_rows = full_lora[-min(3, current_shape[0]):, :min_in].mean(dim=0, keepdim=True)
                    for i in range(min_out, target_out):
                        adapted_full[i, :min_in] = avg_rows * 0.1  # ç¼©å°å¹…åº¦
            
            if target_in > current_shape[1]:
                # åˆ—ç»´åº¦æ‰©å±•ï¼šä½¿ç”¨æœ€åå‡ åˆ—çš„å¹³å‡å€¼
                if current_shape[1] > 0:
                    avg_cols = full_lora[:min_out, -min(3, current_shape[1]):].mean(dim=1, keepdim=True)
                    for j in range(min_in, target_in):
                        adapted_full[:min_out, j] = avg_cols.squeeze() * 0.1  # ç¼©å°å¹…åº¦
        
        # é‡æ–°åˆ†è§£ä¸ºLoRAå½¢å¼
        adapted_lora_A, adapted_lora_B = self.decompose_to_lora(adapted_full, self.lora_rank)
        
        return adapted_lora_A, adapted_lora_B
    
    def transfer_lora_weights(self, source_lora: dict, target_base_weights: dict) -> dict:
        """æ‰§è¡Œç®€åŒ–çš„LoRAæƒé‡è¿ç§»"""
        transferred_lora = {}
        stats = {
            'total_pairs': 0,
            'transferred_pairs': 0,
            'skipped_pairs': 0,
            'transferred_list': [],
            'skipped_list': [],
            'skipped_reasons': {},
            'layer_types': {},
            'processing_times': []
        }
        
        print(f"ğŸš€ ç®€åŒ–LoRAè¿ç§»å¯åŠ¨ - å¤„ç†{len(source_lora)}ä¸ªæƒé‡")
        
        # å°†LoRAæƒé‡æŒ‰A/Bé…å¯¹
        lora_pairs = self._group_lora_pairs(source_lora)
        stats['total_pairs'] = len(lora_pairs)
        
        print(f"ğŸ“Š æ‰¾åˆ°{len(lora_pairs)}ä¸ªLoRAæƒé‡å¯¹")
        
        for i, (base_name, pair_info) in enumerate(lora_pairs.items()):
            if i % 10 == 0:
                print(f"   è¿ç§»è¿›åº¦: {i}/{len(lora_pairs)}")
            
            try:
                start_time = datetime.now()
                
                # è·å–æƒé‡
                lora_A = pair_info['lora_A']
                lora_B = pair_info['lora_B']
                base_key = self._map_lora_to_base_key(pair_info['lora_A_key'])
                
                if base_key not in target_base_weights:
                    reason = "æ‰¾ä¸åˆ°å¯¹åº”çš„ç›®æ ‡åŸºç¡€æƒé‡"
                    stats['skipped_pairs'] += 1
                    stats['skipped_list'].append(base_name)
                    stats['skipped_reasons'][base_name] = reason
                    continue
                
                target_base = target_base_weights[base_key]
                target_shape = target_base.shape
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                layer_type = self._classify_layer_type(pair_info['lora_A_key'])
                if layer_type not in stats['layer_types']:
                    stats['layer_types'][layer_type] = {'total': 0, 'transferred': 0}
                stats['layer_types'][layer_type]['total'] += 1
                
                # æ‰§è¡Œç»´åº¦é€‚é…
                lora_A_target, lora_B_target = self.adapt_lora_dimensions(
                    lora_A.to(self.device), lora_B.to(self.device), target_shape
                )
                
                # ä¿å­˜è¿ç§»ç»“æœ
                transferred_lora[pair_info['lora_A_key']] = lora_A_target.cpu()
                transferred_lora[pair_info['lora_B_key']] = lora_B_target.cpu()
                
                stats['transferred_pairs'] += 1
                stats['transferred_list'].append(base_name)
                stats['layer_types'][layer_type]['transferred'] += 1
                
                # è®°å½•å¤„ç†æ—¶é—´
                end_time = datetime.now()
                processing_time = (end_time - start_time).total_seconds()
                stats['processing_times'].append(processing_time)
                
                # æ¸…ç†GPUå†…å­˜
                del lora_A_target, lora_B_target, target_base
                
            except Exception as e:
                reason = f"è¿ç§»å¤±è´¥: {str(e)}"
                stats['skipped_pairs'] += 1
                stats['skipped_list'].append(base_name)
                stats['skipped_reasons'][base_name] = reason
                logger.warning(f"è¿ç§»å¤±è´¥ {base_name}: {e}")
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if i % 5 == 0:
                if self.device.type == 'cuda':
                    torch.cuda.empty_cache()
                gc.collect()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if stats['processing_times']:
            stats['avg_processing_time'] = sum(stats['processing_times']) / len(stats['processing_times'])
            stats['total_processing_time'] = sum(stats['processing_times'])
        
        self._print_transfer_stats(stats)
        return transferred_lora, stats
    
    def _group_lora_pairs(self, source_lora: dict) -> dict:
        """å°†LoRAæƒé‡æŒ‰A/Bé…å¯¹"""
        pairs = {}
        
        for key, weight in source_lora.items():
            if not key.endswith('.weight'):
                continue
                
            if '.lora_A.' in key:
                base_name = key.replace('.lora_A.weight', '')
                if base_name not in pairs:
                    pairs[base_name] = {}
                pairs[base_name]['lora_A'] = weight
                pairs[base_name]['lora_A_key'] = key
            elif '.lora_B.' in key:
                base_name = key.replace('.lora_B.weight', '')
                if base_name not in pairs:
                    pairs[base_name] = {}
                pairs[base_name]['lora_B'] = weight
                pairs[base_name]['lora_B_key'] = key
        
        # åªä¿ç•™å®Œæ•´çš„A/Bå¯¹
        complete_pairs = {}
        for base_name, pair_info in pairs.items():
            if 'lora_A' in pair_info and 'lora_B' in pair_info:
                complete_pairs[base_name] = pair_info
        
        return complete_pairs
    
    def _map_lora_to_base_key(self, lora_key: str) -> str:
        """å°†LoRAé”®æ˜ å°„åˆ°åŸºç¡€æ¨¡å‹é”®"""
        # ç§»é™¤LoRAç‰¹å®šçš„éƒ¨åˆ†
        base_key = lora_key.replace('.lora_A.weight', '.weight')
        base_key = base_key.replace('.lora_B.weight', '.weight')
        
        # ç§»é™¤ base_model.model. å‰ç¼€ï¼Œè¿™æ˜¯LoRAç‰¹æœ‰çš„
        if base_key.startswith('base_model.model.'):
            base_key = base_key[len('base_model.model.'):]
        
        return base_key
    
    def _classify_layer_type(self, layer_name: str) -> str:
        """åˆ†ç±»å±‚ç±»å‹"""
        layer_name_lower = layer_name.lower()
        
        if 'q_proj' in layer_name_lower:
            return 'q_proj'
        elif 'k_proj' in layer_name_lower:
            return 'k_proj'
        elif 'v_proj' in layer_name_lower:
            return 'v_proj'
        elif 'o_proj' in layer_name_lower:
            return 'o_proj'
        elif 'gate_proj' in layer_name_lower:
            return 'gate_proj'
        elif 'up_proj' in layer_name_lower:
            return 'up_proj'
        elif 'down_proj' in layer_name_lower:
            return 'down_proj'
        elif 'attn' in layer_name_lower:
            return 'attention'
        elif 'mlp' in layer_name_lower:
            return 'mlp'
        else:
            return 'other'
    
    def _print_transfer_stats(self, stats: dict):
        """æ‰“å°è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'ğŸ‰'*20} ç®€åŒ–LoRAè¿ç§»å®Œæˆç»Ÿè®¡ {'ğŸ‰'*20}")
        print(f"{'='*80}")
        print(f"ğŸ“Š LoRAæƒé‡å¯¹ç»Ÿè®¡:")
        print(f"  æ€»æƒé‡å¯¹æ•°: {stats['total_pairs']}")
        print(f"  æˆåŠŸè¿ç§»: {stats['transferred_pairs']}")
        print(f"  è·³è¿‡å¯¹æ•°: {stats['skipped_pairs']}")
        
        if stats['transferred_pairs'] > 0:
            success_rate = (stats['transferred_pairs'] / stats['total_pairs']) * 100
            print(f"  è¿ç§»æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æŒ‰å±‚ç±»å‹ç»Ÿè®¡
        if stats['layer_types']:
            print(f"\nğŸ“‹ æŒ‰å±‚ç±»å‹ç»Ÿè®¡:")
            for layer_type, type_stats in stats['layer_types'].items():
                total = type_stats['total']
                transferred = type_stats['transferred']
                rate = (transferred / total * 100) if total > 0 else 0
                print(f"  {layer_type:12s}: {transferred:2d}/{total:2d} ({rate:5.1f}%)")
        
        # æ€§èƒ½ç»Ÿè®¡
        if 'avg_processing_time' in stats:
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']:.3f}ç§’/å¯¹")
            print(f"  æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.1f}ç§’")
        
        # è·³è¿‡åŸå› ç»Ÿè®¡
        if stats['skipped_reasons']:
            print(f"\nâŒ è·³è¿‡åŸå› ç»Ÿè®¡:")
            reason_counts = {}
            for reason in stats['skipped_reasons'].values():
                reason_counts[reason] = reason_counts.get(reason, 0) + 1
            for reason, count in reason_counts.items():
                print(f"  {reason}: {count}ä¸ª")
        
        print(f"{'='*80}")


def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–LoRAè¿ç§»è„šæœ¬ - æ— éœ€æºæ¨¡å‹")
    parser.add_argument("--source_lora", type=str, required=True,
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, required=True,
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str,
                       default="/root/autodl-tmp/shifted",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--lora_rank", type=int, default=16,
                       help="LoRAç§©")
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œè¾“å‡ºè·¯å¾„
    timestamp = generate_timestamp()
    output_path = generate_output_path(args.source_lora, args.target_model, args.output_base, timestamp)
    
    print(f"ğŸš€ ç®€åŒ–LoRAè¿ç§» - æ— éœ€æºæ¨¡å‹ç‰ˆæœ¬")
    print(f"ğŸ“‚ æºLoRA: {args.source_lora}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡º: {output_path}")
    print(f"âš™ï¸ å‚æ•°: lora_rank={args.lora_rank}")
    
    try:
        # æ£€æŸ¥è·¯å¾„
        for path in [args.source_lora, args.target_model]:
            if not os.path.exists(path):
                print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
                return False
        
        # åˆå§‹åŒ–ç»„ä»¶
        lora_transfer = SimpleLoRATransfer(lora_rank=args.lora_rank)
        loader = ModelWeightLoader()
        
        # åŠ è½½æƒé‡
        print("ğŸ“¥ åŠ è½½LoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)
        
        print("ğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)
        
        # æ‰§è¡Œè¿ç§»
        start_time = datetime.now()
        transferred_lora, stats = lora_transfer.transfer_lora_weights(
            source_lora=source_lora_weights,
            target_base_weights=target_base_weights
        )
        end_time = datetime.now()
        
        if not transferred_lora:
            print("âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•æƒé‡å¯¹")
            return False
        
        # ä¿å­˜ç»“æœ
        os.makedirs(output_path, exist_ok=True)
        save_transferred_lora(transferred_lora, lora_config, output_path)
        
        duration = (end_time - start_time).total_seconds()
        print(f"ğŸ‰ ç®€åŒ–LoRAè¿ç§»å®Œæˆï¼ç”¨æ—¶: {duration:.1f}ç§’")
        print(f"ğŸ“‚ ç»“æœ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)