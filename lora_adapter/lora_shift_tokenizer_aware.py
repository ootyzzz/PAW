#!/usr/bin/env python3
"""
Tokenizer-Aware LoRAè¿ç§»ç®—æ³•
å®ç°åŸºäºtokenizerå¯¹é½çš„å­ç©ºé—´æ˜ å°„ + Procrustesæœ€ä¼˜å¯¹é½

æ ¸å¿ƒæ€æƒ³ï¼š
1. åˆ†ætokenizerå·®å¼‚ï¼Œå»ºç«‹tokenæ˜ å°„å…³ç³»
2. å¯¹æ¯ä¸ªlayer pairï¼Œå…ˆè¿›è¡Œtokenizer-awareçš„æ˜ å°„
3. ç„¶åä½¿ç”¨Procrustesæœ€ä¼˜å¯¹é½ç®—æ³•
4. æœ€ç»ˆå®ç°tokenizer-awareçš„å­ç©ºé—´å¯¹é½
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch
import torch.nn.functional as F
import numpy as np
import gc
from transformers import AutoTokenizer

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))
sys.path.append(str(Path(__file__).parent / "tokenlizer"))

from lora_x_core import LoRAXCore
from model_utils import ModelWeightLoader, save_transferred_lora
from tokenizer_analyzer import TokenizerAnalyzer

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")


def infer_source_model_path(lora_path: str) -> str:
    """æ ¹æ®LoRAè·¯å¾„æ¨æ–­æºæ¨¡å‹è·¯å¾„"""
    lora_path = Path(lora_path)
    
    if lora_path.name == "final_model":
        model_name = lora_path.parent.parent.name  
    else:
        model_name = lora_path.parent.name  
    
    source_model_path = f"/root/autodl-tmp/models/{model_name}"
    return source_model_path


class TokenizerAwareLoRACore(LoRAXCore):
    """åŸºäºTokenizeræ„ŸçŸ¥çš„LoRAè¿ç§»æ ¸å¿ƒç±»"""
    
    def __init__(self, rank=64, similarity_threshold=0.1, lora_rank=16, 
                 source_tokenizer_path=None, target_tokenizer_path=None):
        super().__init__(rank, similarity_threshold)
        self.lora_rank = lora_rank
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–tokenizeråˆ†æå™¨
        if source_tokenizer_path and target_tokenizer_path:
            print("ğŸ” åˆå§‹åŒ–Tokenizeråˆ†æå™¨...")
            self.tokenizer_analyzer = TokenizerAnalyzer(source_tokenizer_path, target_tokenizer_path)
            self.token_mapping = self._build_token_mapping()
            print(f"ğŸ“Š Tokenæ˜ å°„å»ºç«‹å®Œæˆ: {len(self.token_mapping)}ä¸ªæ˜ å°„")
        else:
            self.tokenizer_analyzer = None
            self.token_mapping = None
            print("âš ï¸ æœªæä¾›tokenizerè·¯å¾„ï¼Œå°†ä½¿ç”¨æ ‡å‡†Procrusteså¯¹é½")
    
    def _build_token_mapping(self) -> dict:
        """å»ºç«‹ä»æºtokenizeråˆ°ç›®æ ‡tokenizerçš„æ˜ å°„"""
        source_vocab = self.tokenizer_analyzer.qwen_tokenizer.get_vocab()
        target_vocab = self.tokenizer_analyzer.llama_tokenizer.get_vocab()
        
        mapping = {}
        
        # æ–¹æ³•1: ç²¾ç¡®åŒ¹é…
        for src_token, src_id in source_vocab.items():
            if src_token in target_vocab:
                mapping[src_id] = target_vocab[src_token]
        
        print(f"  ç²¾ç¡®åŒ¹é…: {len(mapping)}ä¸ªtoken")
        
        # æ–¹æ³•2: ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆä¸ºæœªåŒ¹é…çš„tokenï¼‰
        unmatched_count = 0
        for src_token, src_id in source_vocab.items():
            if src_id not in mapping:
                # å¯»æ‰¾æœ€ç›¸ä¼¼çš„token
                best_match = self._find_similar_token(src_token, target_vocab)
                if best_match:
                    mapping[src_id] = target_vocab[best_match]
                    unmatched_count += 1
                else:
                    # æ˜ å°„åˆ°UNK token
                    mapping[src_id] = target_vocab.get('<unk>', 0)
        
        print(f"  ç›¸ä¼¼åº¦åŒ¹é…: {unmatched_count}ä¸ªtoken")
        print(f"  æ€»æ˜ å°„è¦†ç›–ç‡: {len(mapping)/len(source_vocab)*100:.1f}%")
        
        return mapping
    
    def _find_similar_token(self, source_token: str, target_vocab: dict) -> str:
        """å¯»æ‰¾æœ€ç›¸ä¼¼çš„token"""
        # ç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åŒ¹é…
        best_match = None
        best_score = 0
        
        for target_token in target_vocab.keys():
            # è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
            score = self._string_similarity(source_token, target_token)
            if score > best_score and score > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                best_score = score
                best_match = target_token
        
        return best_match
    
    def _string_similarity(self, s1: str, s2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        if not s1 or not s2:
            return 0.0
        
        # ç®€å•çš„Jaccardç›¸ä¼¼åº¦
        set1 = set(s1.lower())
        set2 = set(s2.lower())
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def tokenizer_aware_alignment(self, source_embedding: torch.Tensor, 
                                target_embedding: torch.Tensor) -> torch.Tensor:
        """
        åŸºäºtokenizeræ˜ å°„çš„åµŒå…¥å¯¹é½
        
        Args:
            source_embedding: æºæ¨¡å‹çš„åµŒå…¥è¡¨ç¤º [seq_len, hidden_dim]
            target_embedding: ç›®æ ‡æ¨¡å‹çš„åµŒå…¥è¡¨ç¤º [seq_len, hidden_dim]
            
        Returns:
            aligned_embedding: å¯¹é½åçš„åµŒå…¥è¡¨ç¤º
        """
        if self.token_mapping is None:
            # å¦‚æœæ²¡æœ‰tokenæ˜ å°„ï¼Œç›´æ¥è¿”å›ç›®æ ‡åµŒå…¥
            return target_embedding
        
        # åˆ›å»ºå¯¹é½åçš„åµŒå…¥çŸ©é˜µ
        aligned_embedding = torch.zeros_like(target_embedding)
        
        # åº”ç”¨tokenæ˜ å°„
        for src_id, tgt_id in self.token_mapping.items():
            if src_id < source_embedding.shape[0] and tgt_id < target_embedding.shape[0]:
                # ä½¿ç”¨åŠ æƒå¹³å‡æ¥èåˆæºå’Œç›®æ ‡çš„è¡¨ç¤º
                alpha = 0.7  # æºæ¨¡å‹æƒé‡
                aligned_embedding[tgt_id] = (alpha * source_embedding[src_id] + 
                                           (1-alpha) * target_embedding[tgt_id])
        
        return aligned_embedding
    
    def procrustes_alignment(self, source_matrix: torch.Tensor, target_matrix: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—Procrustesæœ€ä¼˜å¯¹é½å˜æ¢çŸ©é˜µ
        """
        # è®¡ç®— A^T B
        AtB = torch.mm(source_matrix.T, target_matrix)
        
        # SVDåˆ†è§£: A^T B = U Î£ V^T
        U, S, Vh = torch.linalg.svd(AtB, full_matrices=False)
        
        # æœ€ä¼˜æ­£äº¤å˜æ¢: Q* = V U^T
        Q_optimal = torch.mm(Vh.T, U.T)
        
        return Q_optimal
    
    def reconstruct_full_lora(self, lora_A: torch.Tensor, lora_B: torch.Tensor) -> torch.Tensor:
        """é‡æ„å®Œæ•´çš„LoRAæƒé‡çŸ©é˜µ"""
        return torch.mm(lora_B, lora_A)
    
    def decompose_to_lora(self, full_weight: torch.Tensor, rank: int = None) -> tuple:
        """å°†å®Œæ•´æƒé‡çŸ©é˜µåˆ†è§£ä¸ºLoRAå½¢å¼"""
        if rank is None:
            rank = self.lora_rank
            
        # SVDåˆ†è§£
        U, S, Vh = torch.linalg.svd(full_weight.float(), full_matrices=False)
        
        # æˆªæ–­åˆ°æŒ‡å®šrank
        effective_rank = min(rank, min(full_weight.shape), len(S))
        U_truncated = U[:, :effective_rank]
        S_truncated = S[:effective_rank]
        Vh_truncated = Vh[:effective_rank, :]
        
        # æ„é€ LoRAæƒé‡
        sqrt_S = torch.sqrt(S_truncated)
        lora_A = torch.mm(torch.diag(sqrt_S), Vh_truncated)
        lora_B = torch.mm(U_truncated, torch.diag(sqrt_S))
        
        return lora_A, lora_B
    
    def tokenizer_aware_transfer_lora_pair(self, lora_A_source: torch.Tensor, lora_B_source: torch.Tensor,
                                         source_base: torch.Tensor, target_base: torch.Tensor) -> tuple:
        """
        ä½¿ç”¨Tokenizer-Aware + Procrusteså¯¹é½è¿ç§»ä¸€å¯¹LoRAæƒé‡
        
        Returns:
            (lora_A_target, lora_B_target): è¿ç§»åçš„LoRAæƒé‡å¯¹
        """
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = self.device
        lora_A_source = lora_A_source.to(device)
        lora_B_source = lora_B_source.to(device)
        source_base = source_base.to(device)
        target_base = target_base.to(device)
        
        # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
        if source_base.shape != target_base.shape:
            # ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨ç®€åŒ–çš„æŠ•å½±æ–¹æ³•
            return self._handle_dimension_mismatch(lora_