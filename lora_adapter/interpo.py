#!/usr/bin/env python3
"""
LoRA Linear Interpolation Baseline
ç”¨ä¾‹: ä»æºæ¨¡å‹çš„LoRAç›´æ¥çº¿æ€§æ’å€¼åˆ°ç›®æ ‡æ¨¡å‹
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from interpo_core import InterpoCore
from model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")

def infer_source_model_path(lora_path: str) -> str:
    lora_path = Path(lora_path)
    if lora_path.name == "final_model":
        model_name = lora_path.parent.parent.name
    else:
        model_name = lora_path.parent.name
    return f"/root/autodl-tmp/models/{model_name}"

def main():
    parser = argparse.ArgumentParser(description="LoRA Linear Interpolation Baseline")
    parser.add_argument("--source_lora", type=str, required=True,
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, required=True,
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str,
                       default="/root/autodl-tmp/interpolated",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--alpha", type=float, default=1.0,
                       help="æ’å€¼ç³»æ•° Î± (0=ä»…æº, 1=ä»…æ’å€¼ç¼©æ”¾)")
    args = parser.parse_args()

    timestamp = generate_timestamp()
    output_path = os.path.join(args.output_base, timestamp)
    source_model_path = infer_source_model_path(args.source_lora)

    print(f"\nğŸš€ LoRAçº¿æ€§æ’å€¼è¿ç§»å¼€å§‹ ğŸš€")
    print("="*80)
    print(f"ğŸ“‚ æºLoRAè·¯å¾„: {args.source_lora}")
    print(f"ğŸ“‚ æºæ¨¡å‹è·¯å¾„: {source_model_path}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹è·¯å¾„: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"âš™ï¸ æ’å€¼ç³»æ•°: {args.alpha}")
    print(f"ğŸ• æ—¶é—´æˆ³: {timestamp}")
    print("="*80)

    try:
        if not os.path.exists(args.source_lora):
            raise FileNotFoundError(f"æºLoRAè·¯å¾„ä¸å­˜åœ¨: {args.source_lora}")
        if not os.path.exists(source_model_path):
            raise FileNotFoundError(f"æºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {source_model_path}")
        if not os.path.exists(args.target_model):
            raise FileNotFoundError(f"ç›®æ ‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.target_model}")

        interpo = InterpoCore(alpha=args.alpha)
        loader = ModelWeightLoader()

        print("\nğŸ“¥ åŠ è½½æºLoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)

        print("\nğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)

        print("\nğŸ”„ æ‰§è¡ŒLoRAçº¿æ€§æ’å€¼è¿ç§»...")
        with torch.no_grad():
            transferred_lora = interpo.transfer_lora_weights(
                source_lora=source_lora_weights,
                target_base_weights=target_base_weights
            )

        if not transferred_lora:
            print("âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚")
            return False

        os.makedirs(output_path, exist_ok=True)
        print(f"\nğŸ’¾ ä¿å­˜è¿ç§»ç»“æœåˆ°: {output_path}")
        save_transferred_lora(transferred_lora, lora_config, output_path)

        print("\nğŸ‰ LoRAçº¿æ€§æ’å€¼è¿ç§»å®Œæˆï¼")
        return True

    except Exception as e:
        logger.error(f"è¿ç§»è¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
