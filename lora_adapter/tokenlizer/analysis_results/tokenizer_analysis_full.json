{
  "basic_info": {
    "qwen_vocab_size": 151665,
    "llama_vocab_size": 128256,
    "qwen_model_max_length": 131072,
    "llama_model_max_length": 131072,
    "qwen_tokenizer_class": "Qwen2TokenizerFast",
    "llama_tokenizer_class": "PreTrainedTokenizerFast",
    "qwen_special_tokens": [
      "<|vision_pad|>",
      "<|video_pad|>",
      "<|endoftext|>",
      "<|im_end|>",
      "<|quad_start|>",
      "<|image_pad|>",
      "<|object_ref_start|>",
      "<|vision_start|>",
      "<|quad_end|>",
      "<|im_start|>",
      "<|box_start|>",
      "<|box_end|>",
      "<|object_ref_end|>",
      "<|vision_end|>"
    ],
    "llama_special_tokens": [
      "<|begin_of_text|>",
      "<|eot_id|>"
    ],
    "common_special_tokens": []
  },
  "vocab_overlap": {
    "total_unique_tokens": 170355,
    "common_tokens_count": 109566,
    "qwen_only_count": 42099,
    "llama_only_count": 18690,
    "overlap_ratio": 0.6431628070793344,
    "qwen_coverage": 0.7224211255068737,
    "llama_coverage": 0.8542758233532934,
    "common_token_types": {
      "ascii_letters": 21,
      "digits": 0,
      "punctuation": 3,
      "whitespace": 0,
      "chinese": 0,
      "special_chars": 9,
      "subword_pieces": 48,
      "other": 19
    },
    "qwen_only_types": {
      "ascii_letters": 0,
      "digits": 0,
      "punctuation": 0,
      "whitespace": 0,
      "chinese": 0,
      "special_chars": 92,
      "subword_pieces": 8,
      "other": 0
    },
    "llama_only_types": {
      "ascii_letters": 6,
      "digits": 4,
      "punctuation": 0,
      "whitespace": 0,
      "chinese": 0,
      "special_chars": 41,
      "subword_pieces": 48,
      "other": 1
    }
  },
  "encoding_differences": [
    {
      "text": "Hello, how are you?",
      "qwen_tokens": [
        9707,
        11,
        1246,
        525,
        498,
        30
      ],
      "llama_tokens": [
        9906,
        11,
        1268,
        527,
        499,
        30
      ],
      "qwen_decoded": [
        "Hello",
        ",",
        " how",
        " are",
        " you",
        "?"
      ],
      "llama_decoded": [
        "Hello",
        ",",
        " how",
        " are",
        " you",
        "?"
      ],
      "qwen_length": 6,
      "llama_length": 6,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 3.1666666666666665,
        "llama": 3.1666666666666665
      }
    },
    {
      "text": "The quick brown fox jumps over the lazy dog.",
      "qwen_tokens": [
        785,
        3974,
        13876,
        38835,
        34208,
        916,
        279,
        15678,
        5562,
        13
      ],
      "llama_tokens": [
        791,
        4062,
        14198,
        39935,
        35308,
        927,
        279,
        16053,
        5679,
        13
      ],
      "qwen_decoded": [
        "The",
        " quick",
        " brown",
        " fox",
        " jumps",
        " over",
        " the",
        " lazy",
        " dog",
        "."
      ],
      "llama_decoded": [
        "The",
        " quick",
        " brown",
        " fox",
        " jumps",
        " over",
        " the",
        " lazy",
        " dog",
        "."
      ],
      "qwen_length": 10,
      "llama_length": 10,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 4.4,
        "llama": 4.4
      }
    },
    {
      "text": "Machine learning is a subset of artificial intelligence.",
      "qwen_tokens": [
        21605,
        6832,
        374,
        264,
        25993,
        315,
        20443,
        11229,
        13
      ],
      "llama_tokens": [
        22333,
        6975,
        374,
        264,
        27084,
        315,
        21075,
        11478,
        13
      ],
      "qwen_decoded": [
        "Machine",
        " learning",
        " is",
        " a",
        " subset",
        " of",
        " artificial",
        " intelligence",
        "."
      ],
      "llama_decoded": [
        "Machine",
        " learning",
        " is",
        " a",
        " subset",
        " of",
        " artificial",
        " intelligence",
        "."
      ],
      "qwen_length": 9,
      "llama_length": 9,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 6.222222222222222,
        "llama": 6.222222222222222
      }
    },
    {
      "text": "This is a longer sentence with multiple words and punctuation marks!",
      "qwen_tokens": [
        1986,
        374,
        264,
        5021,
        11652,
        448,
        5248,
        4244,
        323,
        61503,
        15423,
        0
      ],
      "llama_tokens": [
        2028,
        374,
        264,
        5129,
        11914,
        449,
        5361,
        4339,
        323,
        62603,
        15785,
        0
      ],
      "qwen_decoded": [
        "This",
        " is",
        " a",
        " longer",
        " sentence",
        " with",
        " multiple",
        " words",
        " and",
        " punctuation",
        " marks",
        "!"
      ],
      "llama_decoded": [
        "This",
        " is",
        " a",
        " longer",
        " sentence",
        " with",
        " multiple",
        " words",
        " and",
        " punctuation",
        " marks",
        "!"
      ],
      "qwen_length": 12,
      "llama_length": 12,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 5.666666666666667,
        "llama": 5.666666666666667
      }
    },
    {
      "text": "你好，你好吗？",
      "qwen_tokens": [
        108386,
        3837,
        108386,
        101037,
        11319
      ],
      "llama_tokens": [
        57668,
        53901,
        104660,
        53901,
        103054,
        11571
      ],
      "qwen_decoded": [
        "你好",
        "，",
        "你好",
        "吗",
        "？"
      ],
      "llama_decoded": [
        "你",
        "好",
        "，你",
        "好",
        "吗",
        "？"
      ],
      "qwen_length": 5,
      "llama_length": 6,
      "length_ratio": 1.2,
      "compression_efficiency": {
        "qwen": 1.4,
        "llama": 1.1666666666666667
      }
    },
    {
      "text": "机器学习是人工智能的重要分支。",
      "qwen_tokens": [
        102182,
        100134,
        20412,
        104455,
        101945,
        103799,
        1773
      ],
      "llama_tokens": [
        33748,
        32648,
        111478,
        21043,
        17792,
        49792,
        118034,
        9554,
        107693,
        17620,
        46456,
        1811
      ],
      "qwen_decoded": [
        "机器",
        "学习",
        "是",
        "人工智能",
        "的重要",
        "分支",
        "。"
      ],
      "llama_decoded": [
        "机",
        "器",
        "学习",
        "是",
        "人",
        "工",
        "智能",
        "的",
        "重要",
        "分",
        "支",
        "。"
      ],
      "qwen_length": 7,
      "llama_length": 12,
      "length_ratio": 1.7142857142857142,
      "compression_efficiency": {
        "qwen": 2.142857142857143,
        "llama": 1.25
      }
    },
    {
      "text": "这是一个包含多个词汇和标点符号的较长句子！",
      "qwen_tokens": [
        105464,
        102298,
        101213,
        110376,
        33108,
        30844,
        27442,
        108872,
        9370,
        112228,
        109949,
        6313
      ],
      "llama_tokens": [
        114880,
        48044,
        120610,
        43240,
        19483,
        106015,
        113912,
        34208,
        31944,
        28542,
        39404,
        18476,
        9554,
        105842,
        46961,
        106808,
        45829,
        6447
      ],
      "qwen_decoded": [
        "这是一个",
        "包含",
        "多个",
        "词汇",
        "和",
        "标",
        "点",
        "符号",
        "的",
        "较长",
        "句子",
        "！"
      ],
      "llama_decoded": [
        "这是",
        "一个",
        "包含",
        "多",
        "个",
        "词",
        "汇",
        "和",
        "标",
        "点",
        "符",
        "号",
        "的",
        "较",
        "长",
        "句",
        "子",
        "！"
      ],
      "qwen_length": 12,
      "llama_length": 18,
      "length_ratio": 1.5,
      "compression_efficiency": {
        "qwen": 1.75,
        "llama": 1.1666666666666667
      }
    },
    {
      "text": "深度学习模型在自然语言处理任务中表现出色。",
      "qwen_tokens": [
        102217,
        100134,
        104949,
        18493,
        99795,
        102064,
        54542,
        88802,
        15946,
        107837,
        38035,
        1773
      ],
      "llama_tokens": [
        102987,
        27479,
        111478,
        123123,
        19000,
        109683,
        120074,
        55642,
        89902,
        16325,
        125993,
        20834,
        39135,
        1811
      ],
      "qwen_decoded": [
        "深度",
        "学习",
        "模型",
        "在",
        "自然",
        "语言",
        "处理",
        "任务",
        "中",
        "表现出",
        "色",
        "。"
      ],
      "llama_decoded": [
        "深",
        "度",
        "学习",
        "模型",
        "在",
        "自然",
        "语言",
        "处理",
        "任务",
        "中",
        "表现",
        "出",
        "色",
        "。"
      ],
      "qwen_length": 12,
      "llama_length": 14,
      "length_ratio": 1.1666666666666667,
      "compression_efficiency": {
        "qwen": 1.75,
        "llama": 1.5
      }
    },
    {
      "text": "Hello 你好 world 世界",
      "qwen_tokens": [
        9707,
        220,
        108386,
        1879,
        220,
        99489
      ],
      "llama_tokens": [
        9906,
        118195,
        53901,
        1917,
        127365
      ],
      "qwen_decoded": [
        "Hello",
        " ",
        "你好",
        " world",
        " ",
        "世界"
      ],
      "llama_decoded": [
        "Hello",
        " 你",
        "好",
        " world",
        " 世界"
      ],
      "qwen_length": 6,
      "llama_length": 5,
      "length_ratio": 0.8333333333333334,
      "compression_efficiency": {
        "qwen": 2.8333333333333335,
        "llama": 3.4
      }
    },
    {
      "text": "Machine Learning 机器学习 is amazing!",
      "qwen_tokens": [
        21605,
        20909,
        220,
        102182,
        100134,
        374,
        7897,
        0
      ],
      "llama_tokens": [
        22333,
        21579,
        220,
        33748,
        32648,
        111478,
        374,
        8056,
        0
      ],
      "qwen_decoded": [
        "Machine",
        " Learning",
        " ",
        "机器",
        "学习",
        " is",
        " amazing",
        "!"
      ],
      "llama_decoded": [
        "Machine",
        " Learning",
        " ",
        "机",
        "器",
        "学习",
        " is",
        " amazing",
        "!"
      ],
      "qwen_length": 8,
      "llama_length": 9,
      "length_ratio": 1.125,
      "compression_efficiency": {
        "qwen": 4.125,
        "llama": 3.6666666666666665
      }
    },
    {
      "text": "Special chars: @#$%^&*()_+-=[]{}|;':\",./<>?",
      "qwen_tokens": [
        20366,
        23000,
        25,
        569,
        48077,
        45899,
        5,
        9,
        368,
        62,
        21473,
        30692,
        6257,
        91,
        26,
        1210,
        497,
        1725,
        21122,
        30
      ],
      "llama_tokens": [
        20989,
        23861,
        25,
        571,
        49177,
        46999,
        5,
        9,
        368,
        62,
        22192,
        31792,
        6390,
        91,
        26,
        1232,
        498,
        1761,
        21806,
        30
      ],
      "qwen_decoded": [
        "Special",
        " chars",
        ":",
        " @",
        "#$",
        "%^",
        "&",
        "*",
        "()",
        "_",
        "+-",
        "=[]",
        "{}",
        "|",
        ";",
        "':",
        "\",",
        "./",
        "<>",
        "?"
      ],
      "llama_decoded": [
        "Special",
        " chars",
        ":",
        " @",
        "#$",
        "%^",
        "&",
        "*",
        "()",
        "_",
        "+-",
        "=[]",
        "{}",
        "|",
        ";",
        "':",
        "\",",
        "./",
        "<>",
        "?"
      ],
      "qwen_length": 20,
      "llama_length": 20,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 2.15,
        "llama": 2.15
      }
    },
    {
      "text": "Numbers: 1234567890",
      "qwen_tokens": [
        27237,
        25,
        220,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        15
      ],
      "llama_tokens": [
        28336,
        25,
        220,
        4513,
        10961,
        16474,
        15
      ],
      "qwen_decoded": [
        "Numbers",
        ":",
        " ",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "0"
      ],
      "llama_decoded": [
        "Numbers",
        ":",
        " ",
        "123",
        "456",
        "789",
        "0"
      ],
      "qwen_length": 13,
      "llama_length": 7,
      "length_ratio": 0.5384615384615384,
      "compression_efficiency": {
        "qwen": 1.4615384615384615,
        "llama": 2.7142857142857144
      }
    },
    {
      "text": "Mixed: abc123中文!@#",
      "qwen_tokens": [
        86433,
        25,
        39022,
        16,
        17,
        18,
        104811,
        0,
        31,
        2
      ],
      "llama_tokens": [
        87533,
        25,
        40122,
        4513,
        108891,
        0,
        31,
        2
      ],
      "qwen_decoded": [
        "Mixed",
        ":",
        " abc",
        "1",
        "2",
        "3",
        "中文",
        "!",
        "@",
        "#"
      ],
      "llama_decoded": [
        "Mixed",
        ":",
        " abc",
        "123",
        "中文",
        "!",
        "@",
        "#"
      ],
      "qwen_length": 10,
      "llama_length": 8,
      "length_ratio": 0.8,
      "compression_efficiency": {
        "qwen": 1.8,
        "llama": 2.25
      }
    },
    {
      "text": "def hello_world(): print('Hello, World!')",
      "qwen_tokens": [
        750,
        23811,
        31792,
        4555,
        1173,
        492,
        9707,
        11,
        4337,
        0,
        863
      ],
      "llama_tokens": [
        755,
        24748,
        32892,
        4658,
        1194,
        493,
        9906,
        11,
        4435,
        0,
        873
      ],
      "qwen_decoded": [
        "def",
        " hello",
        "_world",
        "():",
        " print",
        "('",
        "Hello",
        ",",
        " World",
        "!",
        "')"
      ],
      "llama_decoded": [
        "def",
        " hello",
        "_world",
        "():",
        " print",
        "('",
        "Hello",
        ",",
        " World",
        "!",
        "')"
      ],
      "qwen_length": 11,
      "llama_length": 11,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 3.727272727272727,
        "llama": 3.727272727272727
      }
    },
    {
      "text": "import torch\nfrom transformers import AutoTokenizer",
      "qwen_tokens": [
        474,
        7834,
        198,
        1499,
        86870,
        1159,
        8979,
        37434
      ],
      "llama_tokens": [
        475,
        7990,
        198,
        1527,
        87970,
        1179,
        9156,
        38534
      ],
      "qwen_decoded": [
        "import",
        " torch",
        "\n",
        "from",
        " transformers",
        " import",
        " Auto",
        "Tokenizer"
      ],
      "llama_decoded": [
        "import",
        " torch",
        "\n",
        "from",
        " transformers",
        " import",
        " Auto",
        "Tokenizer"
      ],
      "qwen_length": 8,
      "llama_length": 8,
      "length_ratio": 1.0,
      "compression_efficiency": {
        "qwen": 6.375,
        "llama": 6.375
      }
    },
    {
      "text": "This is a very long sentence that contains many words and should help us understand how different tokenizers handle longer sequences of text with various types of content including punctuation and numbers like 123 and special characters.",
      "qwen_tokens": [
        1986,
        374,
        264,
        1602,
        1293,
        11652,
        429,
        5610,
        1657,
        4244,
        323,
        1265,
        1492,
        601,
        3535,
        1246,
        2155,
        3950,
        12230,
        3705,
        5021,
        23700,
        315,
        1467,
        448,
        5257,
        4494,
        315,
        2213,
        2670,
        61503,
        323,
        5109,
        1075,
        220,
        16,
        17,
        18,
        323,
        3281,
        5766,
        13
      ],
      "llama_tokens": [
        2028,
        374,
        264,
        1633,
        1317,
        11914,
        430,
        5727,
        1690,
        4339,
        323,
        1288,
        1520,
        603,
        3619,
        1268,
        2204,
        4037,
        12509,
        3790,
        5129,
        24630,
        315,
        1495,
        449,
        5370,
        4595,
        315,
        2262,
        2737,
        62603,
        323,
        5219,
        1093,
        220,
        4513,
        323,
        3361,
        5885,
        13
      ],
      "qwen_decoded": [
        "This",
        " is",
        " a",
        " very",
        " long",
        " sentence",
        " that",
        " contains",
        " many",
        " words",
        " and",
        " should",
        " help",
        " us",
        " understand",
        " how",
        " different",
        " token",
        "izers",
        " handle",
        " longer",
        " sequences",
        " of",
        " text",
        " with",
        " various",
        " types",
        " of",
        " content",
        " including",
        " punctuation",
        " and",
        " numbers",
        " like",
        " ",
        "1",
        "2",
        "3",
        " and",
        " special",
        " characters",
        "."
      ],
      "llama_decoded": [
        "This",
        " is",
        " a",
        " very",
        " long",
        " sentence",
        " that",
        " contains",
        " many",
        " words",
        " and",
        " should",
        " help",
        " us",
        " understand",
        " how",
        " different",
        " token",
        "izers",
        " handle",
        " longer",
        " sequences",
        " of",
        " text",
        " with",
        " various",
        " types",
        " of",
        " content",
        " including",
        " punctuation",
        " and",
        " numbers",
        " like",
        " ",
        "123",
        " and",
        " special",
        " characters",
        "."
      ],
      "qwen_length": 42,
      "llama_length": 40,
      "length_ratio": 0.9523809523809523,
      "compression_efficiency": {
        "qwen": 5.642857142857143,
        "llama": 5.925
      }
    }
  ],
  "summary": {
    "vocab_size_ratio": 0.8456532489368015,
    "vocab_overlap_ratio": 0.6431628070793344,
    "avg_length_ratio": 1.0518830128205128,
    "avg_compression_efficiency": {
      "qwen": 3.4133383977133978,
      "llama": 3.421694624819625
    },
    "compatibility_score": 0.7734521574722406,
    "recommendations": [
      "建议优先迁移非嵌入层（attention和MLP层）",
      "考虑实施tokenizer-aware的嵌入层对齐策略"
    ]
  }
}