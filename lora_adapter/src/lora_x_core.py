#!/usr/bin/env python3
"""
LoRA-Xæ ¸å¿ƒå®ç°æ¨¡å—
åŸºäºè®ºæ–‡: LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Tuple, List, Optional
from safetensors import safe_open
import logging

logger = logging.getLogger(__name__)


class LoRAXCore:
    """LoRA-Xæ ¸å¿ƒç®—æ³•å®ç°"""
    
    def __init__(self, rank: int = 320, similarity_threshold: float = 0.3):
        """
        Args:
            rank: SVDæˆªæ–­ç§©ï¼Œè®ºæ–‡ä¸­æ¨è320
            similarity_threshold: å­ç©ºé—´ç›¸ä¼¼æ€§é˜ˆå€¼
        """
        self.rank = rank
        self.similarity_threshold = similarity_threshold
        
    def compute_svd_subspace(self, weight_matrix: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è®¡ç®—æƒé‡çŸ©é˜µçš„SVDå­ç©ºé—´åˆ†è§£ - CUDAåŠ é€Ÿç‰ˆæœ¬
        
        Args:
            weight_matrix: æƒé‡çŸ©é˜µ W âˆˆ R^(mÃ—n)
            
        Returns:
            U_truncated: æˆªæ–­çš„å·¦å¥‡å¼‚çŸ©é˜µ Å¨ âˆˆ R^(mÃ—r)
            S_truncated: æˆªæ–­çš„å¥‡å¼‚å€¼ sÌƒ âˆˆ R^r  
            Vh_truncated: æˆªæ–­çš„å³å¥‡å¼‚çŸ©é˜µ á¹¼^T âˆˆ R^(rÃ—n)
        """
        # ç¡®ä¿åœ¨CUDAè®¾å¤‡ä¸Šè®¡ç®—
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        weight_matrix = weight_matrix.to(device)
        
        # æ‰§è¡ŒSVDåˆ†è§£
        U, S, Vh = torch.linalg.svd(weight_matrix.float(), full_matrices=False)
        
        # æˆªæ–­åˆ°æŒ‡å®šrank
        effective_rank = min(self.rank, min(weight_matrix.shape))
        U_truncated = U[:, :effective_rank]
        S_truncated = S[:effective_rank] 
        Vh_truncated = Vh[:effective_rank, :]
        
        return U_truncated, S_truncated, Vh_truncated
    
    def compute_subspace_similarity(self, U_source: torch.Tensor, U_target: torch.Tensor) -> float:
        """
        è®¡ç®—å­ç©ºé—´ç›¸ä¼¼æ€§ - CUDAåŠ é€Ÿç‰ˆæœ¬
        ä½¿ç”¨Frobeniuså†…ç§¯: ||U_s^T U_t||_F^2 / (||U_s||_F^2 * ||U_t||_F^2)
            
        Returns:
            similarity: ç›¸ä¼¼æ€§åˆ†æ•° [0, 1]
        """
        # ç¡®ä¿åœ¨CUDAè®¾å¤‡ä¸Šè®¡ç®—
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        U_source = U_source.to(device)
        U_target = U_target.to(device)
        
        # å¤„ç†ç»´åº¦å®Œå…¨ä¸åŒ¹é…çš„æƒ…å†µ
        if U_source.shape[0] != U_target.shape[0]:
            # å¦‚æœè¡Œç»´åº¦ä¸åŒï¼Œä½¿ç”¨æŠ•å½±æ–¹æ³•è®¡ç®—ç›¸ä¼¼æ€§
            min_rows = min(U_source.shape[0], U_target.shape[0])
            min_cols = min(U_source.shape[1], U_target.shape[1])
            
            # æˆªæ–­åˆ°æœ€å°ç»´åº¦
            U_s = U_source[:min_rows, :min_cols]
            U_t = U_target[:min_rows, :min_cols]
        else:
            # è¡Œç»´åº¦ç›¸åŒï¼Œåªå¤„ç†åˆ—ç»´åº¦
            min_dim = min(U_source.shape[1], U_target.shape[1])
            U_s = U_source[:, :min_dim]
            U_t = U_target[:, :min_dim]
        
        # è®¡ç®—å†…ç§¯çŸ©é˜µ
        inner_product = torch.mm(U_s.T, U_t)
        
        # è®¡ç®—FrobeniusèŒƒæ•°å¹³æ–¹ä½œä¸ºç›¸ä¼¼æ€§åº¦é‡
        similarity = torch.norm(inner_product, p='fro')**2 / (U_s.shape[1] * U_t.shape[1])
        
        return similarity.item()
    
    def transfer_lora_weights(self, 
                            source_lora: Dict[str, torch.Tensor],
                            target_base_weights: Dict[str, torch.Tensor],
                            source_base_weights: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        æ‰§è¡ŒLoRA-Xè¿ç§»
        
        Args:
            source_lora: æºæ¨¡å‹LoRAæƒé‡å­—å…¸
            target_base_weights: ç›®æ ‡æ¨¡å‹åŸºç¡€æƒé‡  
            source_base_weights: æºæ¨¡å‹åŸºç¡€æƒé‡
            
        Returns:
            transferred_lora: è¿ç§»åçš„LoRAæƒé‡å­—å…¸
        """
        transferred_lora = {}
        transfer_stats = {
            'total_layers': 0,
            'transferred_layers': 0, 
            'skipped_layers': [],
            'skipped_reasons': {},
            'similarity_stats': [],
            'layer_types': {}
        }
        
        # é¢„è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦ï¼ˆå¹¶è¡ŒåŒ–ï¼‰
        logger.info("é¢„è®¡ç®—æ‰€æœ‰å±‚çš„ç›¸ä¼¼åº¦...")
        similarities = self._precompute_similarities_parallel(source_lora, source_base_weights, target_base_weights)
        
        # éå†æºLoRAæƒé‡
        for lora_key in source_lora.keys():
            if not lora_key.endswith('.weight'):
                continue

            transfer_stats['total_layers'] += 1
            
            # åˆ†æå±‚ç±»å‹
            layer_type = self._classify_layer_type(lora_key)
            if layer_type not in transfer_stats['layer_types']:
                transfer_stats['layer_types'][layer_type] = {'total': 0, 'transferred': 0}
            transfer_stats['layer_types'][layer_type]['total'] += 1

            # æ‰¾åˆ°å¯¹åº”çš„åŸºç¡€æƒé‡
            base_key = self._map_lora_to_base_key(lora_key)

            if base_key not in source_base_weights or base_key not in target_base_weights:
                reason = "æ‰¾ä¸åˆ°å¯¹åº”çš„åŸºç¡€æƒé‡"
                logger.warning(f"è·³è¿‡å±‚ {lora_key}: {reason}")
                transfer_stats['skipped_layers'].append(lora_key)
                transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # è·å–æƒé‡
            source_base = source_base_weights[base_key]
            target_base = target_base_weights[base_key]
            lora_weight = source_lora[lora_key]

            # ä½¿ç”¨é¢„è®¡ç®—çš„ç›¸ä¼¼åº¦
            similarity = similarities.get(base_key, 0.0)
            transfer_stats['similarity_stats'].append({
                'layer': lora_key,
                'similarity': similarity,
                'layer_type': layer_type
            })

            # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
            if not self._check_dimension_compatibility(source_base, target_base, lora_weight):
                logger.info(f"å±‚ {lora_key} ç»´åº¦ä¸å…¼å®¹ï¼Œé‡‡ç”¨Frobeniusæœ€å°åŒ–æŠ•å½±")
                try:
                    projected_weight = self._frobenius_projection(lora_weight, source_base, target_base)
                    transferred_lora[lora_key] = projected_weight
                    transfer_stats['transferred_layers'] += 1
                    transfer_stats['layer_types'][layer_type]['transferred'] += 1
                    logger.info(f"æˆåŠŸè¿ç§»å±‚ {lora_key} (FrobeniusæŠ•å½±, ç›¸ä¼¼æ€§={similarity:.3f})")
                except Exception as e:
                    reason = f"FrobeniusæŠ•å½±å¤±è´¥: {e}"
                    logger.warning(reason)
                    transfer_stats['skipped_layers'].append(lora_key)
                    transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # ç›¸ä¼¼æ€§è¿‡æ»¤
            if similarity < self.similarity_threshold:
                reason = f"ç›¸ä¼¼æ€§è¿‡ä½ ({similarity:.3f} < {self.similarity_threshold})"
                logger.info(f"è·³è¿‡å±‚ {lora_key}: {reason}")
                transfer_stats['skipped_layers'].append(lora_key)
                transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # æ‰§è¡Œè¿ç§»
            transferred_weight = self._transfer_single_layer(lora_weight, source_base, target_base)
            transferred_lora[lora_key] = transferred_weight
            transfer_stats['transferred_layers'] += 1
            transfer_stats['layer_types'][layer_type]['transferred'] += 1
            logger.info(f"æˆåŠŸè¿ç§»å±‚ {lora_key}: ç›¸ä¼¼æ€§={similarity:.3f}")
        
        self._log_transfer_stats(transfer_stats)
        return transferred_lora
    
    def _precompute_similarities_parallel(self, source_lora: Dict[str, torch.Tensor], 
                                        source_base_weights: Dict[str, torch.Tensor],
                                        target_base_weights: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """å¹¶è¡Œé¢„è®¡ç®—æ‰€æœ‰å±‚çš„ç›¸ä¼¼åº¦"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        similarities = {}
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦è®¡ç®—çš„å±‚
        valid_keys = []
        for lora_key in source_lora.keys():
            if not lora_key.endswith('.weight'):
                continue
            base_key = self._map_lora_to_base_key(lora_key)
            if base_key in source_base_weights and base_key in target_base_weights:
                valid_keys.append(base_key)
        
        # å»é‡
        unique_keys = list(set(valid_keys))
        logger.info(f"éœ€è¦è®¡ç®—ç›¸ä¼¼åº¦çš„å”¯ä¸€å±‚æ•°: {len(unique_keys)}")
        
        # æ‰¹é‡è®¡ç®—SVDå’Œç›¸ä¼¼åº¦
        for i, base_key in enumerate(unique_keys):
            if i % 10 == 0:
                logger.info(f"è®¡ç®—è¿›åº¦: {i}/{len(unique_keys)}")
            
            try:
                source_base = source_base_weights[base_key].to(device)
                target_base = target_base_weights[base_key].to(device)
                
                U_s, _, _ = self.compute_svd_subspace(source_base)
                U_t, _, _ = self.compute_svd_subspace(target_base)
                similarity = self.compute_subspace_similarity(U_s, U_t)
                similarities[base_key] = similarity
            except Exception as e:
                logger.warning(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥ {base_key}: {e}")
                similarities[base_key] = 0.0
        
        return similarities
    
    def _transfer_single_layer(self, 
                              lora_weight: torch.Tensor,
                              source_base: torch.Tensor, 
                              target_base: torch.Tensor) -> torch.Tensor:
        """
        å•å±‚LoRAæƒé‡è¿ç§»
        å®ç°è®ºæ–‡å…¬å¼3: âˆ†W_tâ†s = U_t U_t^T âˆ†W_s V_t V_t^T
        """
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = lora_weight.device
        target_base = target_base.to(device)
        
        # è·å–ç›®æ ‡æ¨¡å‹å­ç©ºé—´
        U_t, _, Vh_t = self.compute_svd_subspace(target_base)
        V_t = Vh_t.T
        
        # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§ï¼Œå¦‚æœä¸å…¼å®¹åˆ™ä½¿ç”¨FrobeniusæŠ•å½±
        try:
            # å­ç©ºé—´æŠ•å½±è¿ç§»
            # âˆ†W_tâ†s = U_t U_t^T âˆ†W_s V_t V_t^T
            projected_weight = torch.mm(torch.mm(U_t, U_t.T), torch.mm(lora_weight, torch.mm(V_t, V_t.T)))
        except RuntimeError as e:
            if "cannot be multiplied" in str(e):
                # ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨FrobeniusæŠ•å½±
                logger.info(f"ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨FrobeniusæŠ•å½±: {lora_weight.shape} vs ç›®æ ‡å­ç©ºé—´")
                projected_weight = self._frobenius_projection(lora_weight, lora_weight, target_base)
            else:
                raise e
        
        return projected_weight
    
    def _classify_layer_type(self, lora_key: str) -> str:
        """åˆ†ç±»LoRAå±‚ç±»å‹"""
        if 'q_proj' in lora_key:
            return 'query'
        elif 'k_proj' in lora_key:
            return 'key'
        elif 'v_proj' in lora_key:
            return 'value'
        elif 'o_proj' in lora_key:
            return 'output'
        elif 'gate_proj' in lora_key:
            return 'gate'
        elif 'up_proj' in lora_key:
            return 'up'
        elif 'down_proj' in lora_key:
            return 'down'
        elif 'mlp' in lora_key:
            return 'mlp'
        elif 'attn' in lora_key:
            return 'attention'
        else:
            return 'other'
    
    def _map_lora_to_base_key(self, lora_key: str) -> str:
        """æ˜ å°„LoRAæƒé‡ååˆ°åŸºç¡€æ¨¡å‹æƒé‡å"""
        # ç§»é™¤LoRAç‰¹å®šçš„å‰ç¼€/åç¼€
        # ä¾‹å¦‚: base_model.model.model.layers.0.mlp.down_proj.lora_A.weight
        # æ˜ å°„ä¸º: model.layers.0.mlp.down_proj.weight
        base_key = lora_key.replace('base_model.model.', '').replace('.lora_A', '').replace('.lora_B', '')
        return base_key
    
    def _check_dimension_compatibility(self, 
                                     source_base: torch.Tensor, 
                                     target_base: torch.Tensor, 
                                     lora_weight: torch.Tensor) -> bool:
        """æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§"""
        # åŸºç¡€æ£€æŸ¥
        if source_base.dim() != 2 or target_base.dim() != 2:
            return False
        
        # LoRAæƒé‡åº”è¯¥ä¸æŸä¸ªç»´åº¦åŒ¹é…
        s_shape = source_base.shape
        t_shape = target_base.shape
        l_shape = lora_weight.shape
        
        # ç®€åŒ–çš„å…¼å®¹æ€§æ£€æŸ¥
        return len(l_shape) == 2
    
    def _frobenius_projection(self, lora_weight: torch.Tensor, source_base: torch.Tensor, target_base: torch.Tensor) -> torch.Tensor:
        """
        å¯¹äºç»´åº¦ä¸å…¼å®¹çš„å±‚ï¼Œé‡‡ç”¨LoRAç»“æ„æ„ŸçŸ¥çš„æŠ•å½±
        ä¿æŒLoRAçš„ä½ç§©ç»“æ„ (rank=16)
        """
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = lora_weight.device
        source_base = source_base.to(device)
        target_base = target_base.to(device)
        
        # è·å–ç›®æ ‡å½¢çŠ¶å’ŒLoRA rank
        target_shape = target_base.shape
        lora_shape = lora_weight.shape
        lora_rank = 16  # å›ºå®šrank=16
        
        # å¦‚æœå½¢çŠ¶å®Œå…¨åŒ¹é…ï¼Œç›´æ¥è¿”å›
        if lora_shape == target_shape:
            return lora_weight
        
        # åˆ¤æ–­æ˜¯lora_Aè¿˜æ˜¯lora_Bæƒé‡
        if len(lora_shape) == 2 and len(target_shape) == 2:
            if lora_shape[0] == lora_rank:
                # è¿™æ˜¯lora_Aæƒé‡: [16, input_dim] -> [16, target_input_dim]
                target_lora_shape = (lora_rank, target_shape[1])
                logger.info(f"å¤„ç†lora_Aæƒé‡: {lora_shape} -> {target_lora_shape}")
                
                # ä½¿ç”¨SVDæŠ•å½±ä¿æŒrank=16
                U, S, Vh = torch.linalg.svd(lora_weight.float(), full_matrices=False)
                # æˆªæ–­åˆ°rank=16
                rank = min(lora_rank, U.shape[1], Vh.shape[0])
                U_truncated = U[:, :rank]
                S_truncated = S[:rank]
                Vh_truncated = Vh[:rank, :]
                
                # é‡æ„åˆ°ç›®æ ‡ç»´åº¦
                if target_lora_shape[1] <= lora_shape[1]:
                    # ç›®æ ‡ç»´åº¦è¾ƒå°ï¼Œç›´æ¥æˆªæ–­
                    projected = U_truncated @ torch.diag(S_truncated) @ Vh_truncated[:, :target_lora_shape[1]]
                else:
                    # ç›®æ ‡ç»´åº¦è¾ƒå¤§ï¼Œé›¶å¡«å……
                    projected = torch.zeros(target_lora_shape, device=device, dtype=lora_weight.dtype)
                    reconstructed = U_truncated @ torch.diag(S_truncated) @ Vh_truncated
                    projected[:, :reconstructed.shape[1]] = reconstructed
                
                return projected
                
            elif lora_shape[1] == lora_rank:
                # è¿™æ˜¯lora_Bæƒé‡: [output_dim, 16] -> [target_output_dim, 16]
                target_lora_shape = (target_shape[0], lora_rank)
                logger.info(f"å¤„ç†lora_Bæƒé‡: {lora_shape} -> {target_lora_shape}")
                
                # ä½¿ç”¨SVDæŠ•å½±ä¿æŒrank=16
                U, S, Vh = torch.linalg.svd(lora_weight.float(), full_matrices=False)
                # æˆªæ–­åˆ°rank=16
                rank = min(lora_rank, U.shape[1], Vh.shape[0])
                U_truncated = U[:, :rank]
                S_truncated = S[:rank]
                Vh_truncated = Vh[:rank, :]
                
                # é‡æ„åˆ°ç›®æ ‡ç»´åº¦
                if target_lora_shape[0] <= lora_shape[0]:
                    # ç›®æ ‡ç»´åº¦è¾ƒå°ï¼Œç›´æ¥æˆªæ–­
                    projected = U_truncated[:target_lora_shape[0], :] @ torch.diag(S_truncated) @ Vh_truncated
                else:
                    # ç›®æ ‡ç»´åº¦è¾ƒå¤§ï¼Œé›¶å¡«å……
                    projected = torch.zeros(target_lora_shape, device=device, dtype=lora_weight.dtype)
                    reconstructed = U_truncated @ torch.diag(S_truncated) @ Vh_truncated
                    projected[:reconstructed.shape[0], :] = reconstructed
                
                return projected
            else:
                # éæ ‡å‡†LoRAå½¢çŠ¶ï¼Œä½¿ç”¨é€šç”¨æŠ•å½±
                logger.warning(f"éæ ‡å‡†LoRAå½¢çŠ¶ï¼Œä½¿ç”¨é€šç”¨æŠ•å½±: {lora_shape} -> {target_shape}")
                projected = torch.zeros(target_shape, device=device, dtype=lora_weight.dtype)
                min_rows = min(lora_shape[0], target_shape[0])
                min_cols = min(lora_shape[1], target_shape[1])
                projected[:min_rows, :min_cols] = lora_weight[:min_rows, :min_cols]
                return projected
        else:
            # å¯¹äºå…¶ä»–æƒ…å†µï¼Œè¿”å›éšæœºåˆå§‹åŒ–çš„æƒé‡
            logger.warning(f"å½¢çŠ¶ä¸å…¼å®¹ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–: {lora_shape} -> {target_shape}")
            return torch.randn(target_shape, device=device, dtype=lora_weight.dtype) * 0.01

    def _log_transfer_stats(self, stats: Dict):
        """è®°å½•è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print("LoRA-X è¿ç§»ç»Ÿè®¡æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“Š æ€»å±‚æ•°: {stats['total_layers']}")
        print(f"âœ… æˆåŠŸè¿ç§»: {stats['transferred_layers']}")
        print(f"âŒ è·³è¿‡å±‚æ•°: {len(stats['skipped_layers'])}")
        
        if stats['transferred_layers'] > 0:
            success_rate = (stats['transferred_layers'] / stats['total_layers']) * 100
            print(f"ğŸ“ˆ è¿ç§»æˆåŠŸç‡: {success_rate:.1f}%")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚!")
        
        if stats['skipped_layers']:
            print(f"\nğŸ” è·³è¿‡çš„å±‚è¯¦æƒ…:")
            for i, layer in enumerate(stats['skipped_layers'][:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                print(f"  {i+1}. {layer}")
            if len(stats['skipped_layers']) > 10:
                print(f"  ... è¿˜æœ‰ {len(stats['skipped_layers']) - 10} ä¸ªå±‚è¢«è·³è¿‡")
        
        print("="*60)
        
        # åŒæ—¶è®°å½•åˆ°logger
        logger.info(f"LoRA-Xè¿ç§»å®Œæˆ:")
        logger.info(f"  æ€»å±‚æ•°: {stats['total_layers']}")
        logger.info(f"  æˆåŠŸè¿ç§»: {stats['transferred_layers']}")
        logger.info(f"  è·³è¿‡å±‚æ•°: {len(stats['skipped_layers'])}")
        if stats['skipped_layers']:
            logger.info(f"  è·³è¿‡çš„å±‚: {stats['skipped_layers'][:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
