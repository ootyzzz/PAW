"""
/root/PAW/lora_adapter/src/interpo_core.py
LoRA Linear Interpolation Baseline (ç›´æ¥æ’å€¼åˆ° rank=16)
"""

import torch
import logging
from typing import Dict

logger = logging.getLogger(__name__)

class InterpoCore:
    """LoRA Linear Interpolation Baseline"""

    def __init__(self, alpha: float = 0.5, rank: int = 16):
        """
        alpha: æ’å€¼ç³»æ•° (0=æº, 1=ç›®æ ‡)
        rank: å¼ºåˆ¶è¾“å‡ºçš„LoRAç§©
        """
        self.alpha = alpha
        self.rank = rank
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.used_padding = 0
        self.used_truncation = 0

    def _resize_weight(self, weight: torch.Tensor, target_shape: tuple) -> torch.Tensor:
        """
        ä½¿ç”¨çº¿æ€§æ’å€¼/è£å‰ªå¡«å……å°† weight è°ƒæ•´åˆ° target_shape
        """
        src_h, src_w = weight.shape
        tgt_h, tgt_w = target_shape
        resized = torch.zeros((tgt_h, tgt_w), device=self.device, dtype=torch.float32)

        # è¡Œé‡‡æ ·/æ’å€¼
        row_idx = torch.linspace(0, src_h - 1, steps=tgt_h).long()
        # åˆ—é‡‡æ ·/æ’å€¼
        col_idx = torch.linspace(0, src_w - 1, steps=tgt_w).long()

        resized = weight[row_idx][:, col_idx]

        if tgt_h > src_h or tgt_w > src_w:
            self.used_padding += 1
        if tgt_h < src_h or tgt_w < src_w:
            self.used_truncation += 1

        return resized

    def transfer_lora_weights(
        self,
        source_lora: Dict[str, torch.Tensor],
        target_base_weights: Dict[str, torch.Tensor],
    ) -> Dict[str, torch.Tensor]:
        """æ‰§è¡Œçº¿æ€§æ’å€¼è¿ç§»ï¼Œå¼ºåˆ¶è¾“å‡ºç›®æ ‡æ¨¡å‹å¯åŠ è½½çš„ rank=16 LoRA"""
        transferred_lora = {}
        total_layers = len([k for k in source_lora if k.endswith(".weight")])
        logger.info(f"å¼€å§‹æ‰§è¡ŒLoRAçº¿æ€§æ’å€¼ï¼Œæ€»è®¡ {total_layers} å±‚")

        processed = 0
        for lora_key, lora_weight in source_lora.items():
            if not lora_key.endswith(".weight"):
                continue

            base_key = (
                lora_key.replace("base_model.model.", "")
                .replace(".lora_A", "")
                .replace(".lora_B", "")
            )
            if base_key not in target_base_weights:
                continue

            target_shape = target_base_weights[base_key].shape

            if "lora_A" in lora_key:
                # A: [r, in_dim]
                target_shape = (self.rank, target_shape[1])
            elif "lora_B" in lora_key:
                # B: [out_dim, r]
                target_shape = (target_shape[0], self.rank)
            else:
                continue

            logger.info(
                f"ğŸ” [Layer: {lora_key}]\n"
                f"  æºLoRAç»´åº¦: {tuple(lora_weight.shape)}\n"
                f"  ç›®æ ‡LoRAç»´åº¦: {target_shape}"
            )

            projected = self._resize_weight(lora_weight.float(), target_shape)

            transferred_lora[lora_key] = projected.half()
            processed += 1

            if processed % 20 == 0:
                logger.info(f"[è¿›åº¦] å·²å¤„ç† {processed}/{total_layers}")

        logger.info(
            f"è¿ç§»å®Œæˆï¼Œæ€»å…±å¤„ç† {processed} å±‚ï¼Œ"
            f"æˆªæ–­ {self.used_truncation} å±‚ï¼Œ"
            f"å¡«å…… {self.used_padding} å±‚"
        )
        return transferred_lora
