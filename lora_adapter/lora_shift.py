#!/usr/bin/env python3
"""
è‡ªå®šä¹‰LoRAè¿ç§»è„šæœ¬
ä»Meta-Llama-3.1-8B-Instructè¿ç§»LoRAåˆ°Qwen2.5-7B-Instruct
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from lora_x_core import LoRAXCore
from model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")


def infer_source_model_path(lora_path: str) -> str:
    """æ ¹æ®LoRAè·¯å¾„æ¨æ–­æºæ¨¡å‹è·¯å¾„"""
    # ä» /root/autodl-tmp/loraed/Meta-Llama-3.1-8B-Instruct/250728_010944/final_model
    # æ¨æ–­ä¸º /root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct
    lora_path = Path(lora_path)
    
    # å¦‚æœè·¯å¾„ä»¥final_modelç»“å°¾ï¼Œå‘ä¸Šä¸¤çº§è·å–æ¨¡å‹å
    if lora_path.name == "final_model":
        # /root/autodl-tmp/loraed/Meta-Llama-3.1-8B-Instruct/250728_010944/final_model
        # å‘ä¸Šä¸¤çº§: Meta-Llama-3.1-8B-Instruct
        model_name = lora_path.parent.parent.name  
    else:
        # å¦åˆ™å‘ä¸Šä¸€çº§è·å–æ¨¡å‹å
        model_name = lora_path.parent.name  
    
    source_model_path = f"/root/autodl-tmp/models/{model_name}"
    return source_model_path


def print_layer_details(lora_key: str, base_key: str, similarity: float, 
                       source_shape: tuple, target_shape: tuple, 
                       lora_shape: tuple, status: str, reason: str = ""):
    """æ‰“å°å±‚çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"\n{'='*80}")
    print(f"ğŸ” å±‚åˆ†æ: {lora_key}")
    print(f"{'='*80}")
    print(f"ğŸ“‹ åŸºç¡€æƒé‡é”®: {base_key}")
    print(f"ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ•°: {similarity:.4f}")
    print(f"ğŸ“ æºæ¨¡å‹å½¢çŠ¶: {source_shape}")
    print(f"ğŸ“ ç›®æ ‡æ¨¡å‹å½¢çŠ¶: {target_shape}")
    print(f"ğŸ“ LoRAæƒé‡å½¢çŠ¶: {lora_shape}")
    print(f"ğŸ¯ å¤„ç†çŠ¶æ€: {status}")
    if reason:
        print(f"ğŸ’¡ åŸå› : {reason}")
    print(f"{'='*80}")


def print_transfer_summary(stats: dict):
    """æ‰“å°è¿ç§»æ€»ç»“"""
    print(f"\n{'ğŸ‰'*20} è¿ç§»å®Œæˆæ€»ç»“ {'ğŸ‰'*20}")
    print(f"{'='*80}")
    print(f"ğŸ“Š æ€»å±‚æ•°: {stats['total_layers']}")
    print(f"âœ… æˆåŠŸè¿ç§»: {stats['transferred_layers']}")
    print(f"âŒ è·³è¿‡å±‚æ•°: {len(stats['skipped_layers'])}")
    
    if stats['transferred_layers'] > 0:
        success_rate = (stats['transferred_layers'] / stats['total_layers']) * 100
        print(f"ğŸ“ˆ è¿ç§»æˆåŠŸç‡: {success_rate:.1f}%")
    else:
        print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚!")
    
    print(f"\nğŸ¯ æˆåŠŸè¿ç§»çš„å±‚:")
    for i, layer in enumerate(stats.get('transferred_list', []), 1):
        print(f"  {i:2d}. {layer}")
    
    if stats['skipped_layers']:
        print(f"\nâŒ è·³è¿‡çš„å±‚åŠåŸå› :")
        for i, layer in enumerate(stats['skipped_layers'], 1):
            reason = stats['skipped_reasons'].get(layer, "æœªçŸ¥åŸå› ")
            print(f"  {i:2d}. {layer}")
            print(f"      åŸå› : {reason}")
    
    # æŒ‰å±‚ç±»å‹ç»Ÿè®¡
    if 'layer_types' in stats:
        print(f"\nğŸ“‹ æŒ‰å±‚ç±»å‹ç»Ÿè®¡:")
        for layer_type, type_stats in stats['layer_types'].items():
            total = type_stats['total']
            transferred = type_stats['transferred']
            rate = (transferred / total * 100) if total > 0 else 0
            print(f"  {layer_type:12s}: {transferred:2d}/{total:2d} ({rate:5.1f}%)")
    
    print(f"{'='*80}")


class VerboseLoRAXCore(LoRAXCore):
    """å¢å¼ºç‰ˆLoRA-Xæ ¸å¿ƒç±»ï¼Œæä¾›è¯¦ç»†è¾“å‡º"""
    
    def transfer_lora_weights(self, 
                            source_lora: dict,
                            target_base_weights: dict,
                            source_base_weights: dict) -> dict:
        """æ‰§è¡ŒLoRA-Xè¿ç§»ï¼Œæä¾›è¯¦ç»†è¾“å‡º"""
        transferred_lora = {}
        transfer_stats = {
            'total_layers': 0,
            'transferred_layers': 0, 
            'transferred_list': [],
            'skipped_layers': [],
            'skipped_reasons': {},
            'similarity_stats': [],
            'layer_types': {}
        }
        
        print(f"\nğŸš€ å¼€å§‹LoRAæƒé‡è¿ç§»...")
        print(f"ğŸ“Š æºLoRAåŒ…å« {len(source_lora)} ä¸ªæƒé‡")
        print(f"ğŸ“Š æºæ¨¡å‹åŒ…å« {len(source_base_weights)} ä¸ªåŸºç¡€æƒé‡")
        print(f"ğŸ“Š ç›®æ ‡æ¨¡å‹åŒ…å« {len(target_base_weights)} ä¸ªåŸºç¡€æƒé‡")
        
        # é¢„è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        print(f"\nğŸ”„ é¢„è®¡ç®—å±‚ç›¸ä¼¼åº¦...")
        similarities = self._precompute_similarities_verbose(source_lora, source_base_weights, target_base_weights)
        
        # éå†æºLoRAæƒé‡
        for lora_key in source_lora.keys():
            if not lora_key.endswith('.weight'):
                continue

            transfer_stats['total_layers'] += 1
            
            # åˆ†æå±‚ç±»å‹
            layer_type = self._classify_layer_type(lora_key)
            if layer_type not in transfer_stats['layer_types']:
                transfer_stats['layer_types'][layer_type] = {'total': 0, 'transferred': 0}
            transfer_stats['layer_types'][layer_type]['total'] += 1

            # æ‰¾åˆ°å¯¹åº”çš„åŸºç¡€æƒé‡
            base_key = self._map_lora_to_base_key(lora_key)

            if base_key not in source_base_weights or base_key not in target_base_weights:
                reason = "æ‰¾ä¸åˆ°å¯¹åº”çš„åŸºç¡€æƒé‡"
                similarity = 0.0
                source_shape = "N/A"
                target_shape = "N/A"
                lora_shape = tuple(source_lora[lora_key].shape)
                
                print_layer_details(lora_key, base_key, similarity, 
                                  source_shape, target_shape, lora_shape, 
                                  "âŒ è·³è¿‡", reason)
                
                transfer_stats['skipped_layers'].append(lora_key)
                transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # è·å–æƒé‡å’Œå½¢çŠ¶ä¿¡æ¯
            source_base = source_base_weights[base_key]
            target_base = target_base_weights[base_key]
            lora_weight = source_lora[lora_key]
            
            source_shape = tuple(source_base.shape)
            target_shape = tuple(target_base.shape)
            lora_shape = tuple(lora_weight.shape)

            # è·å–ç›¸ä¼¼åº¦
            similarity = similarities.get(base_key, 0.0)
            transfer_stats['similarity_stats'].append({
                'layer': lora_key,
                'similarity': similarity,
                'layer_type': layer_type
            })

            # æ£€æŸ¥ç»´åº¦å…¼å®¹æ€§
            if not self._check_dimension_compatibility(source_base, target_base, lora_weight):
                print_layer_details(lora_key, base_key, similarity, 
                                  source_shape, target_shape, lora_shape, 
                                  "ğŸ”§ FrobeniusæŠ•å½±", "ç»´åº¦ä¸å…¼å®¹ï¼Œä½¿ç”¨æŠ•å½±æ–¹æ³•")
                
                try:
                    projected_weight = self._frobenius_projection(lora_weight, source_base, target_base)
                    transferred_lora[lora_key] = projected_weight
                    transfer_stats['transferred_layers'] += 1
                    transfer_stats['transferred_list'].append(lora_key)
                    transfer_stats['layer_types'][layer_type]['transferred'] += 1
                    print(f"âœ… æŠ•å½±æˆåŠŸ!")
                except Exception as e:
                    reason = f"FrobeniusæŠ•å½±å¤±è´¥: {e}"
                    print_layer_details(lora_key, base_key, similarity, 
                                      source_shape, target_shape, lora_shape, 
                                      "âŒ è·³è¿‡", reason)
                    transfer_stats['skipped_layers'].append(lora_key)
                    transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # ç›¸ä¼¼æ€§è¿‡æ»¤
            if similarity < self.similarity_threshold:
                reason = f"ç›¸ä¼¼æ€§è¿‡ä½ ({similarity:.4f} < {self.similarity_threshold})"
                print_layer_details(lora_key, base_key, similarity, 
                                  source_shape, target_shape, lora_shape, 
                                  "âŒ è·³è¿‡", reason)
                transfer_stats['skipped_layers'].append(lora_key)
                transfer_stats['skipped_reasons'][lora_key] = reason
                continue

            # æ‰§è¡Œè¿ç§»
            print_layer_details(lora_key, base_key, similarity, 
                              source_shape, target_shape, lora_shape, 
                              "ğŸ”„ æ­£åœ¨è¿ç§»", "ç›¸ä¼¼æ€§é€šè¿‡ï¼Œæ‰§è¡Œå­ç©ºé—´æŠ•å½±")
            
            try:
                transferred_weight = self._transfer_single_layer(lora_weight, source_base, target_base)
                transferred_lora[lora_key] = transferred_weight
                transfer_stats['transferred_layers'] += 1
                transfer_stats['transferred_list'].append(lora_key)
                transfer_stats['layer_types'][layer_type]['transferred'] += 1
                print(f"âœ… è¿ç§»æˆåŠŸ!")
            except Exception as e:
                reason = f"è¿ç§»è¿‡ç¨‹å‡ºé”™: {e}"
                print_layer_details(lora_key, base_key, similarity, 
                                  source_shape, target_shape, lora_shape, 
                                  "âŒ è·³è¿‡", reason)
                transfer_stats['skipped_layers'].append(lora_key)
                transfer_stats['skipped_reasons'][lora_key] = reason
        
        print_transfer_summary(transfer_stats)
        return transferred_lora
    
    def _precompute_similarities_verbose(self, source_lora: dict, 
                                       source_base_weights: dict,
                                       target_base_weights: dict) -> dict:
        """é¢„è®¡ç®—ç›¸ä¼¼åº¦ï¼Œæä¾›è¯¦ç»†è¾“å‡º"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        similarities = {}
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦è®¡ç®—çš„å±‚
        valid_keys = []
        for lora_key in source_lora.keys():
            if not lora_key.endswith('.weight'):
                continue
            base_key = self._map_lora_to_base_key(lora_key)
            if base_key in source_base_weights and base_key in target_base_weights:
                valid_keys.append(base_key)
        
        # å»é‡
        unique_keys = list(set(valid_keys))
        print(f"ğŸ“Š éœ€è¦è®¡ç®—ç›¸ä¼¼åº¦çš„å”¯ä¸€å±‚æ•°: {len(unique_keys)}")
        
        # æ‰¹é‡è®¡ç®—SVDå’Œç›¸ä¼¼åº¦
        for i, base_key in enumerate(unique_keys):
            print(f"ğŸ”„ è®¡ç®—è¿›åº¦: {i+1}/{len(unique_keys)} - {base_key}")
            
            try:
                source_base = source_base_weights[base_key].to(device)
                target_base = target_base_weights[base_key].to(device)
                
                print(f"   æºæƒé‡å½¢çŠ¶: {tuple(source_base.shape)}")
                print(f"   ç›®æ ‡æƒé‡å½¢çŠ¶: {tuple(target_base.shape)}")
                
                U_s, _, _ = self.compute_svd_subspace(source_base)
                U_t, _, _ = self.compute_svd_subspace(target_base)
                similarity = self.compute_subspace_similarity(U_s, U_t)
                similarities[base_key] = similarity
                
                print(f"   âœ… ç›¸ä¼¼åº¦: {similarity:.4f}")
                
            except Exception as e:
                print(f"   âŒ è®¡ç®—å¤±è´¥: {e}")
                similarities[base_key] = 0.0
        
        return similarities


def main():
    parser = argparse.ArgumentParser(description="è‡ªå®šä¹‰LoRAè¿ç§»è„šæœ¬")
    parser.add_argument("--source_lora", type=str, 
                       default="/root/autodl-tmp/loraed/Meta-Llama-3.1-8B-Instruct/251728_010944",
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, 
                       default="/root/autodl-tmp/models/Qwen2.5-7B-Instruct",
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str, 
                       default="/root/autodl-tmp/shifted/arc-challenge/Llama-3.1-8B_to_Qwen2.5-7B",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--rank", type=int, default=320,
                       help="SVDæˆªæ–­ç§©")
    parser.add_argument("--similarity_threshold", type=float, default=0.3,
                       help="å­ç©ºé—´ç›¸ä¼¼æ€§é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæ—¶é—´æˆ³å’Œè¾“å‡ºè·¯å¾„
    timestamp = generate_timestamp()
    output_path = os.path.join(args.output_base, timestamp)
    
    # æ¨æ–­æºæ¨¡å‹è·¯å¾„
    source_model_path = infer_source_model_path(args.source_lora)
    
    print(f"\n{'ğŸš€'*20} LoRAè¿ç§»å¼€å§‹ {'ğŸš€'*20}")
    print(f"{'='*80}")
    print(f"ğŸ“‚ æºLoRAè·¯å¾„: {args.source_lora}")
    print(f"ğŸ“‚ æºæ¨¡å‹è·¯å¾„: {source_model_path}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹è·¯å¾„: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"âš™ï¸  SVDæˆªæ–­ç§©: {args.rank}")
    print(f"âš™ï¸  ç›¸ä¼¼æ€§é˜ˆå€¼: {args.similarity_threshold}")
    print(f"ğŸ• æ—¶é—´æˆ³: {timestamp}")
    print(f"{'='*80}")
    
    try:
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.source_lora):
            print(f"âŒ é”™è¯¯: æºLoRAè·¯å¾„ä¸å­˜åœ¨: {args.source_lora}")
            return False
        
        if not os.path.exists(source_model_path):
            print(f"âŒ é”™è¯¯: æºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {source_model_path}")
            return False
            
        if not os.path.exists(args.target_model):
            print(f"âŒ é”™è¯¯: ç›®æ ‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.target_model}")
            return False
        
        # åˆå§‹åŒ–ç»„ä»¶
        print(f"\nğŸ”§ åˆå§‹åŒ–LoRA-Xæ ¸å¿ƒç»„ä»¶...")
        lora_x = VerboseLoRAXCore(rank=args.rank, similarity_threshold=args.similarity_threshold)
        loader = ModelWeightLoader()
        
        # åŠ è½½æºLoRAæƒé‡
        print(f"\nğŸ“¥ åŠ è½½æºLoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(source_lora_weights)} ä¸ªLoRAæƒé‡")
        
        # åŠ è½½åŸºç¡€æ¨¡å‹æƒé‡
        print(f"\nğŸ“¥ åŠ è½½æºæ¨¡å‹æƒé‡...")
        source_base_weights = loader.load_base_model_weights(source_model_path)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(source_base_weights)} ä¸ªæºæ¨¡å‹æƒé‡")
        
        print(f"\nğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(target_base_weights)} ä¸ªç›®æ ‡æ¨¡å‹æƒé‡")
        
        # æ‰§è¡Œè¿ç§»
        print(f"\nğŸ”„ æ‰§è¡ŒLoRA-Xè¿ç§»...")
        transferred_lora = lora_x.transfer_lora_weights(
            source_lora=source_lora_weights,
            target_base_weights=target_base_weights,
            source_base_weights=source_base_weights
        )
        
        if not transferred_lora:
            print(f"âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚")
            return False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ ä¿å­˜è¿ç§»ç»“æœåˆ°: {output_path}")
        save_transferred_lora(transferred_lora, lora_config, output_path)
        
        # ä¿å­˜è¿ç§»æ—¥å¿—
        log_file = os.path.join(output_path, "transfer_log.txt")
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"LoRAè¿ç§»æ—¥å¿—\n")
            f.write(f"æ—¶é—´æˆ³: {timestamp}\n")
            f.write(f"æºLoRA: {args.source_lora}\n")
            f.write(f"æºæ¨¡å‹: {source_model_path}\n")
            f.write(f"ç›®æ ‡æ¨¡å‹: {args.target_model}\n")
            f.write(f"è¾“å‡ºè·¯å¾„: {output_path}\n")
            f.write(f"SVDæˆªæ–­ç§©: {args.rank}\n")
            f.write(f"ç›¸ä¼¼æ€§é˜ˆå€¼: {args.similarity_threshold}\n")
        
        print(f"\nğŸ‰ LoRAè¿ç§»å®Œæˆï¼")
        print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ è¿ç§»è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
