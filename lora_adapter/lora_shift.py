#!/usr/bin/env python3
"""
æ–¹æ³•2 LoRA-Xçš„ç²—ç³™æ¨¡ä»¿
ç”¨ä¾‹ ä»Meta-Llama-3.1-8B-Instructè¿ç§»LoRAåˆ°Qwen2.5-7B-Instruct
"""

import argparse
import logging
import sys
import os
from pathlib import Path
from datetime import datetime
import torch

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.lora_x_core import LoRAXCore
from src.model_utils import ModelWeightLoader, save_transferred_lora

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³æ ¼å¼: YYMMDD_HHMMSS"""
    now = datetime.now()
    return now.strftime("%y%m%d_%H%M%S")

def infer_source_model_path(lora_path: str) -> str:
    lora_path = Path(lora_path)
    if lora_path.name == "final_model":
        model_name = lora_path.parent.parent.name  
    else:
        model_name = lora_path.parent.name  
    return f"/root/autodl-tmp/models/{model_name}"

def main():
    parser = argparse.ArgumentParser(description="LoRA-X Frobeniusæœ€ä¼˜è¿‘ä¼¼è¿ç§»")
    parser.add_argument("--source_lora", type=str, required=True,
                       help="æºLoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model", type=str, required=True,
                       help="ç›®æ ‡åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_base", type=str,
                       default="/root/autodl-tmp/shifted/arc-challenge/Llama-to-Qwen",
                       help="è¾“å‡ºåŸºç¡€è·¯å¾„")
    parser.add_argument("--rank", type=int, default=320,
                       help="SVDæˆªæ–­ç§©")
    args = parser.parse_args()

    timestamp = generate_timestamp()
    output_path = os.path.join(args.output_base, timestamp)
    source_model_path = infer_source_model_path(args.source_lora)

    print(f"\nğŸš€ LoRAè¿ç§»å¼€å§‹ ğŸš€")
    print("="*80)
    print(f"ğŸ“‚ æºLoRAè·¯å¾„: {args.source_lora}")
    print(f"ğŸ“‚ æºæ¨¡å‹è·¯å¾„: {source_model_path}")
    print(f"ğŸ“‚ ç›®æ ‡æ¨¡å‹è·¯å¾„: {args.target_model}")
    print(f"ğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"âš™ï¸  SVDæˆªæ–­ç§©: {args.rank}")
    print(f"ğŸ• æ—¶é—´æˆ³: {timestamp}")
    print("="*80)

    try:
        if not os.path.exists(args.source_lora):
            raise FileNotFoundError(f"æºLoRAè·¯å¾„ä¸å­˜åœ¨: {args.source_lora}")
        if not os.path.exists(source_model_path):
            raise FileNotFoundError(f"æºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {source_model_path}")
        if not os.path.exists(args.target_model):
            raise FileNotFoundError(f"ç›®æ ‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.target_model}")

        lora_x = LoRAXCore(rank=args.rank)
        loader = ModelWeightLoader()

        print("\nğŸ“¥ åŠ è½½æºLoRAæƒé‡...")
        source_lora_weights, lora_config = loader.load_lora_weights(args.source_lora)

        print("\nğŸ“¥ åŠ è½½æºæ¨¡å‹æƒé‡...")
        source_base_weights = loader.load_base_model_weights(source_model_path)

        print("\nğŸ“¥ åŠ è½½ç›®æ ‡æ¨¡å‹æƒé‡...")
        target_base_weights = loader.load_base_model_weights(args.target_model)

        print("\nğŸ”„ æ‰§è¡ŒLoRA-X Frobeniusæœ€ä¼˜è¿‘ä¼¼è¿ç§»...")
        with torch.no_grad():
            transferred_lora = lora_x.transfer_lora_weights(
                source_lora=source_lora_weights,
                target_base_weights=target_base_weights,
                source_base_weights=source_base_weights
            )

        if not transferred_lora:
            print("âŒ è¿ç§»å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸè¿ç§»ä»»ä½•å±‚")
            return False

        os.makedirs(output_path, exist_ok=True)
        print(f"\nğŸ’¾ ä¿å­˜è¿ç§»ç»“æœåˆ°: {output_path}")
        save_transferred_lora(transferred_lora, lora_config, output_path)

        print("\nğŸ‰ LoRAè¿ç§»å®Œæˆï¼")
        return True

    except Exception as e:
        logger.error(f"è¿ç§»è¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
