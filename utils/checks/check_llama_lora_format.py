#!/usr/bin/env python3
"""
æ£€æŸ¥æ–°è®­ç»ƒçš„Llama LoRAæƒé‡æ ¼å¼
"""
import torch
from safetensors import safe_open

def check_llama_lora_format():
    """æ£€æŸ¥Llama LoRAæƒé‡æ ¼å¼"""
    llama_lora_path = "/root/PAW/train_lora/runs/Llama-3.2-3B-Instruct/arc-challenge_lora_20250724_140508/final_model/adapter_model.safetensors"
    
    print("ğŸ” æ£€æŸ¥æ–°è®­ç»ƒçš„Llama LoRAæƒé‡æ ¼å¼:")
    print(f"æ–‡ä»¶è·¯å¾„: {llama_lora_path}")
    
    with safe_open(llama_lora_path, framework="pt", device="cpu") as f:
        print(f"\næ€»æƒé‡æ•°: {len(f.keys())}")
        
        # æ˜¾ç¤ºå‰15ä¸ªæƒé‡
        for i, key in enumerate(f.keys()):
            if i >= 15:
                print("...")
                break
            weight = f.get_tensor(key)
            print(f"  {key}: {weight.shape}")
        
        # æ£€æŸ¥ç‰¹å®šå±‚çš„Aå’ŒBæƒé‡
        print(f"\nğŸ¯ å…³é”®å±‚æƒé‡æ£€æŸ¥:")
        for key in f.keys():
            if "layers.0.self_attn.k_proj" in key:
                weight = f.get_tensor(key)
                print(f"  {key}: {weight.shape}")

if __name__ == "__main__":
    check_llama_lora_format()
