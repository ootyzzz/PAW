"""
improved_data_cleaner.py
æ”¹è¿›ç‰ˆæ•°æ®æ¸…æ´—å·¥å…·ï¼Œè¿›ä¸€æ­¥ç»Ÿä¸€æ ¼å¼
"""

import json
import random
import os
from pathlib import Path
from typing import List, Dict, Any

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    data = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
    return data

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def clean_and_standardize_sample(sample: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¿›ä¸€æ­¥æ¸…ç†å’Œæ ‡å‡†åŒ–å•ä¸ªæ ·æœ¬
    ç»Ÿä¸€æ ¼å¼ï¼š
    - id: ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²æ ¼å¼ dataset_originalid
    - dataset: æ•°æ®é›†åç§°
    - task_type: ä»»åŠ¡ç±»å‹
    - input: æ¸…ç†åçš„è¾“å…¥æ–‡æœ¬
    - options: é€‰é¡¹åˆ—è¡¨
    - target: ç­”æ¡ˆæ–‡æœ¬
    - target_idx: ç­”æ¡ˆç´¢å¼•
    """
    
    dataset = sample.get('dataset_source', sample.get('dataset', ''))
    original_data = sample.get('original_data', sample)
    
    # å¦‚æœå·²ç»æ˜¯æ¸…æ´—åçš„æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
    if 'input' in sample and 'options' in sample:
        # ç»Ÿä¸€IDæ ¼å¼
        original_id = sample.get('id', hash(str(sample)) % 100000)
        unified_id = f"{dataset}_{original_id}"
        
        return {
            "id": unified_id,
            "dataset": dataset,
            "task_type": sample.get('task_type', 'unknown'),
            "input": sample.get('input', '').strip(),
            "options": sample.get('options', []),
            "target": sample.get('target', ''),
            "target_idx": sample.get('target_idx', -1)
        }
    
    # å¦åˆ™ä»original_dataè§£æ
    # ç»Ÿä¸€IDæ ¼å¼
    if dataset == 'hellaswag':
        original_id = original_data.get('ind', sample.get('id', ''))
        unified_id = f"hellaswag_{original_id}"
    elif dataset == 'winogrande':
        original_id = hash(original_data.get('sentence', '')) % 100000
        unified_id = f"winogrande_{original_id}"
    elif dataset == 'piqa':
        original_id = hash(original_data.get('goal', '')) % 100000
        unified_id = f"piqa_{original_id}"
    elif dataset == 'boolq':
        original_id = hash(original_data.get('question', '')) % 100000
        unified_id = f"boolq_{original_id}"
    elif dataset in ['arc-challenge', 'arc-easy']:
        original_id = original_data.get('id', sample.get('id', ''))
        unified_id = f"{dataset}_{original_id}".replace('/', '_')
    elif dataset == 'openbookqa':
        original_id = original_data.get('id', sample.get('id', ''))
        unified_id = f"openbookqa_{original_id}"
    else:
        unified_id = f"{dataset}_{hash(str(sample)) % 100000}"
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    if dataset in ['arc-challenge', 'arc-easy']:
        input_text = original_data.get('question', '')
        options = original_data.get('choices', {}).get('text', [])
        answer_key = original_data.get('answerKey', '')
        target_idx = ord(answer_key) - ord('A') if answer_key else -1
        target = options[target_idx] if 0 <= target_idx < len(options) else ""
        task_type = "multiple_choice"
        
    elif dataset == 'boolq':
        input_text = original_data.get('question', '')
        options = ["False", "True"]
        answer = original_data.get('answer', False)
        target_idx = 1 if answer else 0
        target = "True" if answer else "False"
        task_type = "yes_no_question"
        
    elif dataset == 'hellaswag':
        input_text = original_data.get('ctx', '')
        options = original_data.get('endings', [])
        label = original_data.get('label', '')
        target_idx = int(label) if label.isdigit() else -1
        target = options[target_idx] if 0 <= target_idx < len(options) else ""
        task_type = "sentence_completion"
        
    elif dataset == 'openbookqa':
        input_text = original_data.get('question_stem', '')
        options = original_data.get('choices', {}).get('text', [])
        answer_key = original_data.get('answerKey', '')
        target_idx = ord(answer_key) - ord('A') if answer_key else -1
        target = options[target_idx] if 0 <= target_idx < len(options) else ""
        task_type = "multiple_choice"
        
    elif dataset == 'piqa':
        input_text = original_data.get('goal', '')
        options = [original_data.get('sol1', ''), original_data.get('sol2', '')]
        target_idx = -1  # PIQAé€šå¸¸æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾
        target = ""  # æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ
        task_type = "physical_reasoning"
        
    elif dataset == 'winogrande':
        input_text = original_data.get('sentence', '')
        options = [original_data.get('option1', ''), original_data.get('option2', '')]
        answer = original_data.get('answer', '')
        target_idx = 0 if answer == "1" else 1 if answer == "2" else -1
        target = options[target_idx] if 0 <= target_idx < len(options) else ""
        task_type = "pronoun_resolution"
        
    else:
        # é»˜è®¤å¤„ç†
        input_text = sample.get('question', sample.get('input', ''))
        options = sample.get('choices', sample.get('options', []))
        target_idx = sample.get('answer_index', sample.get('target_idx', -1))
        target = sample.get('answer', sample.get('target', ''))
        task_type = "unknown"
    
    # æ¸…ç†è¾“å…¥æ–‡æœ¬
    input_text = input_text.strip()
    if len(input_text) > 1000:  # é™åˆ¶è¾“å…¥é•¿åº¦
        input_text = input_text[:1000] + "..."
    
    # æ¸…ç†é€‰é¡¹
    options = [opt.strip() for opt in options if opt and opt.strip()]
    
    return {
        "id": unified_id,
        "dataset": dataset,
        "task_type": task_type,
        "input": input_text,
        "options": options,
        "target": target,
        "target_idx": target_idx
    }

def improve_cleaned_data(input_file: str, output_file: str):
    """è¿›ä¸€æ­¥æ”¹è¿›å·²æ¸…æ´—çš„æ•°æ®"""
    
    print(f"ğŸ”„ åŠ è½½æ•°æ®: {input_file}")
    data = load_jsonl(input_file)
    
    print(f"ğŸ“Š å¤„ç† {len(data)} ä¸ªæ ·æœ¬...")
    
    improved_data = []
    stats = {}
    
    for i, sample in enumerate(data):
        try:
            improved_sample = clean_and_standardize_sample(sample)
            improved_data.append(improved_sample)
            
            # ç»Ÿè®¡
            dataset = improved_sample['dataset']
            task_type = improved_sample['task_type']
            
            if dataset not in stats:
                stats[dataset] = {'total': 0, 'task_types': {}}
            stats[dataset]['total'] += 1
            stats[dataset]['task_types'][task_type] = stats[dataset]['task_types'].get(task_type, 0) + 1
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†ç¬¬{i+1}ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
    save_jsonl(improved_data, output_file)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ”¹è¿›åæ•°æ®ç»Ÿè®¡:")
    print("=" * 60)
    for dataset, info in stats.items():
        print(f"{dataset}: {info['total']} æ ·æœ¬")
        for task_type, count in info['task_types'].items():
            print(f"  - {task_type}: {count} æ ·æœ¬")
    print("-" * 60)
    print(f"æ€»è®¡: {len(improved_data)} æ ·æœ¬")
    
    return improved_data, stats

def create_sample_file(data: List[Dict[str, Any]], output_file: str, sample_size: int = 20):
    """åˆ›å»ºæ ·æœ¬æ–‡ä»¶"""
    sample_data = data[:sample_size]
    save_jsonl(sample_data, output_file)
    print(f"ğŸ“‹ æ ·æœ¬æ–‡ä»¶: {output_file} ({len(sample_data)} æ ·æœ¬)")

def main():
    """ä¸»å‡½æ•°"""
    
    # è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶
    input_file = "raw_datasets/commonsense/cs_all_cleaned.jsonl"
    output_file = "raw_datasets/commonsense/cs_all_improved.jsonl"
    sample_file = "raw_datasets/commonsense/cs_all_improved_sample.jsonl"
    
    print("ğŸš€ å¼€å§‹æ•°æ®æ”¹è¿›...")
    
    # è¿›ä¸€æ­¥æ”¹è¿›æ¸…æ´—
    improved_data, stats = improve_cleaned_data(input_file, output_file)
    
    # åˆ›å»ºæ ·æœ¬æ–‡ä»¶
    create_sample_file(improved_data, sample_file)
    
    print("\nâœ… æ•°æ®æ”¹è¿›å®Œæˆ!")

if __name__ == "__main__":
    main()
