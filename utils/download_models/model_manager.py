#!/usr/bin/env python3
"""
model_manager.py - å¢å¼ºç‰ˆæ¨¡å‹ç®¡ç†å™¨
æ”¯æŒQwen2.5æ¨¡å‹åŠ è½½ã€LoRAé…ç½®ç®¡ç†å’Œæ¨¡å‹çŠ¶æ€æ£€æŸ¥
"""

import os
import torch
import argparse
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import json
import logging

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
    from peft import LoraConfig, get_peft_model, TaskType
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = logging.getLogger(__name__)

class ModelManager:
    """å¢å¼ºçš„æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "./models"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹é…ç½®ç¼“å­˜
        self.model_configs = {}
        
        # æ£€æŸ¥ä¾èµ–
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("Transformers and/or PEFT not available. Some features will be limited.")
    
    def download_model(self, model_name: str, save_local: bool = True) -> bool:
        """
        ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
        
        Args:
            model_name: æ¨¡å‹åç§°
            save_local: æ˜¯å¦ä¿å­˜åˆ°æœ¬åœ°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not TRANSFORMERS_AVAILABLE:
            logger.error("Transformers not available. Please install: pip install transformers torch")
            return False
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä¿å­˜è·¯å¾„: {self.cache_dir}")
        
        try:
            # ä¸‹è½½é…ç½®
            print("ğŸ“‹ ä¸‹è½½é…ç½®æ–‡ä»¶...")
            config = AutoConfig.from_pretrained(
                model_name,
                cache_dir=str(self.cache_dir),
                trust_remote_code=True
            )
            
            # ä¸‹è½½tokenizer
            print("ğŸ“¦ ä¸‹è½½tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=str(self.cache_dir),
                trust_remote_code=True
            )
            
            # ä¸‹è½½æ¨¡å‹
            print("ğŸ¤– ä¸‹è½½æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=str(self.cache_dir),
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map=None  # å…ˆä¸æ˜ å°„åˆ°è®¾å¤‡
            )
            
            # å¦‚æœéœ€è¦ä¿å­˜åˆ°æœ¬åœ°ç›®å½•
            if save_local:
                local_path = self.cache_dir / model_name.replace("/", "_")
                local_path.mkdir(exist_ok=True)
                
                print(f"ğŸ’¾ ä¿å­˜åˆ°æœ¬åœ°: {local_path}")
                model.save_pretrained(local_path)
                tokenizer.save_pretrained(local_path)
                config.save_pretrained(local_path)
            
            print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            self._display_model_info(model, model_name)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def load_model_for_lora(
        self, 
        model_path: str,
        lora_config: Optional[Dict[str, Any]] = None,
        device_map: str = "auto"
    ) -> Tuple[Any, Any, Any]:
        """
        åŠ è½½æ¨¡å‹å¹¶åº”ç”¨LoRAé…ç½®
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            lora_config: LoRAé…ç½®
            device_map: è®¾å¤‡æ˜ å°„ç­–ç•¥
            
        Returns:
            tuple: (model, tokenizer, lora_config)
        """
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("Transformers and PEFT required for LoRA functionality")
        
        print(f"ï¿½ åŠ è½½æ¨¡å‹: {model_path}")
        
        try:
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map=device_map if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            
            # åº”ç”¨LoRAé…ç½®
            if lora_config is None:
                lora_config = self._get_default_lora_config()
            
            peft_config = LoraConfig(**lora_config)
            model = get_peft_model(base_model, peft_config)
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
            print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {model.get_nb_trainable_parameters():,}")
            print(f"ğŸ“Š æ€»å‚æ•°: {model.num_parameters():,}")
            
            return model, tokenizer, lora_config
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _get_default_lora_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤LoRAé…ç½®"""
        return {
            'r': 16,
            'lora_alpha': 32,
            'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
            'lora_dropout': 0.1,
            'bias': 'none',
            'task_type': TaskType.CAUSAL_LM,
        }
    
    def check_model_compatibility(self, model_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ¨¡å‹å…¼å®¹æ€§
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            
        Returns:
            dict: å…¼å®¹æ€§æ£€æŸ¥ç»“æœ
        """
        result = {
            "path_exists": False,
            "config_valid": False,
            "tokenizer_valid": False,
            "model_valid": False,
            "cuda_compatible": False,
            "lora_compatible": False,
            "details": {}
        }
        
        try:
            # æ£€æŸ¥è·¯å¾„
            path = Path(model_path)
            result["path_exists"] = path.exists()
            
            if not result["path_exists"]:
                result["details"]["error"] = f"Model path does not exist: {model_path}"
                return result
            
            if not TRANSFORMERS_AVAILABLE:
                result["details"]["error"] = "Transformers not available"
                return result
            
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            try:
                config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
                result["config_valid"] = True
                result["details"]["model_type"] = config.model_type
                result["details"]["hidden_size"] = getattr(config, 'hidden_size', 'unknown')
            except Exception as e:
                result["details"]["config_error"] = str(e)
            
            # æ£€æŸ¥tokenizer
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                result["tokenizer_valid"] = True
                result["details"]["vocab_size"] = tokenizer.vocab_size
                result["details"]["pad_token"] = tokenizer.pad_token is not None
            except Exception as e:
                result["details"]["tokenizer_error"] = str(e)
            
            # æ£€æŸ¥æ¨¡å‹åŠ è½½
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    device_map=None,  # ä¸æ˜ å°„åˆ°è®¾å¤‡
                    torch_dtype=torch.float32  # ä½¿ç”¨float32é¿å…è®¾å¤‡é—®é¢˜
                )
                result["model_valid"] = True
                result["details"]["num_parameters"] = model.num_parameters()
                
                # æ£€æŸ¥LoRAå…¼å®¹æ€§
                try:
                    lora_config = LoraConfig(**self._get_default_lora_config())
                    get_peft_model(model, lora_config)
                    result["lora_compatible"] = True
                except Exception as e:
                    result["details"]["lora_error"] = str(e)
                
            except Exception as e:
                result["details"]["model_error"] = str(e)
            
            # æ£€æŸ¥CUDAå…¼å®¹æ€§
            result["cuda_compatible"] = torch.cuda.is_available()
            result["details"]["cuda_version"] = torch.version.cuda if torch.cuda.is_available() else None
            result["details"]["device_count"] = torch.cuda.device_count() if torch.cuda.is_available() else 0
            
        except Exception as e:
            result["details"]["general_error"] = str(e)
        
        return result
    
    def _display_model_info(self, model, model_name: str):
        """æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯"""
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - åç§°: {model_name}")
        print(f"  - å‚æ•°é‡: {model.num_parameters():,}")
        print(f"  - ç¼“å­˜è·¯å¾„: {self.cache_dir}")
        print(f"  - è®¾å¤‡: {next(model.parameters()).device}")
        print(f"  - æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
    
    def list_models(self) -> None:
        """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
        print(f"ğŸ“š å·²ä¸‹è½½çš„æ¨¡å‹ ({self.cache_dir}):")
        
        if not self.cache_dir.exists():
            print("ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return
        
        # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
        model_dirs = []
        for item in self.cache_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                has_model_files = (
                    any(item.glob("*.bin")) or 
                    any(item.glob("*.safetensors")) or
                    (item / "config.json").exists()
                )
                if has_model_files:
                    model_dirs.append(item)
        
        if not model_dirs:
            print("  æš‚æ— å·²ä¸‹è½½çš„æ¨¡å‹")
            return
        
        for i, model_dir in enumerate(model_dirs, 1):
            print(f"  {i}. {model_dir.name}")
            
            # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶å¤§å°
            total_size = sum(f.stat().st_size for f in model_dir.rglob("*") if f.is_file())
            size_mb = total_size / (1024 * 1024)
            print(f"     å¤§å°: {size_mb:.1f} MB")
            
            # æ˜¾ç¤ºå…¼å®¹æ€§çŠ¶æ€
            compat = self.check_model_compatibility(str(model_dir))
            status = "âœ…" if compat["lora_compatible"] else "âŒ"
            print(f"     LoRAå…¼å®¹: {status}")
    
    def save_model_config(self, model_name: str, config: Dict[str, Any]):
        """ä¿å­˜æ¨¡å‹é…ç½®"""
        config_file = self.cache_dir / f"{model_name.replace('/', '_')}_config.json"
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
    
    def load_model_config(self, model_name: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        config_file = self.cache_dir / f"{model_name.replace('/', '_')}_config.json"
        if config_file.exists():
            with open(config_file, 'r') as f:
                return json.load(f)
        return None

def main():
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæ¨¡å‹ç®¡ç†å·¥å…·")
    parser.add_argument("--action", 
                       choices=["download", "list", "check", "test_lora"], 
                       default="list",
                       help="æ“ä½œç±»å‹")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--cache_dir", type=str, default="./models",
                       help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    parser.add_argument("--model_path", type=str,
                       help="æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºæ£€æŸ¥å’Œæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸ—ï¸  Enhanced Model Manager")
    print("=" * 50)
    
    manager = ModelManager(args.cache_dir)
    
    if args.action == "download":
        manager.download_model(args.model)
    
    elif args.action == "list":
        manager.list_models()
    
    elif args.action == "check":
        model_path = args.model_path or args.model
        result = manager.check_model_compatibility(model_path)
        print(f"ğŸ” å…¼å®¹æ€§æ£€æŸ¥ç»“æœ:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    
    elif args.action == "test_lora":
        model_path = args.model_path or args.model
        try:
            model, tokenizer, lora_config = manager.load_model_for_lora(model_path)
            print("âœ… LoRAæµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“Š LoRAé…ç½®: {lora_config}")
        except Exception as e:
            print(f"âŒ LoRAæµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
