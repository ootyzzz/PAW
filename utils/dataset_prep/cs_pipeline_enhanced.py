"""
æ‰©å±•çš„Commonsenseæ•°æ®é›†å¤„ç†Pipeline
å¤„ç†æ‰€æœ‰splits: train, validation, test

è¿™æ˜¯ä¸€ä¸ªæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼Œç”¨äºå¤„ç†7ä¸ªcommonsenseæ•°æ®é›†çš„æ‰€æœ‰splitsã€‚
æ”¯æŒindividualæ•°æ®é›†å¤„ç†å’Œmixedæ•°æ®é›†ç”Ÿæˆã€‚

å‚æ•°ä½¿ç”¨æŒ‡å—:
===============

åŸºç¡€ä½¿ç”¨:
# é»˜è®¤å¤„ç†æ‰€æœ‰æ•°æ®é›†çš„æ‰€æœ‰splits (train/validation/test)ï¼Œç”Ÿæˆindividualå’Œmixedæ•°æ®
python utils/dataset_prep/cs_pipeline_all_splits.py

# åªå¤„ç†ç‰¹å®šæ•°æ®é›†
python utils/dataset_prep/cs_pipeline_all_splits.py --datasets arc-challenge arc-easy boolq

# åªå¤„ç†ç‰¹å®šsplits  
python utils/dataset_prep/cs_pipeline_all_splits.py --splits train validation

# åªç”Ÿæˆmixedæ•°æ®ï¼Œä¸ç”Ÿæˆindividualæ•°æ®é›†
python utils/dataset_prep/cs_pipeline_all_splits.py --mixed-only

# åªç”Ÿæˆindividualæ•°æ®é›†ï¼Œä¸ç”Ÿæˆmixedæ•°æ®
python utils/dataset_prep/cs_pipeline_all_splits.py --individual-only

# ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•
python utils/dataset_prep/cs_pipeline_all_splits.py --datasets_dir ./my_raw_data --output_dir ./my_output

# éªŒè¯æ¨¡å¼ - åªæ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œä¸ç”Ÿæˆæ–‡ä»¶
python utils/dataset_prep/cs_pipeline_all_splits.py --validate_only

# è®¾ç½®éšæœºç§å­
python utils/dataset_prep/cs_pipeline_all_splits.py --seed 123

å¯ç”¨æ•°æ®é›†:
- arc-challenge: AI2 Reasoning Challenge (æŒ‘æˆ˜ç‰ˆ)
- arc-easy: AI2 Reasoning Challenge (ç®€å•ç‰ˆ) 
- boolq: Boolean Questions
- hellaswag: HellaSwagå¸¸è¯†æ¨ç†
- openbookqa: Open Book Question Answering
- piqa: Physical Interaction QA
- winogrande: Winograndeä»£è¯è§£æ

è¾“å‡ºæ–‡ä»¶ç»“æ„:
data_to_lora/cs/
â”œâ”€â”€ mixed/
â”‚   â”œâ”€â”€ cs_mixed_train.jsonl                    # åŸå§‹æ··åˆè®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ cs_mixed_formatted_train.jsonl         # æ ¼å¼åŒ–æ··åˆè®­ç»ƒæ•°æ® 
â”‚   â”œâ”€â”€ cs_mixed_validation.jsonl              # åŸå§‹æ··åˆéªŒè¯æ•°æ®
â”‚   â”œâ”€â”€ cs_mixed_formatted_validation.jsonl    # æ ¼å¼åŒ–æ··åˆéªŒè¯æ•°æ®
â”‚   â”œâ”€â”€ cs_mixed_test.jsonl                    # åŸå§‹æ··åˆæµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ cs_mixed_formatted_test.jsonl          # æ ¼å¼åŒ–æ··åˆæµ‹è¯•æ•°æ®
â”œâ”€â”€ arc-challenge/
â”‚   â”œâ”€â”€ arc-challenge_train_formatted.jsonl
â”‚   â”œâ”€â”€ arc-challenge_validation_formatted.jsonl
â”‚   â””â”€â”€ arc-challenge_test_formatted.jsonl
â”œâ”€â”€ arc-easy/
â”‚   â””â”€â”€ ...
â””â”€â”€ [å…¶ä»–æ•°æ®é›†]/
    â””â”€â”€ ...

ä½¿ç”¨åœºæ™¯:
- é¦–æ¬¡è®¾ç½®: è¿è¡Œé»˜è®¤å‘½ä»¤å¤„ç†æ‰€æœ‰æ•°æ®
- å¢é‡æ›´æ–°: æŒ‡å®šç‰¹å®šæ•°æ®é›†é‡æ–°å¤„ç†
- è°ƒè¯•éªŒè¯: ä½¿ç”¨ --validate_only æ£€æŸ¥æ•°æ®è´¨é‡
- è‡ªå®šä¹‰è®­ç»ƒ: é€‰æ‹©ç‰¹å®šæ•°æ®é›†å’Œsplitsç»„åˆ
"""

import json
import random
import os
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import argparse

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒæ ¼å¼ï¼ˆJSONå’ŒPythonå­—å…¸æ ¼å¼ï¼‰"""
    data = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        # å…ˆå°è¯•æ ‡å‡†JSONè§£æ
                        data.append(json.loads(line))
                    except json.JSONDecodeError:
                        try:
                            # å¦‚æœå¤±è´¥ï¼Œå°è¯•evalè§£æï¼ˆå¤„ç†å•å¼•å·æ ¼å¼ï¼Œå¦‚ARCæ•°æ®é›†ï¼‰
                            data.append(eval(line))
                        except Exception as e:
                            print(f"âš ï¸  è­¦å‘Š: æ— æ³•è§£æç¬¬{line_num}è¡Œ: {str(e)[:100]}...")
                            continue
    return data

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def standardize_format(data: List[Dict[str, Any]], dataset_name: str) -> List[Dict[str, Any]]:
    """æ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†çš„æ ¼å¼"""
    standardized = []
    
    for i, item in enumerate(data):
        standard_item = {
            "dataset_source": dataset_name,
            "original_data": item
        }
        
        if dataset_name in ['arc-challenge', 'arc-easy']:
            # ARCæ ¼å¼: question, choices, answerKey
            choices_dict = item.get("choices", {})
            choices_list = []
            if isinstance(choices_dict, dict):
                labels = choices_dict.get("label", [])
                texts = choices_dict.get("text", [])
                choices_list = [f"{label}: {text}" for label, text in zip(labels, texts)]
            
            standard_item.update({
                "question": item.get("question", ""),
                "choices": choices_list,
                "answer": item.get("answerKey", ""),
                "answer_index": ord(item.get("answerKey", "A")) - ord('A') if item.get("answerKey") else -1
            })
            
        elif dataset_name == 'boolq':
            # BoolQæ ¼å¼: question, passage, answer
            standard_item.update({
                "question": f"Question: {item.get('question', '')}\nPassage: {item.get('passage', '')}",
                "choices": ["False", "True"],
                "answer": str(item.get("answer", False)),
                "answer_index": 1 if item.get("answer") else 0
            })
            
        elif dataset_name == 'hellaswag':
            # HellaSwagæ ¼å¼: ctx, endings, label
            ctx = item.get("ctx", "")
            endings = item.get("endings", [])
            label = item.get("label")
            
            # å¤„ç†labelå¯èƒ½æ˜¯å­—ç¬¦ä¸²çš„æƒ…å†µ
            if isinstance(label, str) and label.isdigit():
                label_idx = int(label)
            elif isinstance(label, int):
                label_idx = label
            else:
                label_idx = -1
            
            standard_item.update({
                "question": ctx,
                "choices": endings,
                "answer": endings[label_idx] if endings and label_idx >= 0 and label_idx < len(endings) else "",
                "answer_index": label_idx
            })
            
        elif dataset_name == 'openbookqa':
            # OpenBookQAæ ¼å¼: question_stem, choices, answerKey
            choices_dict = item.get("choices", {})
            choices_list = []
            if isinstance(choices_dict, dict):
                labels = choices_dict.get("label", [])
                texts = choices_dict.get("text", [])
                choices_list = [f"{label}: {text}" for label, text in zip(labels, texts)]
            
            standard_item.update({
                "question": item.get("question_stem", ""),
                "choices": choices_list,
                "answer": item.get("answerKey", ""),
                "answer_index": ord(item.get("answerKey", "A")) - ord('A') if item.get("answerKey") else -1
            })
            
        elif dataset_name == 'piqa':
            # PIQAæ ¼å¼: goal, sol1, sol2 (æ²¡æœ‰label)
            standard_item.update({
                "question": item.get("goal", ""),
                "choices": [item.get("sol1", ""), item.get("sol2", "")],
                "answer_index": -1,  # PIQAæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾
                "answer": ""  # æ²¡æœ‰é¢„å®šä¹‰ç­”æ¡ˆ
            })
            
        elif dataset_name == 'winogrande':
            # Winograndeæ ¼å¼: sentence, option1, option2, answer
            standard_item.update({
                "question": item.get("sentence", ""),
                "choices": [item.get("option1", ""), item.get("option2", "")],
                "answer": item.get("answer", ""),
                "answer_index": 0 if item.get("answer") == "1" else 1 if item.get("answer") == "2" else -1
            })
        
        standardized.append(standard_item)
    
    return standardized

def format_to_final(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºæœ€ç»ˆçš„æ ‡å‡†æ ¼å¼"""
    formatted_data = []
    
    for i, sample in enumerate(data):
        dataset = sample.get('dataset_source', '')
        original_data = sample.get('original_data', sample)
        
        # ç”Ÿæˆç»Ÿä¸€ID
        if dataset == 'hellaswag':
            original_id = original_data.get('ind', f'h_{i}')
            unified_id = f"hellaswag_{original_id}"
            task_type = "sentence_completion"
        elif dataset == 'winogrande':
            original_id = hash(str(original_data.get('sentence', ''))) % 100000
            unified_id = f"winogrande_{original_id}"
            task_type = "pronoun_resolution"
        elif dataset == 'piqa':
            original_id = hash(str(original_data.get('goal', ''))) % 100000
            unified_id = f"piqa_{original_id}"
            task_type = "physical_reasoning"
        elif dataset == 'boolq':
            original_id = hash(str(original_data.get('question', ''))) % 100000
            unified_id = f"boolq_{original_id}"
            task_type = "yes_no_question"
        elif dataset in ['arc-challenge', 'arc-easy']:
            original_id = original_data.get('id', f'arc_{i}')
            unified_id = f"{dataset}_{original_id}"
            task_type = "multiple_choice"
        elif dataset == 'openbookqa':
            original_id = original_data.get('id', f'obqa_{i}')
            unified_id = f"openbookqa_{original_id}"
            task_type = "multiple_choice"
        else:
            unified_id = f"{dataset}_{i}"
            task_type = "unknown"
        
        # æ„å»ºæœ€ç»ˆæ ¼å¼
        choices = sample.get('choices', [])
        answer_idx = sample.get('answer_index', -1)
        answer_text = sample.get('answer', '')
        
        # å¦‚æœanswer_textä¸ºç©ºä½†æœ‰answer_indexï¼Œä»choicesä¸­è·å–
        if not answer_text and answer_idx >= 0 and answer_idx < len(choices):
            answer_text = choices[answer_idx]
        
        formatted_sample = {
            "id": unified_id,
            "dataset": dataset,
            "task_type": task_type,
            "input": sample.get('question', ''),
            "options": choices,
            "target": answer_text,
            "target_idx": answer_idx
        }
        
        formatted_data.append(formatted_sample)
    
    return formatted_data

def process_single_split(datasets_dir: str, split_name: str, output_dir: str, datasets_filter: List[str] = None, seed: int = 42):
    """å¤„ç†å•ä¸ªsplitï¼ˆtrain/validation/testï¼‰"""
    
    # ä½¿ç”¨è¿‡æ»¤å™¨æˆ–é»˜è®¤æ‰€æœ‰æ•°æ®é›†
    if datasets_filter:
        datasets = datasets_filter
    else:
        datasets = [
            'arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 
            'openbookqa', 'piqa', 'winogrande'
        ]
    
    all_data = []
    stats = {}
    
    print(f"ğŸ”„ å¼€å§‹å¤„ç† {split_name} split...")
    
    for dataset_name in datasets:
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        file_name = f"{dataset_name}_{split_name}.jsonl"
        if split_name == "validation":
            # æœ‰äº›æ•°æ®é›†å¯èƒ½ç”¨valè€Œä¸æ˜¯validation
            val_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_val.jsonl")
            validation_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_validation.jsonl")
            
            if os.path.exists(validation_file):
                file_path = validation_file
            elif os.path.exists(val_file):
                file_path = val_file
            else:
                print(f"âš ï¸  è­¦å‘Š: {dataset_name} æ²¡æœ‰æ‰¾åˆ°validationæ–‡ä»¶")
                stats[dataset_name] = 0
                continue
        else:
            file_path = os.path.join(datasets_dir, dataset_name, file_name)
        
        # åŠ è½½æ•°æ®
        data = load_jsonl(file_path)
        
        if not data:
            print(f"âš ï¸  è­¦å‘Š: {dataset_name} {split_name}é›†ä¸ºç©ºæˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            stats[dataset_name] = 0
            continue
        
        # æ ‡å‡†åŒ–æ ¼å¼
        standardized_data = standardize_format(data, dataset_name)
        
        all_data.extend(standardized_data)
        stats[dataset_name] = len(standardized_data)
        print(f"   {dataset_name}: {len(standardized_data)} æ ·æœ¬")
    
    if not all_data:
        print(f"âŒ {split_name} splitæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®")
        return stats
    
    # æ‰“ä¹±æ•°æ®ï¼ˆå¯é€‰ï¼Œå¯¹äºtesté€šå¸¸ä¸æ‰“ä¹±ï¼‰
    if split_name == "train":
        print("ğŸ”€ æ‰“ä¹±æ•°æ®é¡ºåº...")
        random.seed(seed)
        random.shuffle(all_data)
    
    # ä¿å­˜æ··åˆæ•°æ®ï¼ˆå…ˆä¿å­˜åˆ°mixedå­ç›®å½•ï¼‰
    mixed_output_dir = os.path.join(output_dir, "mixed")
    mixed_output_file = os.path.join(mixed_output_dir, f"cs_mixed_{split_name}.jsonl")
    save_jsonl(all_data, mixed_output_file)
    print(f"âœ… æ··åˆæ•°æ®å·²ä¿å­˜åˆ°: {mixed_output_file}")
    
    # æ ¼å¼åŒ–ä¸ºæœ€ç»ˆæ ¼å¼
    print("ğŸ”„ æ ¼å¼åŒ–ä¸ºæœ€ç»ˆæ ¼å¼...")
    formatted_data = format_to_final(all_data)
    
    # ä¿å­˜æ ¼å¼åŒ–æ•°æ®
    formatted_output_file = os.path.join(mixed_output_dir, f"cs_mixed_formatted_{split_name}.jsonl")
    save_jsonl(formatted_data, formatted_output_file)
    print(f"âœ… æ ¼å¼åŒ–æ•°æ®å·²ä¿å­˜åˆ°: {formatted_output_file}")
    
    total_samples = sum(stats.values())
    stats['total'] = total_samples
    
    print(f"\nğŸ“Š {split_name.upper()} æ•°æ®é›†ç»Ÿè®¡:")
    print("=" * 50)
    for dataset_name, count in stats.items():
        if dataset_name != 'total':
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            print(f"{dataset_name:15}: {count:6,} æ ·æœ¬ ({percentage:5.1f}%)")
    print("-" * 50)
    print(f"{'æ€»è®¡':15}: {total_samples:6,} æ ·æœ¬ (100.0%)")
    print()
    
    return stats

def process_individual_datasets(datasets_dir: str, output_base_dir: str, datasets_filter: List[str] = None, splits_filter: List[str] = None):
    """å¤„ç†å„ä¸ªæ•°æ®é›†çš„individual splits"""
    
    # ä½¿ç”¨è¿‡æ»¤å™¨æˆ–é»˜è®¤å€¼
    if datasets_filter:
        datasets = datasets_filter
    else:
        datasets = [
            'arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 
            'openbookqa', 'piqa', 'winogrande'
        ]
    
    if splits_filter:
        splits = splits_filter
    else:
        splits = ['train', 'validation', 'test']
    
    print("ğŸ”„ å¼€å§‹å¤„ç†å„æ•°æ®é›†çš„individual splits...")
    
    for dataset_name in datasets:
        print(f"\nğŸ“‚ å¤„ç†æ•°æ®é›†: {dataset_name}")
        
        for split_name in splits:
            # æ„å»ºè¾“å…¥æ–‡ä»¶è·¯å¾„
            file_name = f"{dataset_name}_{split_name}.jsonl"
            if split_name == "validation":
                # æ£€æŸ¥validationæˆ–valæ–‡ä»¶
                val_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_val.jsonl")
                validation_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_validation.jsonl")
                
                if os.path.exists(validation_file):
                    input_file = validation_file
                elif os.path.exists(val_file):
                    input_file = val_file
                else:
                    print(f"   âš ï¸  è·³è¿‡: æ²¡æœ‰æ‰¾åˆ°validationæ–‡ä»¶")
                    continue
            else:
                input_file = os.path.join(datasets_dir, dataset_name, file_name)
            
            if not os.path.exists(input_file):
                print(f"   âš ï¸  è·³è¿‡: {split_name} æ–‡ä»¶ä¸å­˜åœ¨")
                continue
            
            # åŠ è½½æ•°æ®
            data = load_jsonl(input_file)
            if not data:
                print(f"   âš ï¸  è·³è¿‡: {split_name} æ–‡ä»¶ä¸ºç©º")
                continue
            
            # æ ‡å‡†åŒ–æ ¼å¼
            standardized_data = standardize_format(data, dataset_name)
            
            # æ ¼å¼åŒ–ä¸ºæœ€ç»ˆæ ¼å¼
            formatted_data = format_to_final(standardized_data)
            
            # æ„å»ºè¾“å‡ºè·¯å¾„
            output_dir = os.path.join(output_base_dir, dataset_name)
            output_file = os.path.join(output_dir, f"{dataset_name}_{split_name}_formatted.jsonl")
            
            # ä¿å­˜æ ¼å¼åŒ–æ•°æ®
            save_jsonl(formatted_data, output_file)
            print(f"   âœ… {split_name}: {len(formatted_data)} æ ·æœ¬ -> {output_file}")

def validate_datasets(datasets_dir: str, datasets: List[str], splits: List[str]) -> bool:
    """éªŒè¯æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®"""
    print("ğŸ” éªŒè¯æ•°æ®é›†æ–‡ä»¶...")
    
    all_valid = True
    for dataset_name in datasets:
        print(f"\nğŸ“‚ éªŒè¯ {dataset_name}:")
        
        for split_name in splits:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            file_name = f"{dataset_name}_{split_name}.jsonl"
            if split_name == "validation":
                val_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_val.jsonl")
                validation_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_validation.jsonl")
                
                if os.path.exists(validation_file):
                    file_path = validation_file
                elif os.path.exists(val_file):
                    file_path = val_file
                else:
                    print(f"   âŒ {split_name}: æ–‡ä»¶ä¸å­˜åœ¨")
                    all_valid = False
                    continue
            else:
                file_path = os.path.join(datasets_dir, dataset_name, file_name)
            
            if not os.path.exists(file_path):
                print(f"   âŒ {split_name}: æ–‡ä»¶ä¸å­˜åœ¨ ({file_path})")
                all_valid = False
                continue
            
            # å°è¯•åŠ è½½å¹¶éªŒè¯æ ¼å¼
            try:
                data = load_jsonl(file_path)
                if not data:
                    print(f"   âš ï¸ {split_name}: æ–‡ä»¶ä¸ºç©º")
                else:
                    print(f"   âœ… {split_name}: {len(data)} æ ·æœ¬")
            except Exception as e:
                print(f"   âŒ {split_name}: æ ¼å¼é”™è¯¯ - {e}")
                all_valid = False
    
    if all_valid:
        print("\nâœ… æ‰€æœ‰æ•°æ®é›†éªŒè¯é€šè¿‡")
    else:
        print("\nâŒ æ•°æ®é›†éªŒè¯å¤±è´¥")
    
    return all_valid

def show_processing_plan(datasets: List[str], splits: List[str], process_mixed: bool, process_individual: bool, output_dir: str):
    """æ˜¾ç¤ºå¤„ç†è®¡åˆ’"""
    print("ğŸ“‹ å¤„ç†è®¡åˆ’:")
    print(f"  æ•°æ®é›†: {datasets}")
    print(f"  Splits: {splits}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    if process_mixed:
        print("ğŸ”„ å°†ç”Ÿæˆæ··åˆæ•°æ®é›†:")
        for split in splits:
            print(f"  - {output_dir}/mixed/cs_mixed_{split}.jsonl")
            print(f"  - {output_dir}/mixed/cs_mixed_formatted_{split}.jsonl")
        print()
    
    if process_individual:
        print("ğŸ”„ å°†ç”Ÿæˆä¸ªåˆ«æ•°æ®é›†:")
        for dataset in datasets:
            for split in splits:
                print(f"  - {output_dir}/{dataset}/{dataset}_{split}_formatted.jsonl")
    
    print("ğŸƒ è¿™æ˜¯dry runæ¨¡å¼ï¼Œä¸ä¼šå®é™…å¤„ç†æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ‰©å±•çš„Commonsenseæ•°æ®é›†å¤„ç†Pipeline")
    parser.add_argument("--datasets", nargs='*', 
                       help="è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨ã€‚é»˜è®¤å¤„ç†æ‰€æœ‰7ä¸ªæ•°æ®é›†ã€‚å¯é€‰: arc-challenge, arc-easy, boolq, hellaswag, openbookqa, piqa, winogrande")
    parser.add_argument("--splits", nargs='*', default=['train', 'validation', 'test'],
                       help="è¦å¤„ç†çš„splitsåˆ—è¡¨ã€‚é»˜è®¤: train validation test")
    parser.add_argument("--datasets_dir", type=str, default="raw_datasets/cs",
                       help="åŸå§‹æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="data_to_lora/cs",
                       help="è¾“å‡ºåŸºç›®å½•")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­ç”¨äºæ‰“ä¹±æ•°æ®")
    parser.add_argument("--mixed_only", action="store_true",
                       help="åªç”Ÿæˆmixedæ•°æ®ï¼Œä¸å¤„ç†individualæ•°æ®é›†")
    parser.add_argument("--individual_only", action="store_true",
                       help="åªå¤„ç†individualæ•°æ®é›†ï¼Œä¸ç”Ÿæˆmixedæ•°æ®")
    parser.add_argument("--validate_only", action="store_true",
                       help="ä»…éªŒè¯æ•°æ®æ ¼å¼ï¼Œä¸ç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--dry_run", action="store_true",
                       help="å¹²è¿è¡Œï¼Œæ˜¾ç¤ºå¤„ç†è®¡åˆ’ä½†ä¸å®é™…å¤„ç†")
    
    args = parser.parse_args()
    
    # æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†
    all_datasets = [
        'arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 
        'openbookqa', 'piqa', 'winogrande'
    ]
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if args.datasets is None or len(args.datasets) == 0:
        # é»˜è®¤ï¼šå¤„ç†æ‰€æœ‰æ•°æ®é›†
        datasets_to_process = all_datasets
        print("ğŸš€ Commonsenseæ•°æ®é›†å¤„ç†Pipeline")
        print("=" * 70)
        print(f"åŸå§‹æ•°æ®ç›®å½•: {args.datasets_dir}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"å¤„ç†æ¨¡å¼: æ‰€æœ‰æ•°æ®é›† (é»˜è®¤)")
        print(f"å¤„ç†splits: {args.splits}")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
    else:
        # éªŒè¯ç”¨æˆ·æŒ‡å®šçš„æ•°æ®é›†
        datasets_to_process = []
        for dataset in args.datasets:
            if dataset in all_datasets:
                datasets_to_process.append(dataset)
            else:
                print(f"âš ï¸ è­¦å‘Š: æœªçŸ¥æ•°æ®é›† '{dataset}'ï¼Œå°†è¢«è·³è¿‡")
        
        if not datasets_to_process:
            print("âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®é›†è¦å¤„ç†")
            return False
        
        print("ğŸš€ Commonsenseæ•°æ®é›†å¤„ç†Pipeline")
        print("=" * 70)
        print(f"åŸå§‹æ•°æ®ç›®å½•: {args.datasets_dir}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"å¤„ç†æ•°æ®é›†: {datasets_to_process}")
        print(f"å¤„ç†splits: {args.splits}")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
    
    # æ£€æŸ¥äº’æ–¥å‚æ•°
    if args.mixed_only and args.individual_only:
        print("âŒ é”™è¯¯: --mixed_only å’Œ --individual_only ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        return False
    
    # ç¡®å®šå¤„ç†æ¨¡å¼
    process_mixed = not args.individual_only
    process_individual = not args.mixed_only
    
    if args.validate_only:
        print("\nğŸ” éªŒè¯æ¨¡å¼: æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§...")
        return validate_datasets(args.datasets_dir, datasets_to_process, args.splits)
    
    if args.dry_run:
        print("\nğŸƒ Dry Runæ¨¡å¼: æ˜¾ç¤ºå¤„ç†è®¡åˆ’...")
        show_processing_plan(datasets_to_process, args.splits, process_mixed, process_individual, args.output_dir)
        return True
    
    try:
        success = True
        all_stats = {}
        
        # 1. å¤„ç†æ··åˆæ•°æ®é›†
        if process_mixed:
            print("\n" + "=" * 70)
            print("ğŸ”„ æ­¥éª¤1: å¤„ç†æ··åˆæ•°æ®é›†...")
            print("=" * 70)
            
            for split_name in args.splits:
                print(f"\nğŸ“‹ å¤„ç† {split_name.upper()} split...")
                stats = process_single_split(
                    datasets_dir=args.datasets_dir,
                    split_name=split_name,
                    output_dir=args.output_dir,
                    datasets_filter=datasets_to_process,
                    seed=args.seed
                )
                all_stats[split_name] = stats
        
        # 2. å¤„ç†ä¸ªåˆ«æ•°æ®é›†
        if process_individual:
            print("\n" + "=" * 70)
            print("ğŸ”„ æ­¥éª¤2: å¤„ç†ä¸ªåˆ«æ•°æ®é›†...")
            print("=" * 70)
            process_individual_datasets(
                datasets_dir=args.datasets_dir,
                output_base_dir=args.output_dir,
                datasets_filter=datasets_to_process,
                splits_filter=args.splits
            )
        
        # 3. ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        print("\n" + "=" * 70)
        print("ğŸ‰ å¤„ç†å®Œæˆ!")
        print("=" * 70)
        
        if process_mixed and all_stats:
            print("\nğŸ“ˆ æ··åˆæ•°æ®é›†ç»Ÿè®¡:")
            for split_name, stats in all_stats.items():
                total = stats.get('total', 0)
                print(f"  {split_name:12}: {total:6,} æ ·æœ¬")
        
        print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°: {args.output_dir}")
        
        return success
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
