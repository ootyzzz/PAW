"""
mix_commonsense_datasets.py
åˆå¹¶å¹¶æ‰“ä¹±7ä¸ªcommonsenseæ•°æ®é›†çš„å·¥å…·
"""

import json
import random
import os
from pathlib import Path
from typing import List, Dict, Any

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒæ ¼å¼ï¼ˆJSONå’ŒPythonå­—å…¸æ ¼å¼ï¼‰"""
    data = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        # å…ˆå°è¯•æ ‡å‡†JSONè§£æ
                        data.append(json.loads(line))
                    except json.JSONDecodeError:
                        try:
                            # å¦‚æœå¤±è´¥ï¼Œå°è¯•evalè§£æï¼ˆå¤„ç†å•å¼•å·æ ¼å¼ï¼Œå¦‚ARCæ•°æ®é›†ï¼‰
                            data.append(eval(line))
                        except Exception as e:
                            print(f"âš ï¸  è­¦å‘Š: æ— æ³•è§£æç¬¬{line_num}è¡Œ: {str(e)[:100]}...")
                            continue
    return data

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def standardize_format(data: List[Dict[str, Any]], dataset_name: str) -> List[Dict[str, Any]]:
    """
    æ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†çš„æ ¼å¼
    ç»Ÿä¸€è½¬æ¢ä¸ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„æ ¼å¼ï¼š
    - dataset_source: æ•°æ®é›†æ¥æº
    - question: é—®é¢˜æ–‡æœ¬
    - choices: é€‰æ‹©é¡¹åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
    - answer: ç­”æ¡ˆ
    - original_data: åŸå§‹æ•°æ®ï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
    """
    standardized = []
    
    for item in data:
        standard_item = {
            "dataset_source": dataset_name,
            "original_data": item.copy()
        }
        
        if dataset_name in ['arc-challenge', 'arc-easy']:
            # ARCæ ¼å¼: question, choices{text, label}, answerKey
            standard_item.update({
                "question": item["question"],
                "choices": item["choices"]["text"],
                "answer": item["answerKey"],
                "answer_index": ord(item["answerKey"]) - ord('A')
            })
            
        elif dataset_name == 'boolq':
            # BoolQæ ¼å¼: question, answer (true/false), passage
            standard_item.update({
                "question": item["question"],
                "choices": ["False", "True"],
                "answer": "True" if item["answer"] else "False",
                "answer_index": 1 if item["answer"] else 0,
                "passage": item.get("passage", "")
            })
            
        elif dataset_name == 'hellaswag':
            # HellaSwagæ ¼å¼: ctx, endings, label
            standard_item.update({
                "question": item["ctx"],
                "choices": item["endings"],
                "answer_index": int(item["label"]) if item["label"] != "" else -1,
                "answer": item["endings"][int(item["label"])] if item["label"] != "" and item["label"].isdigit() else ""
            })
            
        elif dataset_name == 'openbookqa':
            # OpenBookQAæ ¼å¼: question_stem, choices{text, label}, answerKey
            standard_item.update({
                "question": item["question_stem"],
                "choices": item["choices"]["text"],
                "answer": item["answerKey"],
                "answer_index": ord(item["answerKey"]) - ord('A')
            })
            
        elif dataset_name == 'piqa':
            # PIQAæ ¼å¼: goal, sol1, sol2 (æ²¡æœ‰labelï¼Œè¿™æ˜¯ä¸€ä¸ªé€‰æ‹©é¢˜ä½†æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ)
            standard_item.update({
                "question": item["goal"],
                "choices": [item["sol1"], item["sol2"]],
                "answer_index": -1,  # PIQAæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾
                "answer": ""  # æ²¡æœ‰é¢„å®šä¹‰ç­”æ¡ˆ
            })
            
        elif dataset_name == 'winogrande':
            # Winograndeæ ¼å¼: sentence, option1, option2, answer
            standard_item.update({
                "question": item["sentence"],
                "choices": [item["option1"], item["option2"]],
                "answer": item["answer"],
                "answer_index": 0 if item["answer"] == "1" else 1 if item["answer"] == "2" else -1
            })
        
        standardized.append(standard_item)
    
    return standardized

def mix_commonsense_datasets(
    datasets_dir: str = "raw_datasets",
    output_dir: str = "raw_datasets/commonsense",
    seed: int = 42,
    max_samples_per_dataset: int = None
) -> Dict[str, int]:
    """
    åˆå¹¶å¹¶æ‰“ä¹±7ä¸ªcommonsenseæ•°æ®é›†
    
    Args:
        datasets_dir: æ•°æ®é›†æ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        seed: éšæœºç§å­
        max_samples_per_dataset: æ¯ä¸ªæ•°æ®é›†æœ€å¤§æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # å®šä¹‰æ•°æ®é›†åˆ—è¡¨
    datasets = [
        'arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 
        'openbookqa', 'piqa', 'winogrande'
    ]
    
    all_data = []
    stats = {}
    
    print("ğŸ”„ å¼€å§‹åˆå¹¶æ•°æ®é›†...")
    
    for dataset_name in datasets:
        # print(f"ğŸ“‚ å¤„ç†æ•°æ®é›†: {dataset_name}")
        
        # æ„å»ºè®­ç»ƒé›†æ–‡ä»¶è·¯å¾„
        train_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_train.jsonl")
        
        # åŠ è½½æ•°æ®
        data = load_jsonl(train_file)
        
        if not data:
            print(f"âš ï¸  è­¦å‘Š: {dataset_name} è®­ç»ƒé›†ä¸ºç©ºæˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            stats[dataset_name] = 0
            continue
        
        # print(f"   åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if max_samples_per_dataset and len(data) > max_samples_per_dataset:
            data = random.sample(data, max_samples_per_dataset)
            print(f"   éšæœºé‡‡æ · {max_samples_per_dataset} ä¸ªæ ·æœ¬")
        
        # æ ‡å‡†åŒ–æ ¼å¼
        standardized_data = standardize_format(data, dataset_name)
        
        all_data.extend(standardized_data)
        stats[dataset_name] = len(standardized_data)
    
    # æ‰“ä¹±æ•°æ®
    print("ğŸ”€ æ‰“ä¹±æ•°æ®é¡ºåº...")
    random.shuffle(all_data)
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†
    output_file = os.path.join(output_dir, "cs_mixed.jsonl")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
    save_jsonl(all_data, output_file)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(all_data)
    stats['total'] = total_samples
    
    print("\nğŸ“Š æ•°æ®é›†åˆå¹¶å®Œæˆ!")
    print("=" * 50)
    for dataset_name, count in stats.items():
        if dataset_name != 'total':
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            print(f"{dataset_name:15}: {count:6,} æ ·æœ¬ ({percentage:5.1f}%)")
    print("-" * 50)
    print(f"{'æ€»è®¡':15}: {total_samples:6,} æ ·æœ¬ (100.0%)")
    
    return stats

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆå¹¶commonsenseæ•°æ®é›†")
    parser.add_argument("--datasets_dir", type=str, default="raw_datasets",
                       help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="raw_datasets/commonsense",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="æ¯ä¸ªæ•°æ®é›†æœ€å¤§æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    # åˆå¹¶æ•°æ®é›†
    stats = mix_commonsense_datasets(
        datasets_dir=args.datasets_dir,
        output_dir=args.output_dir,
        seed=args.seed,
        max_samples_per_dataset=args.max_samples
    )

if __name__ == "__main__":
    main()
