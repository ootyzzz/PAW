"""
create_balanced_commonsense.py
åˆ›å»ºå¹³è¡¡çš„commonsenseæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°é‡ç›¸ç­‰
"""

import json
import random
import os
from pathlib import Path
from typing import List, Dict, Any

def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ–‡ä»¶ï¼Œæ”¯æŒä¸åŒæ ¼å¼ï¼ˆJSONå’ŒPythonå­—å…¸æ ¼å¼ï¼‰"""
    data = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        # å…ˆå°è¯•æ ‡å‡†JSONè§£æ
                        data.append(json.loads(line))
                    except json.JSONDecodeError:
                        try:
                            # å¦‚æœå¤±è´¥ï¼Œå°è¯•evalè§£æï¼ˆå¤„ç†å•å¼•å·æ ¼å¼ï¼Œå¦‚ARCæ•°æ®é›†ï¼‰
                            data.append(eval(line))
                        except Exception as e:
                            print(f"âš ï¸  è­¦å‘Š: æ— æ³•è§£æç¬¬{line_num}è¡Œ: {str(e)[:100]}...")
                            continue
    return data

def save_jsonl(data: List[Dict[str, Any]], file_path: str):
    """ä¿å­˜æ•°æ®åˆ°JSONLæ–‡ä»¶"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

def standardize_format(data: List[Dict[str, Any]], dataset_name: str) -> List[Dict[str, Any]]:
    """
    æ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†çš„æ ¼å¼
    ç»Ÿä¸€è½¬æ¢ä¸ºåŒ…å«ä»¥ä¸‹å­—æ®µçš„æ ¼å¼ï¼š
    - dataset_source: æ•°æ®é›†æ¥æº
    - question: é—®é¢˜æ–‡æœ¬
    - choices: é€‰æ‹©é¡¹åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
    - answer: ç­”æ¡ˆ
    - original_data: åŸå§‹æ•°æ®ï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
    """
    standardized = []
    
    for item in data:
        standard_item = {
            "dataset_source": dataset_name,
            "original_data": item.copy()
        }
        
        if dataset_name in ['arc-challenge', 'arc-easy']:
            # ARCæ ¼å¼: question, choices{text, label}, answerKey
            standard_item.update({
                "question": item["question"],
                "choices": item["choices"]["text"],
                "answer": item["answerKey"],
                "answer_index": ord(item["answerKey"]) - ord('A')
            })
            
        elif dataset_name == 'boolq':
            # BoolQæ ¼å¼: question, answer (true/false), passage
            standard_item.update({
                "question": item["question"],
                "choices": ["False", "True"],
                "answer": "True" if item["answer"] else "False",
                "answer_index": 1 if item["answer"] else 0,
                "passage": item.get("passage", "")
            })
            
        elif dataset_name == 'hellaswag':
            # HellaSwagæ ¼å¼: ctx, endings, label
            standard_item.update({
                "question": item["ctx"],
                "choices": item["endings"],
                "answer_index": int(item["label"]) if item["label"] != "" else -1,
                "answer": item["endings"][int(item["label"])] if item["label"] != "" and item["label"].isdigit() else ""
            })
            
        elif dataset_name == 'openbookqa':
            # OpenBookQAæ ¼å¼: question_stem, choices{text, label}, answerKey
            standard_item.update({
                "question": item["question_stem"],
                "choices": item["choices"]["text"],
                "answer": item["answerKey"],
                "answer_index": ord(item["answerKey"]) - ord('A')
            })
            
        elif dataset_name == 'piqa':
            # PIQAæ ¼å¼: goal, sol1, sol2 (æ²¡æœ‰labelï¼Œè¿™æ˜¯ä¸€ä¸ªé€‰æ‹©é¢˜ä½†æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ)
            standard_item.update({
                "question": item["goal"],
                "choices": [item["sol1"], item["sol2"]],
                "answer_index": -1,  # PIQAæ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ ‡ç­¾
                "answer": ""  # æ²¡æœ‰é¢„å®šä¹‰ç­”æ¡ˆ
            })
            
        elif dataset_name == 'winogrande':
            # Winograndeæ ¼å¼: sentence, option1, option2, answer
            standard_item.update({
                "question": item["sentence"],
                "choices": [item["option1"], item["option2"]],
                "answer": item["answer"],
                "answer_index": 0 if item["answer"] == "1" else 1 if item["answer"] == "2" else -1
            })
        
        standardized.append(standard_item)
    
    return standardized

def create_balanced_dataset(
    datasets_dir: str = "raw_datasets",
    output_dir: str = "raw_datasets/commonsense",
    samples_per_dataset: int = None,
    seed: int = 42
) -> Dict[str, int]:
    """
    åˆ›å»ºå¹³è¡¡çš„commonsenseæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°é‡ç›¸ç­‰
    
    Args:
        datasets_dir: æ•°æ®é›†æ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        samples_per_dataset: æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æœ€å°æ•°æ®é›†çš„å¤§å°ï¼‰
        seed: éšæœºç§å­
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # å®šä¹‰æ•°æ®é›†åˆ—è¡¨
    datasets = [
        'arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 
        'openbookqa', 'piqa', 'winogrande'
    ]
    
    dataset_sizes = {}
    all_datasets_data = {}
    
    print("ğŸ”„ åˆ†æå„æ•°æ®é›†å¤§å°...")
    
    # é¦–å…ˆåˆ†ææ‰€æœ‰æ•°æ®é›†çš„å¤§å°
    for dataset_name in datasets:
        train_file = os.path.join(datasets_dir, dataset_name, f"{dataset_name}_train.jsonl")
        data = load_jsonl(train_file)
        
        if not data:
            print(f"âš ï¸  è­¦å‘Š: {dataset_name} è®­ç»ƒé›†ä¸ºç©ºæˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            continue
            
        dataset_sizes[dataset_name] = len(data)
        all_datasets_data[dataset_name] = data
        print(f"ğŸ“‚ {dataset_name}: {len(data)} æ ·æœ¬")
    
    # ç¡®å®šæ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ•°é‡
    if samples_per_dataset is None:
        # ä½¿ç”¨æœ€å°æ•°æ®é›†çš„å¤§å°
        samples_per_dataset = min(dataset_sizes.values())
        print(f"\nğŸ¯ è‡ªåŠ¨è®¾ç½®æ¯ä¸ªæ•°æ®é›†æ ·æœ¬æ•°ä¸º: {samples_per_dataset} (æœ€å°æ•°æ®é›†å¤§å°)")
    else:
        print(f"\nğŸ¯ è®¾ç½®æ¯ä¸ªæ•°æ®é›†æ ·æœ¬æ•°ä¸º: {samples_per_dataset}")
    
    balanced_data = []
    balanced_stats = {}
    
    print("\nğŸ”„ åˆ›å»ºå¹³è¡¡æ•°æ®é›†...")
    
    for dataset_name in datasets:
        if dataset_name not in all_datasets_data:
            balanced_stats[dataset_name] = 0
            continue
            
        data = all_datasets_data[dataset_name]
        
        # å¦‚æœæ•°æ®é‡å¤§äºç›®æ ‡æ•°é‡ï¼Œéšæœºé‡‡æ ·
        if len(data) > samples_per_dataset:
            sampled_data = random.sample(data, samples_per_dataset)
            print(f"ğŸ“Š {dataset_name}: ä» {len(data)} ä¸ªæ ·æœ¬ä¸­éšæœºé‡‡æ · {samples_per_dataset} ä¸ª")
        else:
            sampled_data = data
            print(f"ğŸ“Š {dataset_name}: ä½¿ç”¨å…¨éƒ¨ {len(data)} ä¸ªæ ·æœ¬")
        
        # æ ‡å‡†åŒ–æ ¼å¼
        standardized_data = standardize_format(sampled_data, dataset_name)
        balanced_data.extend(standardized_data)
        balanced_stats[dataset_name] = len(standardized_data)
    
    # æ‰“ä¹±æ•°æ®
    print("ğŸ”€ æ‰“ä¹±æ•°æ®é¡ºåº...")
    random.shuffle(balanced_data)
    
    # ä¿å­˜å¹³è¡¡æ•°æ®é›†
    output_file = os.path.join(output_dir, "cs_balanced.jsonl")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
    save_jsonl(balanced_data, output_file)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(balanced_data)
    balanced_stats['total'] = total_samples
    
    print("\nğŸ“Š å¹³è¡¡æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print("=" * 50)
    for dataset_name, count in balanced_stats.items():
        if dataset_name != 'total':
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            print(f"{dataset_name:15}: {count:6,} æ ·æœ¬ ({percentage:5.1f}%)")
    print("-" * 50)
    print(f"{'æ€»è®¡':15}: {total_samples:6,} æ ·æœ¬ (100.0%)")
    
    return balanced_stats

def update_stats_file(output_dir: str, all_stats: Dict[str, int], balanced_stats: Dict[str, int]):
    """æ›´æ–°ç»Ÿè®¡æ–‡ä»¶ï¼ŒåŒ…å«å…¨é‡å’Œå¹³è¡¡æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯"""
    
    combined_stats = {
        "cs_all": all_stats,
        "cs_balanced": balanced_stats,
        "description": {
            "cs_all": "åŒ…å«æ‰€æœ‰7ä¸ªæ•°æ®é›†çš„å®Œæ•´è®­ç»ƒæ•°æ®",
            "cs_balanced": "æ¯ä¸ªæ•°æ®é›†æ ·æœ¬æ•°é‡ç›¸ç­‰çš„å¹³è¡¡æ•°æ®é›†"
        }
    }
    
    stats_file = os.path.join(output_dir, "dataset_stats.json")
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(combined_stats, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯å·²æ›´æ–°åˆ°: {stats_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ›å»ºå¹³è¡¡çš„commonsenseæ•°æ®é›†")
    parser.add_argument("--datasets_dir", type=str, default="raw_datasets",
                       help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--output_dir", type=str, default="raw_datasets/commonsense",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--samples_per_dataset", type=int, default=None,
                       help="æ¯ä¸ªæ•°æ®é›†çš„æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æœ€å°æ•°æ®é›†çš„å¤§å°ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹³è¡¡æ•°æ®é›†
    balanced_stats = create_balanced_dataset(
        datasets_dir=args.datasets_dir,
        output_dir=args.output_dir,
        samples_per_dataset=args.samples_per_dataset,
        seed=args.seed
    )
    
    # è¯»å–ç°æœ‰çš„å…¨é‡æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(args.output_dir, "dataset_stats.json")
    if os.path.exists(stats_file):
        with open(stats_file, 'r', encoding='utf-8') as f:
            all_stats = json.load(f)
    else:
        # å¦‚æœæ²¡æœ‰ç°æœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
        all_stats = {
            "arc-challenge": 1119,
            "arc-easy": 2251,
            "boolq": 9427,
            "hellaswag": 39905,
            "openbookqa": 4957,
            "piqa": 16113,
            "winogrande": 40398,
            "total": 114170
        }
    
    # æ›´æ–°ç»Ÿè®¡æ–‡ä»¶
    update_stats_file(args.output_dir, all_stats, balanced_stats)

if __name__ == "__main__":
    main()
