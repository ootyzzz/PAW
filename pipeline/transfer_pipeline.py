#!/usr/bin/env python3
"""
transfer_pipeline.py
è‡ªåŠ¨åŒ–LoRAè®­ç»ƒå’Œè¿ç§»ç®¡é“

ä¸»è¦åŠŸèƒ½:
1. è®­ç»ƒ source model + LoRA
2. è¿ç§» source LoRA â†’ target model
3. è¯„ä¼°ç›®æ ‡åŸºç¡€æ¨¡å‹
4. è¯„ä¼°è¿ç§»LoRA
5. è®­ç»ƒ target model + LoRA (å¯é€‰)
6. è¯„ä¼°æºåŸºç¡€æ¨¡å‹
7. ç”Ÿæˆè¯¦ç»†çš„ç»“æœæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
python transfer_pipeline.py \
  --source_model Llama-3.2-3B-Instruct \
  --target_model Qwen_Qwen2.5-1.5B \
  --dataset arc-challenge

python transfer_pipeline.py \
  --source_model gemma-2-2b-it \
  --target_model Qwen_Qwen2.5-1.5B \
  --dataset arc-challenge

python pipeline/experiments/run_single_experiment.py --base_model /root/autodl-tmp/models/Qwen_Qwen2.5-1.5B --target_model /root/autodl-tmp/models/Llama-3.2-3B-Instruct --dataset arc-challenge --eval_only
    
å¿«é€Ÿæµ‹è¯•:
python transfer_pipeline.py --quick_test
"""

import os
import sys
import argparse
import warnings

# æ·»åŠ pipelineæ¨¡å—åˆ°è·¯å¾„
pipeline_root = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'pipeline')
if pipeline_root not in sys.path:
    sys.path.insert(0, pipeline_root)

# ä¿®å¤MKLçº¿ç¨‹å±‚å†²çª - å¿…é¡»åœ¨å¯¼å…¥numpy/pandasä¹‹å‰è®¾ç½®
os.environ['MKL_SERVICE_FORCE_INTEL'] = '1'
os.environ['MKL_THREADING_LAYER'] = 'GNU'

# å±è”½ Transformers è­¦å‘Šï¼Œä½†ä¿ç•™é‡è¦ä¿¡æ¯
warnings.filterwarnings("ignore", message=".*cache_implementation.*")
warnings.filterwarnings("ignore", message=".*generation flags are not valid.*")
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

from core.pipeline import TransferPipeline


def main():
    parser = argparse.ArgumentParser(
        description="LoRAè®­ç»ƒå’Œè¿ç§»è‡ªåŠ¨åŒ–ç®¡é“",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼
  python transfer_pipeline.py
  
  # å¿«é€Ÿæµ‹è¯• (0.5Bâ†’1.5B, 20æ­¥è®­ç»ƒ, 5%è¯„ä¼°)
  python transfer_pipeline.py --quick_test
  
  # è‡ªå®šä¹‰æ¨¡å‹
  python transfer_pipeline.py --source_model gemma-2-2b-it --target_model Qwen_Qwen2.5-1.5B --dataset arc-challenge
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--source_model", type=str, default=None,
                       help="æºæ¨¡å‹è·¯å¾„æˆ–åç§° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶)")
    parser.add_argument("--target_model", type=str, default=None,
                       help="ç›®æ ‡æ¨¡å‹è·¯å¾„æˆ–åç§° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶)")
    parser.add_argument("--dataset", type=str, default=None,
                       help="æ•°æ®é›†åç§° (é»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶)")
    parser.add_argument("--config", type=str, 
                       default=os.path.join(os.path.dirname(__file__), "config", "pipeline_config.yaml"),
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--eval_only", action="store_true",
                       help="ä»…è¿è¡Œè¯„ä¼°ï¼Œè·³è¿‡è®­ç»ƒå’Œè¿ç§»")
    parser.add_argument("--quick_test", action="store_true",
                       help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šè‡ªåŠ¨ä½¿ç”¨1.5Bâ†’0.5Bé…ç½®")
    
    args = parser.parse_args()
    
    # Quick test mode: use preset configuration
    if args.quick_test:
        print("Quick Test Mode: 0.5B â†’ 1.5B")
        
        # Create pipeline instance (using quick test configuration)
        pipeline = TransferPipeline(quick_test=True)
        
        # Use recommended configuration
        if not args.source_model:
            args.source_model = pipeline.config.get('recommended_models.source')
        if not args.target_model:
            args.target_model = pipeline.config.get('recommended_models.target')
        if not args.dataset:
            args.dataset = pipeline.config.get('recommended_models.dataset')
            
        print(f"Source Model: {args.source_model}")
        print(f"Target Model: {args.target_model}")
        print(f"Dataset: {args.dataset}")
        print(f"Training Steps: 20, Evaluation Ratio: 5%")
        print("")
    else:
        # Create pipeline instance (using normal configuration)
        pipeline = TransferPipeline(args.config)
        
        # Use default values from config if not provided
        if not args.source_model:
            args.source_model = pipeline.config.get('default_experiment.source_model')
        if not args.target_model:
            args.target_model = pipeline.config.get('default_experiment.target_model')
        if not args.dataset:
            args.dataset = pipeline.config.get('default_experiment.dataset')
        if not args.eval_only:
            args.eval_only = pipeline.config.get('default_experiment.eval_only', False)
        
        # Validate that we have all required parameters
        if not all([args.source_model, args.target_model, args.dataset]):
            print("ERROR: Missing required parameters. Check configuration file or provide:")
            print("  --source_model [model_name]")
            print("  --target_model [model_name]") 
            print("  --dataset [dataset_name]")
            print("NOTE: Or use --quick_test for automatic configuration")
            return False
        
        print(f"Using Configuration: {args.config}")
        print(f"Source Model: {args.source_model}")
        print(f"Target Model: {args.target_model}")
        print(f"Dataset: {args.dataset}")
        print(f"Eval Only: {args.eval_only}")
        print("")
    
    # å¤„ç†æ¨¡å‹è·¯å¾„
    if not args.source_model.startswith('/'):
        args.source_model = pipeline.config.get_model_path(args.source_model)
    if not args.target_model.startswith('/'):
        args.target_model = pipeline.config.get_model_path(args.target_model)
    
    # éªŒè¯æ¨¡å‹å­˜åœ¨
    if not os.path.exists(args.source_model):
        print(f"âŒ æºæ¨¡å‹ä¸å­˜åœ¨: {args.source_model}")
        return False
    if not os.path.exists(args.target_model):
        print(f"âŒ ç›®æ ‡æ¨¡å‹ä¸å­˜åœ¨: {args.target_model}")
        return False
    
    # éªŒè¯æ•°æ®é›†
    supported_datasets = pipeline.config.get('training.datasets', [])
    if args.dataset not in supported_datasets:
        print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {args.dataset}")
        print(f"âœ… æ”¯æŒçš„æ•°æ®é›†: {', '.join(supported_datasets)}")
        return False
    
    # è¿è¡Œç®¡é“
    success = pipeline.run_pipeline(
        args.source_model, 
        args.target_model, 
        args.dataset,
        eval_only=args.eval_only
    )
    
    if success:
        print(f"\nğŸ‰ ç®¡é“æ‰§è¡ŒæˆåŠŸ!")
        return True
    else:
        print(f"\nâŒ ç®¡é“æ‰§è¡Œå¤±è´¥!")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
