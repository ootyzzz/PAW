"""
ç»“æœç®¡ç†æ¨¡å—
è´Ÿè´£å®éªŒç»“æœçš„ä¿å­˜ã€åŠ è½½å’Œç®¡ç†
"""

import os
import json
import pandas as pd
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
from .config import PipelineConfig
from .utils import ModelUtils, DataCleaner


class ResultsManager:
    """ç»“æœç®¡ç†å™¨"""
    
    def __init__(self, config: PipelineConfig, verbose: bool = True):
        self.config = config
        self.verbose = verbose
        self._ensure_results_dir()
    
    def _ensure_results_dir(self):
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        results_dir = self.config.get('paths.results_dir')
        os.makedirs(results_dir, exist_ok=True)
    
    def save_results(self, experiment_data: Dict[str, Any]):
        """ä¿å­˜å®éªŒç»“æœ"""
        if self.verbose:
            print(f"\nğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        
        # æ¸…ç†æ•°æ®
        clean_data = DataCleaner.clean_dict(experiment_data)
        
        # ä¿å­˜åˆ°CSV
        success = self._save_to_csv(clean_data)
        
        if success:
            # æ›´æ–°Markdownæ€»ç»“
            try:
                self._update_markdown_summary()
                if self.verbose:
                    csv_path = os.path.join(self.config.get('paths.results_dir'), 
                                          self.config.get('results.csv_file'))
                    md_path = os.path.join(self.config.get('paths.results_dir'), 
                                         self.config.get('results.markdown_file'))
                    print(f"Results saved to:")
                    print(f"  CSV: {csv_path}")
                    print(f"  Markdown: {md_path}")
            except Exception as md_error:
                if self.verbose:
                    print(f"WARNING: Failed to update Markdown: {md_error}")
        else:
            # Fallback save
            self._backup_save(clean_data)
    
    def _save_to_csv(self, clean_data: Dict[str, Any]) -> bool:
        """ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        csv_path = os.path.join(self.config.get('paths.results_dir'), 
                               self.config.get('results.csv_file'))
        
        try:
            # å®šä¹‰æœŸæœ›çš„åˆ—
            expected_columns = [
                'experiment_id', 'source_model', 'target_model', 'dataset',
                'source_acc', 'source_lora_acc', 'target_acc', 'target_lora_acc', 'transferred_acc',
                'source_lora_path', 'target_lora_path', 'transferred_lora_path',
                'timestamp', 'notes', 'training_config'
            ]
            
            # ç¡®ä¿æ‰€æœ‰æœŸæœ›çš„åˆ—éƒ½å­˜åœ¨
            for col in expected_columns:
                if col not in clean_data:
                    clean_data[col] = None
            
            # åˆ›å»ºæ–°è¡Œï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯åŸºæœ¬ç±»å‹
            new_row_data = {}
            for col in expected_columns:
                value = clean_data[col]
                if value is None:
                    new_row_data[col] = None
                else:
                    new_row_data[col] = str(value) if not isinstance(value, (int, float, bool)) else value
            
            df_new = pd.DataFrame([new_row_data])
            
            # å¤„ç†ç°æœ‰æ•°æ®
            if os.path.exists(csv_path):
                try:
                    # å®‰å…¨è¯»å–CSVï¼Œå¤„ç†å¯èƒ½çš„numpyæ•°ç»„
                    df_existing = pd.read_csv(csv_path, converters={
                        col: lambda x: x if not str(x).startswith('[') else str(x) 
                        for col in expected_columns
                    })
                    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
                    for col in expected_columns:
                        if col not in df_existing.columns:
                            df_existing[col] = None
                    df_existing = df_existing[expected_columns]
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                except Exception as e:
                    if self.verbose:
                        print(f"WARNING: Error reading existing results: {e}")
                        print("NOTE: Will create new results file")
                    df_combined = df_new
            else:
                df_combined = df_new
            
            df_combined.to_csv(csv_path, index=False)
            return True
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ ä¿å­˜CSVæ—¶å‡ºé”™: {e}")
            return False
    
    def _backup_save(self, clean_data: Dict[str, Any]):
        """å¤‡ç”¨ä¿å­˜æ–¹æ³•"""
        csv_path = os.path.join(self.config.get('paths.results_dir'), 
                               self.config.get('results.csv_file'))
        backup_path = csv_path.replace('.csv', '_backup.txt')
        
        try:
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(f"å®éªŒç»“æœå¤‡ä»½ - {datetime.now()}\n")
                f.write("=" * 50 + "\n")
                for key, value in clean_data.items():
                    f.write(f"{key}: {value}\n")
            
            if self.verbose:
                print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°å¤‡ç”¨æ–‡ä»¶: {backup_path}")
        except Exception as backup_error:
            if self.verbose:
                print(f"âŒ å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {backup_error}")
    
    def save_partial_results(self, results: Dict[str, Any], status_message: str):
        """ç«‹å³ä¿å­˜éƒ¨åˆ†ç»“æœ"""
        try:
            if self.verbose:
                print(f"ğŸ’¾ {status_message} - ç«‹å³æ›´æ–°ç»“æœ...")
            
            # æ¸…ç†æ•°æ®
            clean_results = DataCleaner.clean_dict(results)
            
            # ç«‹å³å†™å…¥CSV (è¿½åŠ /æ›´æ–°æ¨¡å¼)
            csv_path = os.path.join(self.config.get('paths.results_dir'), 
                                   self.config.get('results.csv_file'))
            
            # è¯»å–ç°æœ‰æ•°æ®
            df_existing = self._load_existing_csv(csv_path)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå®éªŒ
            experiment_id = clean_results.get('experiment_id')
            if not df_existing.empty and experiment_id:
                # æ›´æ–°ç°æœ‰è®°å½•
                mask = df_existing['experiment_id'] == experiment_id
                if mask.any():
                    for key, value in clean_results.items():
                        if key in df_existing.columns:
                            df_existing.loc[mask, key] = value
                        else:
                            df_existing[key] = None
                            df_existing.loc[mask, key] = value
                else:
                    # æ·»åŠ æ–°è®°å½•
                    new_row = pd.DataFrame([clean_results])
                    df_existing = pd.concat([df_existing, new_row], ignore_index=True)
            else:
                # æ·»åŠ æ–°è®°å½•
                new_row = pd.DataFrame([clean_results])
                df_existing = pd.concat([df_existing, new_row], ignore_index=True)
            
            # ä¿å­˜æ›´æ–°åçš„CSV
            df_existing.to_csv(csv_path, index=False)
            
            # ç«‹å³æ›´æ–°Markdownæ‘˜è¦
            self._update_markdown_summary()
            
            if self.verbose:
                print(f"âœ… ç»“æœå·²ä¿å­˜: {csv_path}")
            
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ éƒ¨åˆ†ç»“æœä¿å­˜å¤±è´¥: {e}")
            # å¤‡ç”¨ä¿å­˜æ–¹æ³•
            self._backup_partial_save(results, status_message)
    
    def _load_existing_csv(self, csv_path: str) -> pd.DataFrame:
        """å®‰å…¨åŠ è½½ç°æœ‰CSVæ–‡ä»¶"""
        if not os.path.exists(csv_path):
            return pd.DataFrame()
        
        try:
            # å®‰å…¨è¯»å–CSVï¼Œå¤„ç†å¯èƒ½çš„ç±»å‹é—®é¢˜
            df_existing = pd.read_csv(csv_path, dtype=str)  # å…ˆéƒ½è¯»æˆå­—ç¬¦ä¸²
            # ç„¶åå¤„ç†æ•°å€¼åˆ—
            numeric_cols = ['source_acc', 'target_acc', 'transferred_acc', 'source_lora_acc', 'target_lora_acc']
            for col in numeric_cols:
                if col in df_existing.columns:
                    df_existing[col] = pd.to_numeric(df_existing[col], errors='coerce')
            
            return df_existing
        except Exception as read_error:
            if self.verbose:
                print(f"âš ï¸ è¯»å–ç°æœ‰CSVå¤±è´¥: {read_error}")
            return pd.DataFrame()
    
    def _backup_partial_save(self, results: Dict[str, Any], status_message: str):
        """éƒ¨åˆ†ç»“æœçš„å¤‡ç”¨ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå¤‡ç”¨
        backup_path = os.path.join(self.config.get('paths.results_dir'), 
                                  f"partial_backup_{timestamp}.json")
        try:
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            if self.verbose:
                print(f"ğŸ“ å¤‡ç”¨ä¿å­˜: {backup_path}")
        except Exception:
            # æ–‡æœ¬å¤‡ç”¨
            text_backup = os.path.join(self.config.get('paths.results_dir'), 
                                     f"text_backup_{timestamp}.txt")
            with open(text_backup, 'w', encoding='utf-8') as f:
                f.write(f"å®éªŒç»“æœ - {status_message} - {datetime.now()}\n")
                f.write("=" * 50 + "\n")
                for key, value in results.items():
                    f.write(f"{key}: {value}\n")
            if self.verbose:
                print(f"ğŸ“ æ–‡æœ¬å¤‡ç”¨ä¿å­˜: {text_backup}")
    
    def check_existing_experiment(self, source_model: str, target_model: str, dataset: str) -> Optional[pd.Series]:
        """æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå®éªŒ"""
        csv_path = os.path.join(self.config.get('paths.results_dir'), 
                               self.config.get('results.csv_file'))
        
        if not os.path.exists(csv_path):
            return None
            
        try:
            df = pd.read_csv(csv_path)
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºæ–‡ä»¶æˆ–åªæœ‰è¡¨å¤´
            if df.empty or len(df) == 0:
                return None
                
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹è¿›è¡Œæ¯”è¾ƒ
            df['source_model'] = df['source_model'].astype(str)
            df['target_model'] = df['target_model'].astype(str)
            df['dataset'] = df['dataset'].astype(str)
            
            existing = df[
                (df['source_model'] == str(source_model)) & 
                (df['target_model'] == str(target_model)) & 
                (df['dataset'] == str(dataset))
            ]
            if not existing.empty:
                return existing.iloc[-1]  # Return the latest record
        except Exception as e:
            if self.verbose:
                print(f"WARNING: Error reading history: {e}")
                print(f"NOTE: Will recreate results file")
        
        return None
    
    def _update_markdown_summary(self):
        """æ›´æ–°Markdownæ€»ç»“æ–‡ä»¶"""
        csv_path = os.path.join(self.config.get('paths.results_dir'), 
                               self.config.get('results.csv_file'))
        md_path = os.path.join(self.config.get('paths.results_dir'), 
                              self.config.get('results.markdown_file'))
        
        if not os.path.exists(csv_path):
            return
        
        try:
            df = pd.read_csv(csv_path)
            if df.empty:
                return
            
            content = self._generate_markdown_content(df)
            
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(content)
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ ç”ŸæˆMarkdownå¤±è´¥: {e}")
    
    def _generate_markdown_content(self, df: pd.DataFrame) -> str:
        """Generate Markdown content"""
        content = f"""# LoRA Transfer Experiment Results Summary

> Auto-generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
> Management script: transfer_pipeline.py  
> Total experiments: {len(df)}

## Experiment Overview

This document records all LoRA training and transfer experiment results, including:
- Base model performance
- LoRAå¾®è°ƒåæ€§èƒ½  
- è·¨æ¨¡å‹LoRAè¿ç§»æ€§èƒ½
- è¯¦ç»†çš„é…ç½®ä¿¡æ¯

---

## æœ€æ–°å®éªŒç»“æœ

"""
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„æ˜¾ç¤ºç»“æœ
        for dataset in df['dataset'].unique():
            dataset_df = df[df['dataset'] == dataset]
            content += f"\n### æ•°æ®é›†: {dataset}\n\n"
            
            for _, row in dataset_df.iterrows():
                content += f"#### å®éªŒ: {row['source_model']} â†’ {row['target_model']}\n\n"
                content += "| æ¨¡å‹é…ç½® | å‡†ç¡®ç‡ | æå‡ | å¤‡æ³¨ |\n"
                content += "|---------|--------|------|------|\n"
                
                # åŸºç¡€æ¨¡å‹è¡Œ
                source_acc = row.get('source_acc', 0)
                target_acc = row.get('target_acc', 0)
                
                content += f"| {ModelUtils.get_model_short_name(row['source_model'])} (source) | {source_acc:.4f} | - | åŸºç¡€æ¨¡å‹ |\n"
                
                if pd.notna(row.get('source_lora_acc')):
                    improvement = (row['source_lora_acc'] - source_acc) * 100
                    content += f"| {ModelUtils.get_model_short_name(row['source_model'])} + LoRA | {row['source_lora_acc']:.4f} | +{improvement:.2f}% | æºæ¨¡å‹å¾®è°ƒ |\n"
                
                content += f"| {ModelUtils.get_model_short_name(row['target_model'])} (target) | {target_acc:.4f} | - | åŸºç¡€æ¨¡å‹ |\n"
                
                if pd.notna(row.get('target_lora_acc')):
                    improvement = (row['target_lora_acc'] - target_acc) * 100
                    content += f"| {ModelUtils.get_model_short_name(row['target_model'])} + LoRA | {row['target_lora_acc']:.4f} | +{improvement:.2f}% | ç›®æ ‡æ¨¡å‹å¾®è°ƒ |\n"
                
                if pd.notna(row.get('transferred_acc')):
                    improvement = (row['transferred_acc'] - target_acc) * 100
                    content += f"| {ModelUtils.get_model_short_name(row['target_model'])} + è¿ç§»LoRA | {row['transferred_acc']:.4f} | +{improvement:.2f}% | è¿ç§»LoRA |\n"
                
                content += f"\n**å®éªŒæ—¶é—´:** {row['timestamp']}  \n"
                content += f"**é…ç½®:** {row['training_config']}\n\n"
                content += "---\n\n"
        
        content += f"""
## ä½¿ç”¨è¯´æ˜

### è¿è¡Œæ–°å®éªŒ
```bash
python transfer_pipeline.py \\
  --source_model Llama-3.2-3B-Instruct \\
  --target_model Qwen_Qwen2.5-1.5B \\
  --dataset arc-challenge
```

### æŸ¥çœ‹è¯¦ç»†æ•°æ®
```bash
# æŸ¥çœ‹CSVæ ¼å¼æ•°æ® (å¯ç”¨Excelæ‰“å¼€)
cat results/experiment_results.csv

# åªè¿è¡Œè¯„ä¼° (è·³è¿‡è®­ç»ƒ)
python transfer_pipeline.py --source_model ... --target_model ... --dataset ... --eval_only
```

---

*æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return content
