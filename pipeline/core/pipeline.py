"""
LoRAè®­ç»ƒå’Œè¿ç§»ä¸»ç®¡é“
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„ç®¡é“æŽ¥å£
"""

import os
from typing import Dict, Any, Optional
from tqdm import tqdm

from .config import PipelineConfig, QuickTestConfig
from .trainer import ModelTrainer
from .evaluator import ModelEvaluator
from .transfer import LoRATransfer
from .results import ResultsManager
from .utils import ModelUtils, get_timestamp


class TransferPipeline:
    """LoRAè¿ç§»ç®¡é“ä¸»ç±»"""
    
    def __init__(self, config_path: str = None, quick_test: bool = False):
        """åˆå§‹åŒ–ç®¡é“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            quick_test: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½®
        """
        if quick_test:
            self.config = QuickTestConfig()
        else:
            self.config = PipelineConfig(config_path)
        
        self.timestamp = get_timestamp()
        self.experiment_id = None
        
        # åˆå§‹åŒ–æ¨¡å—
        self.trainer = ModelTrainer(self.config)
        self.evaluator = ModelEvaluator(self.config)
        self.transfer = LoRATransfer(self.config)
        self.results = ResultsManager(self.config)
    
    def run_pipeline(self, source_model: str, target_model: str, dataset: str, 
                    eval_only: bool = False) -> bool:
        """è¿è¡Œå®Œæ•´ç®¡é“ - æ–°æµç¨‹ï¼šè¯„ä¼°æºåŸºç¡€ â†’ è®­ç»ƒæºLoRA â†’ è¿ç§» â†’ è¯„ä¼°ç›®æ ‡åŸºç¡€ â†’ è¯„ä¼°è¿ç§»LoRA â†’ è®­ç»ƒç›®æ ‡LoRA
        
        Args:
            source_model: æºæ¨¡åž‹è·¯å¾„
            target_model: ç›®æ ‡æ¨¡åž‹è·¯å¾„
            dataset: æ•°æ®é›†åç§°
            eval_only: ä»…è¿è¡Œè¯„ä¼°ï¼Œè·³è¿‡è®­ç»ƒå’Œè¿ç§»
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # éªŒè¯è¾“å…¥
        if not self._validate_inputs(source_model, target_model, dataset):
            return False
        
        # åˆ›å»ºå®žéªŒID
        self.experiment_id = ModelUtils.create_experiment_id(
            source_model, target_model, dataset, self.timestamp
        )
        
        print("\n" + "="*60)
        print("LoRA Transfer Pipeline - Experiment Started")
        print("="*60)
        print(f"Experiment ID: {self.experiment_id}")
        print(f"Source Model:  {ModelUtils.get_model_short_name(source_model)}")
        print(f"Target Model:  {ModelUtils.get_model_short_name(target_model)}")
        print(f"Dataset:       {dataset}")
        
        # Display configuration info
        max_steps = self.config.get('training.default_max_steps', 600)
        sample_ratio = self.config.get('evaluation.sample_ratio', 1.0)
        batch_size = self.config.get('training.default_batch_size', 4)
        
        print(f"Training:      {max_steps} steps, batch_size={batch_size}")
        print(f"Evaluation:    {sample_ratio*100:.0f}% sample ratio")
        
        # Check if this is quick test mode
        if hasattr(self.config, 'is_quick_test') and self.config.is_quick_test:
            print(f"Mode:          Quick Test (Fast)")
        else:
            print(f"Mode:          Full Pipeline")
        
        print("="*60)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åŽ†å²è®°å½•
        if not eval_only:
            existing = self.results.check_existing_experiment(source_model, target_model, dataset)
            if existing is not None:
                action = self._handle_existing_experiment(existing, source_model, target_model, dataset)
                if action == 'abort':
                    print("Experiment cancelled.")
                    return False
                elif action == 'delete':
                    print("ðŸ—‘ï¸ Cleaning up existing experiment outputs...")
                    self._cleanup_experiment_outputs(source_model, target_model, dataset)
                    print("âœ… Cleanup completed. Starting fresh experiment.")
                elif action == 'continue':
                    print("â© Continuing with existing experiment outputs.")
        
        # åˆå§‹åŒ–ç»“æžœå­—å…¸
        results = self._init_results_dict(source_model, target_model, dataset)
        
        # æ‰§è¡Œç®¡é“æ­¥éª¤
        return self._execute_pipeline_steps(results, source_model, target_model, dataset, eval_only)
    
    def _validate_inputs(self, source_model: str, target_model: str, dataset: str) -> bool:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        # å¤„ç†æ¨¡åž‹è·¯å¾„
        if not source_model.startswith('/'):
            source_model = self.config.get_model_path(source_model)
        if not target_model.startswith('/'):
            target_model = self.config.get_model_path(target_model)
        
        # éªŒè¯æ¨¡åž‹å­˜åœ¨
        if not ModelUtils.check_model_exists(source_model):
            print(f"âŒ æºæ¨¡åž‹ä¸å­˜åœ¨: {source_model}")
            return False
        if not ModelUtils.check_model_exists(target_model):
            print(f"âŒ ç›®æ ‡æ¨¡åž‹ä¸å­˜åœ¨: {target_model}")
            return False
        
        # éªŒè¯æ•°æ®é›†
        supported_datasets = self.config.get('training.datasets', [])
        if dataset not in supported_datasets:
            print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset}")
            print(f"âœ… æ”¯æŒçš„æ•°æ®é›†: {', '.join(supported_datasets)}")
            return False
        
        return True
    
    def _handle_existing_experiment(self, existing: Dict, source_model: str, target_model: str, dataset: str) -> str:
        """å¤„ç†å·²å­˜åœ¨çš„å®žéªŒï¼Œè¿”å›žç”¨æˆ·é€‰æ‹©çš„æ“ä½œ"""
        import os
        import shutil
        import glob
        from pathlib import Path
        
        print(f"\nWarning: Found existing experiment (timestamp: {existing['timestamp']})")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤è¡Œä¸º
        default_action = self.config.get('experiment_management.existing_experiment_action', 'prompt')
        
        if default_action == 'prompt':
            print("\nWhat would you like to do?")
            print("  [C]ontinue - Keep existing outputs and continue")
            print("  [D]elete   - Delete all existing outputs and start fresh")
            print("  [Y]es      - Same as Continue (for backward compatibility)")
            print("  [N]o       - Same as Abort (for backward compatibility)")
            
            while True:
                response = input("Choose an option (C/D/Y/N): ").strip().lower()
                if response in ['c', 'continue', 'y', 'yes']:
                    return 'continue'
                elif response in ['d', 'delete']:
                    return 'delete'
                elif response in ['n', 'no', '']:
                    return 'abort'
                else:
                    print("Invalid option. Please choose C, D, Y, or N.")
        elif default_action == 'continue':
            print("â© Auto-continuing (configured in YAML)")
            return 'continue'
        elif default_action == 'delete':
            print("ðŸ—‘ï¸ Auto-deleting (configured in YAML)")
            return 'delete'
        elif default_action == 'abort':
            print("âŒ Auto-aborting (configured in YAML)")
            return 'abort'
        else:
            print(f"âš ï¸ Unknown default action: {default_action}, prompting user")
            return self._handle_existing_experiment(existing, source_model, target_model, dataset)
    
    def _cleanup_experiment_outputs(self, source_model: str, target_model: str, dataset: str):
        """æ¸…ç†å®žéªŒè¾“å‡ºæ–‡ä»¶"""
        import os
        import shutil
        import glob
        from pathlib import Path
        
        cleanup_targets = self.config.get('experiment_management.cleanup_targets', [])
        preserve_patterns = self.config.get('experiment_management.preserve_patterns', [])
        
        # æå–æ¨¡åž‹åç§°ï¼ˆåŽ»é™¤è·¯å¾„ï¼‰
        source_name = Path(source_model).name
        target_name = Path(target_model).name
        
        cleaned_count = 0
        
        for target in cleanup_targets:
            try:
                if target == 'training_outputs':
                    # æ¸…ç†è®­ç»ƒç»“æžœ
                    pattern_paths = [
                        f"./train_lora/runs/{dataset}/{source_name}/*",
                        f"./train_lora/runs/{dataset}/{target_name}/*"
                    ]
                    for pattern in pattern_paths:
                        for path in glob.glob(pattern):
                            if self._should_preserve(path, preserve_patterns):
                                continue
                            if os.path.isdir(path):
                                shutil.rmtree(path)
                                print(f"  ðŸ—‘ï¸ Removed training output: {path}")
                                cleaned_count += 1
                
                elif target == 'transferred_lora':
                    # æ¸…ç†è¿ç§»çš„LoRA
                    pattern = f"../autodl-tmp/transferred_lora/{dataset}/{source_name}_to_{target_name}/*"
                    for path in glob.glob(pattern):
                        if self._should_preserve(path, preserve_patterns):
                            continue
                        if os.path.isdir(path):
                            shutil.rmtree(path)
                            print(f"  ðŸ—‘ï¸ Removed transferred LoRA: {path}")
                            cleaned_count += 1
                
                elif target == 'evaluation_results':
                    # æ¸…ç†è¯„ä¼°ç»“æžœ
                    pattern_paths = [
                        f"./eval/results/*{source_name}*",
                        f"./eval/results/*{target_name}*",
                        f"./eval/results/*{dataset}*"
                    ]
                    for pattern in pattern_paths:
                        for path in glob.glob(pattern):
                            if self._should_preserve(path, preserve_patterns):
                                continue
                            if os.path.isfile(path):
                                os.remove(path)
                                print(f"  ðŸ—‘ï¸ Removed evaluation result: {path}")
                                cleaned_count += 1
                
                elif target == 'pipeline_results':
                    # æ¸…ç†Pipelineç»“æžœï¼Œä½†ä¿ç•™æ€»ä½“ç»“æžœæ–‡ä»¶çš„ç»“æž„
                    results_dir = Path("./results")
                    if results_dir.exists():
                        # åˆ é™¤ç‰¹å®šå®žéªŒçš„å¤‡ä»½æ–‡ä»¶
                        backup_pattern = f"backup_*.json"
                        for backup_file in results_dir.glob(backup_pattern):
                            if self._should_preserve(str(backup_file), preserve_patterns):
                                continue
                            backup_file.unlink()
                            print(f"  ðŸ—‘ï¸ Removed backup: {backup_file}")
                            cleaned_count += 1
                        
                        # æ¸…ç†CSVä¸­çš„ç›¸å…³æ¡ç›®ï¼ˆè¿™ä¸ªæ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶è·³è¿‡è‡ªåŠ¨æ¸…ç†ï¼‰
                        print(f"  â„¹ï¸ Note: CSV entries for this experiment may still exist in experiment_results.csv")
                        
            except Exception as e:
                print(f"  âš ï¸ Error cleaning {target}: {e}")
        
        print(f"  âœ… Cleaned {cleaned_count} items")
    
    def _should_preserve(self, path: str, preserve_patterns: list) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«ä¿ç•™"""
        import fnmatch
        path_name = os.path.basename(path)
        for pattern in preserve_patterns:
            if fnmatch.fnmatch(path_name, pattern) or fnmatch.fnmatch(path, pattern):
                return True
        return False
    
    def _init_results_dict(self, source_model: str, target_model: str, dataset: str) -> Dict[str, Any]:
        """åˆå§‹åŒ–ç»“æžœå­—å…¸"""
        return {
            'experiment_id': self.experiment_id,
            'source_model': source_model,
            'target_model': target_model,
            'dataset': dataset,
            'timestamp': self.timestamp,
            'training_config': f"batch_size={self.config.get('training.default_batch_size')}, "
                              f"max_steps={self.config.get('training.default_max_steps')}, "
                              f"lr={self.config.get('training.default_lr')}",
            'notes': 'è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆ'
        }
    
    def _execute_pipeline_steps(self, results: Dict[str, Any], source_model: str, 
                               target_model: str, dataset: str, eval_only: bool) -> bool:
        """æ‰§è¡Œç®¡é“æ­¥éª¤"""
        # æ€»æ˜¯æ˜¾ç¤ºå®Œæ•´çš„6æ­¥è¿›åº¦æ¡
        progress_bar = tqdm(total=6, desc="Pipeline Progress", position=1, leave=True, ncols=80)
        
        try:
            # æ­¥éª¤1: è¯„ä¼°æºåŸºç¡€æ¨¡åž‹
            if not self._step_eval_source_base(results, source_model, dataset, progress_bar):
                print("âš ï¸ æºåŸºç¡€æ¨¡åž‹è¯„ä¼°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # æ­¥éª¤2: è®­ç»ƒæºLoRA
            if not eval_only:
                if not self._step_train_source_lora(results, source_model, dataset, progress_bar):
                    raise Exception("æºæ¨¡åž‹è®­ç»ƒå¤±è´¥")
            else:
                self._step_skip_with_reason("STEP 2/6: TRAIN SOURCE LORA", "ä»…è¯„ä¼°æ¨¡å¼ï¼Œè·³è¿‡è®­ç»ƒ", progress_bar)
            
            # æ­¥éª¤3: è¿ç§»LoRA
            if not eval_only:
                if not self._step_transfer_lora(results, source_model, target_model, dataset, progress_bar):
                    raise Exception("LoRAè¿ç§»å¤±è´¥")
            else:
                self._step_skip_with_reason("STEP 3/6: TRANSFER LORA", "ä»…è¯„ä¼°æ¨¡å¼ï¼Œè·³è¿‡è¿ç§»", progress_bar)
            
            # æ­¥éª¤4: è¯„ä¼°ç›®æ ‡åŸºç¡€æ¨¡åž‹
            if not self._step_eval_target_base(results, target_model, dataset, progress_bar):
                print("âš ï¸ ç›®æ ‡åŸºç¡€æ¨¡åž‹è¯„ä¼°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # æ­¥éª¤5: è¯„ä¼°è¿ç§»LoRA
            if not eval_only:
                if not self._step_eval_transferred_lora(results, target_model, dataset, progress_bar):
                    print("âš ï¸ è¿ç§»LoRAè¯„ä¼°å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            else:
                self._step_skip_with_reason("STEP 5/6: EVAL TRANSFERRED LORA", "ä»…è¯„ä¼°æ¨¡å¼ï¼Œæ— è¿ç§»LoRAå¯è¯„ä¼°", progress_bar)
            
            # æ­¥éª¤6: è®­ç»ƒç›®æ ‡LoRA
            if not eval_only:
                if not self._step_train_target_lora(results, target_model, dataset, progress_bar):
                    print("âš ï¸ ç›®æ ‡æ¨¡åž‹è®­ç»ƒå¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            else:
                self._step_skip_with_reason("STEP 6/6: TRAIN TARGET LORA", "ä»…è¯„ä¼°æ¨¡å¼ï¼Œè·³è¿‡è®­ç»ƒ", progress_bar)
            
            # æœ€ç»ˆä¿å­˜å®Œæ•´ç»“æžœ
            progress_bar.set_description("Saving Results")
            self.results.save_results(results)
            progress_bar.close()
            
            # æ‰“å°æ€»ç»“
            self._print_summary(results)
            
            # æç¤ºå¯é€‰å‘½ä»¤
            if not eval_only:
                self._print_optional_commands(source_model, target_model, dataset)
            
            print("\nPipeline completed successfully!")
            return True
            
        except Exception as e:
            progress_bar.close()
            print(f"\nPipeline failed: {e}")
            # ä¿å­˜éƒ¨åˆ†ç»“æžœ
            self.results.save_partial_results(results, f"å¤±è´¥: {e}")
            return False
    
    def _step_skip_with_reason(self, step_title: str, reason: str, progress_bar: tqdm):
        """æ˜¾ç¤ºè·³è¿‡çš„æ­¥éª¤åŠåŽŸå› """
        print(f"\n{'='*60}")
        print(step_title)
        print("="*60)
        print(f"ðŸš« è·³è¿‡åŽŸå› : {reason}")
        print("="*60)
        progress_bar.update(1)
    
    def _step_train_source_lora(self, results: Dict[str, Any], source_model: str, 
                               dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤2: è®­ç»ƒæºLoRA"""
        print(f"\n{'='*60}")
        print("STEP 2/6: TRAIN SOURCE LORA")
        print("="*60)
        
        source_lora_path, source_lora_acc, status_msg = self.trainer.train_model(source_model, dataset)
        print(f"çŠ¶æ€: {status_msg}")
        print(f"ðŸ” DEBUG: è®­ç»ƒå™¨è¿”å›žçš„å‡†ç¡®çŽ‡: {source_lora_acc}")
        if source_lora_path is None:
            return False
        
        results.update({
            'source_lora_path': source_lora_path,
            'source_lora_acc': source_lora_acc,
        })
        self.results.save_partial_results(results, "æºLoRAè®­ç»ƒå®Œæˆ")
        progress_bar.update(1)
        return True
    
    def _step_transfer_lora(self, results: Dict[str, Any], source_model: str,
                           target_model: str, dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤3: è¿ç§»LoRA"""
        print(f"\n{'='*60}")
        print("STEP 3/6: TRANSFER LORA")
        print("="*60)
        
        transferred_lora_path = self.transfer.transfer_lora(
            results['source_lora_path'], source_model, target_model, dataset
        )
        if transferred_lora_path is None:
            return False
        
        results['transferred_lora_path'] = transferred_lora_path
        self.results.save_partial_results(results, "LoRAè¿ç§»å®Œæˆ")
        progress_bar.update(1)
        return True
    
    def _step_eval_target_base(self, results: Dict[str, Any], target_model: str, 
                              dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤4: è¯„ä¼°ç›®æ ‡åŸºç¡€æ¨¡åž‹"""
        print(f"\n{'='*60}")
        print("STEP 4/6: EVAL TARGET BASE MODEL")
        print("="*60)
        
        target_acc = self.evaluator.evaluate_base_model(target_model, dataset)
        results['target_acc'] = target_acc
        self.results.save_partial_results(results, "ç›®æ ‡åŸºç¡€æ¨¡åž‹è¯„ä¼°å®Œæˆ")
        progress_bar.update(1)
        return target_acc is not None
    
    def _step_eval_transferred_lora(self, results: Dict[str, Any], target_model: str, 
                                   dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤5: è¯„ä¼°è¿ç§»LoRA"""
        print(f"\n{'='*60}")
        print("STEP 5/6: EVAL TRANSFERRED LORA")
        print("="*60)
        
        transferred_acc = self.evaluator.evaluate_lora_model(
            results['transferred_lora_path'], target_model, dataset
        )
        results['transferred_acc'] = transferred_acc
        self.results.save_partial_results(results, "è¿ç§»LoRAè¯„ä¼°å®Œæˆ")
        progress_bar.update(1)
        return transferred_acc is not None
    
    def _step_train_target_lora(self, results: Dict[str, Any], target_model: str, 
                               dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤6: è®­ç»ƒç›®æ ‡LoRA"""
        print(f"\n{'='*60}")
        print("STEP 6/6: TRAIN TARGET LORA")
        print("="*60)
        
        target_lora_path, target_lora_acc, status_msg = self.trainer.train_model(target_model, dataset)
        print(f"çŠ¶æ€: {status_msg}")
        if target_lora_path is None:
            target_lora_acc = None
        
        results.update({
            'target_lora_path': target_lora_path,
            'target_lora_acc': target_lora_acc,
        })
        self.results.save_partial_results(results, "ç›®æ ‡LoRAè®­ç»ƒå®Œæˆ")
        progress_bar.update(1)
        return True  # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­
    
    def _step_eval_source_base(self, results: Dict[str, Any], source_model: str, 
                              dataset: str, progress_bar: tqdm) -> bool:
        """æ­¥éª¤1: è¯„ä¼°æºåŸºç¡€æ¨¡åž‹"""
        print(f"\n{'='*60}")
        print("STEP 1/6: EVAL SOURCE BASE MODEL")
        print("="*60)
        
        source_acc = self.evaluator.evaluate_base_model(source_model, dataset)
        results['source_acc'] = source_acc
        self.results.save_partial_results(results, "æºåŸºç¡€æ¨¡åž‹è¯„ä¼°å®Œæˆ")
        progress_bar.update(1)
        return source_acc is not None
    
    def _print_summary(self, results: Dict[str, Any]):
        """Print experiment summary"""
        print(f"\n{'='*60}")
        print("EXPERIMENT SUMMARY")
        print("=" * 60)
        
        source_name = ModelUtils.get_model_short_name(results['source_model'])
        target_name = ModelUtils.get_model_short_name(results['target_model'])
        
        # Handle potentially None values
        source_acc = results.get('source_acc', 0) or 0
        target_acc = results.get('target_acc', 0) or 0
        source_lora_acc = results.get('source_lora_acc')
        target_lora_acc = results.get('target_lora_acc')
        transferred_acc = results.get('transferred_acc')
        
        print(f"Source Model ({source_name}):     {source_acc:.4f}")
        if source_lora_acc is not None:
            improvement = (source_lora_acc - source_acc) * 100
            sign = "+" if improvement >= 0 else ""
            print(f"Source + LoRA:              {source_lora_acc:.4f} ({sign}{improvement:.2f}%)")
        
        print(f"Target Model ({target_name}):     {target_acc:.4f}")
        
        if transferred_acc is not None:
            improvement = (transferred_acc - target_acc) * 100
            sign = "+" if improvement >= 0 else ""
            print(f"Target + Transferred LoRA:  {transferred_acc:.4f} ({sign}{improvement:.2f}%)")
        
        if target_lora_acc is not None:
            improvement = (target_lora_acc - target_acc) * 100
            sign = "+" if improvement >= 0 else ""
            print(f"Target + Direct LoRA:       {target_lora_acc:.4f} ({sign}{improvement:.2f}%)")
        
        print("=" * 60)
        print(f"Detailed results: results/experiment_summary.md")
    
    def _print_optional_commands(self, source_model: str, target_model: str, dataset: str):
        """Print optional target model LoRA training commands"""
        target_name = ModelUtils.get_model_short_name(target_model)
        
        print(f"\n{'-'*60}")
        print(f"OPTIONAL: Train {target_name} LoRA for Comparison")
        print("-" * 60)
        
        # Training command
        train_cmd = f"python {self.config.get('paths.train_script')} " \
                   f"--dataset {dataset} " \
                   f"--base_model {target_model} " \
                   f"--bs {self.config.get('training.default_batch_size')} " \
                   f"--max_steps {self.config.get('training.default_max_steps')}"
        
        print(f"Train {target_name} LoRA:")
        print(f"  {train_cmd}")
        
        # Evaluation command 
        eval_cmd = f"python {self.config.get('paths.eval_script')} " \
                  f"--models_list [trained_model_path] " \
                  f"--dataset {dataset} " \
                  f"--sample_ratio {self.config.get('evaluation.sample_ratio')} " \
                  f"--base_model {target_model}"
        
        print(f"\nEvaluate {target_name} LoRA:")
        print(f"  {eval_cmd}")
