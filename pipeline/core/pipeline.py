"""
LoRAËÆ≠ÁªÉÂíåËøÅÁßª‰∏ªÁÆ°ÈÅì
Êï¥ÂêàÊâÄÊúâÊ®°ÂùóÔºåÊèê‰æõÁªü‰∏ÄÁöÑÁÆ°ÈÅìÊé•Âè£
"""

import os
from typing import Dict, Any, Optional
from tqdm import tqdm

from .config import PipelineConfig, QuickTestConfig
from .trainer import ModelTrainer
from .evaluator import ModelEvaluator
from .transfer import LoRATransfer
from .results import ResultsManager
from .utils import ModelUtils, get_timestamp


class TransferPipeline:
    """LoRAËøÅÁßªÁÆ°ÈÅì‰∏ªÁ±ª"""
    
    def __init__(self, config_path: str = None, quick_test: bool = False):
        """ÂàùÂßãÂåñÁÆ°ÈÅì
        
        Args:
            config_path: ÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ
            quick_test: ÊòØÂê¶‰ΩøÁî®Âø´ÈÄüÊµãËØïÈÖçÁΩÆ
        """
        if quick_test:
            self.config = QuickTestConfig()
        else:
            self.config = PipelineConfig(config_path)
        
        self.timestamp = get_timestamp()
        self.experiment_id = None
        
        # ÂàùÂßãÂåñÊ®°Âùó
        self.trainer = ModelTrainer(self.config)
        self.evaluator = ModelEvaluator(self.config)
        self.transfer = LoRATransfer(self.config)
        self.results = ResultsManager(self.config)
    
    def run_pipeline(self, source_model: str, target_model: str, dataset: str, 
                    eval_only: bool = False) -> bool:
        """ËøêË°åÂÆåÊï¥ÁÆ°ÈÅì - Êñ∞ÊµÅÁ®ãÔºöËÆ≠ÁªÉÊ∫êLoRA ‚Üí ËøÅÁßª ‚Üí ËØÑ‰º∞ÁõÆÊ†áÂü∫Á°Ä ‚Üí ËØÑ‰º∞ËøÅÁßªLoRA ‚Üí ËÆ≠ÁªÉÁõÆÊ†áLoRA ‚Üí ËØÑ‰º∞Ê∫êÂü∫Á°Ä
        
        Args:
            source_model: Ê∫êÊ®°ÂûãË∑ØÂæÑ
            target_model: ÁõÆÊ†áÊ®°ÂûãË∑ØÂæÑ
            dataset: Êï∞ÊçÆÈõÜÂêçÁß∞
            eval_only: ‰ªÖËøêË°åËØÑ‰º∞ÔºåË∑≥ËøáËÆ≠ÁªÉÂíåËøÅÁßª
            
        Returns:
            ÊòØÂê¶ÊàêÂäü
        """
        # È™åËØÅËæìÂÖ•
        if not self._validate_inputs(source_model, target_model, dataset):
            return False
        
        # ÂàõÂª∫ÂÆûÈ™åID
        self.experiment_id = ModelUtils.create_experiment_id(
            source_model, target_model, dataset, self.timestamp
        )
        
        print("\n" + "="*60)
        print("LoRA Transfer Pipeline - Experiment Started")
        print("="*60)
        print(f"Experiment ID: {self.experiment_id}")
        print(f"Source Model:  {ModelUtils.get_model_short_name(source_model)}")
        print(f"Target Model:  {ModelUtils.get_model_short_name(target_model)}")
        print(f"Dataset:       {dataset}")
        print("="*60)
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÂéÜÂè≤ËÆ∞ÂΩï
        if not eval_only:
            existing = self.results.check_existing_experiment(source_model, target_model, dataset)
            if existing is not None:
                action = self._handle_existing_experiment(existing, source_model, target_model, dataset)
                if action == 'abort':
                    print("Experiment cancelled.")
                    return False
                elif action == 'delete':
                    print("üóëÔ∏è Cleaning up existing experiment outputs...")
                    self._cleanup_experiment_outputs(source_model, target_model, dataset)
                    print("‚úÖ Cleanup completed. Starting fresh experiment.")
                elif action == 'continue':
                    print("‚è© Continuing with existing experiment outputs.")
        
        # ÂàùÂßãÂåñÁªìÊûúÂ≠óÂÖ∏
        results = self._init_results_dict(source_model, target_model, dataset)
        
        # ÊâßË°åÁÆ°ÈÅìÊ≠•È™§
        return self._execute_pipeline_steps(results, source_model, target_model, dataset, eval_only)
    
    def _validate_inputs(self, source_model: str, target_model: str, dataset: str) -> bool:
        """È™åËØÅËæìÂÖ•ÂèÇÊï∞"""
        # Â§ÑÁêÜÊ®°ÂûãË∑ØÂæÑ
        if not source_model.startswith('/'):
            source_model = self.config.get_model_path(source_model)
        if not target_model.startswith('/'):
            target_model = self.config.get_model_path(target_model)
        
        # È™åËØÅÊ®°ÂûãÂ≠òÂú®
        if not ModelUtils.check_model_exists(source_model):
            print(f"‚ùå Ê∫êÊ®°Âûã‰∏çÂ≠òÂú®: {source_model}")
            return False
        if not ModelUtils.check_model_exists(target_model):
            print(f"‚ùå ÁõÆÊ†áÊ®°Âûã‰∏çÂ≠òÂú®: {target_model}")
            return False
        
        # È™åËØÅÊï∞ÊçÆÈõÜ
        supported_datasets = self.config.get('training.datasets', [])
        if dataset not in supported_datasets:
            print(f"‚ùå ‰∏çÊîØÊåÅÁöÑÊï∞ÊçÆÈõÜ: {dataset}")
            print(f"‚úÖ ÊîØÊåÅÁöÑÊï∞ÊçÆÈõÜ: {', '.join(supported_datasets)}")
            return False
        
        return True
    
    def _handle_existing_experiment(self, existing: Dict, source_model: str, target_model: str, dataset: str) -> str:
        """Â§ÑÁêÜÂ∑≤Â≠òÂú®ÁöÑÂÆûÈ™åÔºåËøîÂõûÁî®Êà∑ÈÄâÊã©ÁöÑÊìç‰Ωú"""
        import os
        import shutil
        import glob
        from pathlib import Path
        
        print(f"\nWarning: Found existing experiment (timestamp: {existing['timestamp']})")
        
        # Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑÈªòËÆ§Ë°å‰∏∫
        default_action = self.config.get('experiment_management.existing_experiment_action', 'prompt')
        
        if default_action == 'prompt':
            print("\nWhat would you like to do?")
            print("  [C]ontinue - Keep existing outputs and continue")
            print("  [D]elete   - Delete all existing outputs and start fresh")
            print("  [A]bort    - Cancel this experiment")
            print("  [Y]es      - Same as Continue (for backward compatibility)")
            print("  [N]o       - Same as Abort (for backward compatibility)")
            
            while True:
                response = input("Choose an option (C/D/A/Y/N): ").strip().lower()
                if response in ['c', 'continue', 'y', 'yes']:
                    return 'continue'
                elif response in ['d', 'delete']:
                    return 'delete'
                elif response in ['a', 'abort', 'n', 'no', '']:
                    return 'abort'
                else:
                    print("Invalid option. Please choose C, D, A, Y, or N.")
        elif default_action == 'continue':
            print("‚è© Auto-continuing (configured in YAML)")
            return 'continue'
        elif default_action == 'delete':
            print("üóëÔ∏è Auto-deleting (configured in YAML)")
            return 'delete'
        elif default_action == 'abort':
            print("‚ùå Auto-aborting (configured in YAML)")
            return 'abort'
        else:
            print(f"‚ö†Ô∏è Unknown default action: {default_action}, prompting user")
            return self._handle_existing_experiment(existing, source_model, target_model, dataset)
    
    def _cleanup_experiment_outputs(self, source_model: str, target_model: str, dataset: str):
        """Ê∏ÖÁêÜÂÆûÈ™åËæìÂá∫Êñá‰ª∂"""
        import os
        import shutil
        import glob
        from pathlib import Path
        
        cleanup_targets = self.config.get('experiment_management.cleanup_targets', [])
        preserve_patterns = self.config.get('experiment_management.preserve_patterns', [])
        
        # ÊèêÂèñÊ®°ÂûãÂêçÁß∞ÔºàÂéªÈô§Ë∑ØÂæÑÔºâ
        source_name = Path(source_model).name
        target_name = Path(target_model).name
        
        cleaned_count = 0
        
        for target in cleanup_targets:
            try:
                if target == 'training_outputs':
                    # Ê∏ÖÁêÜËÆ≠ÁªÉÁªìÊûú
                    pattern_paths = [
                        f"/root/PAW/train_lora/runs/{dataset}/{source_name}/*",
                        f"/root/PAW/train_lora/runs/{dataset}/{target_name}/*"
                    ]
                    for pattern in pattern_paths:
                        for path in glob.glob(pattern):
                            if self._should_preserve(path, preserve_patterns):
                                continue
                            if os.path.isdir(path):
                                shutil.rmtree(path)
                                print(f"  üóëÔ∏è Removed training output: {path}")
                                cleaned_count += 1
                
                elif target == 'transferred_lora':
                    # Ê∏ÖÁêÜËøÅÁßªÁöÑLoRA
                    pattern = f"/root/autodl-tmp/transferred_lora/{dataset}/{source_name}_to_{target_name}/*"
                    for path in glob.glob(pattern):
                        if self._should_preserve(path, preserve_patterns):
                            continue
                        if os.path.isdir(path):
                            shutil.rmtree(path)
                            print(f"  üóëÔ∏è Removed transferred LoRA: {path}")
                            cleaned_count += 1
                
                elif target == 'evaluation_results':
                    # Ê∏ÖÁêÜËØÑ‰º∞ÁªìÊûú
                    pattern_paths = [
                        f"/root/PAW/eval/results/*{source_name}*",
                        f"/root/PAW/eval/results/*{target_name}*",
                        f"/root/PAW/eval/results/*{dataset}*"
                    ]
                    for pattern in pattern_paths:
                        for path in glob.glob(pattern):
                            if self._should_preserve(path, preserve_patterns):
                                continue
                            if os.path.isfile(path):
                                os.remove(path)
                                print(f"  üóëÔ∏è Removed evaluation result: {path}")
                                cleaned_count += 1
                
                elif target == 'pipeline_results':
                    # Ê∏ÖÁêÜPipelineÁªìÊûúÔºå‰ΩÜ‰øùÁïôÊÄª‰ΩìÁªìÊûúÊñá‰ª∂ÁöÑÁªìÊûÑ
                    results_dir = Path("/root/PAW/results")
                    if results_dir.exists():
                        # Âà†Èô§ÁâπÂÆöÂÆûÈ™åÁöÑÂ§á‰ªΩÊñá‰ª∂
                        backup_pattern = f"backup_*.json"
                        for backup_file in results_dir.glob(backup_pattern):
                            if self._should_preserve(str(backup_file), preserve_patterns):
                                continue
                            backup_file.unlink()
                            print(f"  üóëÔ∏è Removed backup: {backup_file}")
                            cleaned_count += 1
                        
                        # Ê∏ÖÁêÜCSV‰∏≠ÁöÑÁõ∏ÂÖ≥Êù°ÁõÆÔºàËøô‰∏™ÊØîËæÉÂ§çÊùÇÔºåÊöÇÊó∂Ë∑≥ËøáËá™Âä®Ê∏ÖÁêÜÔºâ
                        print(f"  ‚ÑπÔ∏è Note: CSV entries for this experiment may still exist in experiment_results.csv")
                        
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error cleaning {target}: {e}")
        
        print(f"  ‚úÖ Cleaned {cleaned_count} items")
    
    def _should_preserve(self, path: str, preserve_patterns: list) -> bool:
        """Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â∫îËØ•Ë¢´‰øùÁïô"""
        import fnmatch
        path_name = os.path.basename(path)
        for pattern in preserve_patterns:
            if fnmatch.fnmatch(path_name, pattern) or fnmatch.fnmatch(path, pattern):
                return True
        return False
    
    def _init_results_dict(self, source_model: str, target_model: str, dataset: str) -> Dict[str, Any]:
        """ÂàùÂßãÂåñÁªìÊûúÂ≠óÂÖ∏"""
        return {
            'experiment_id': self.experiment_id,
            'source_model': source_model,
            'target_model': target_model,
            'dataset': dataset,
            'timestamp': self.timestamp,
            'training_config': f"batch_size={self.config.get('training.default_batch_size')}, "
                              f"max_steps={self.config.get('training.default_max_steps')}, "
                              f"lr={self.config.get('training.default_lr')}",
            'notes': 'Ëá™Âä®ÂåñÁÆ°ÈÅìÁîüÊàê'
        }
    
    def _execute_pipeline_steps(self, results: Dict[str, Any], source_model: str, 
                               target_model: str, dataset: str, eval_only: bool) -> bool:
        """ÊâßË°åÁÆ°ÈÅìÊ≠•È™§"""
        # ÊÄªÊòØÊòæÁ§∫ÂÆåÊï¥ÁöÑ6Ê≠•ËøõÂ∫¶Êù°
        progress_bar = tqdm(total=6, desc="Pipeline Progress", position=1, leave=True, ncols=80)
        
        try:
            # Ê≠•È™§1: ËÆ≠ÁªÉÊ∫êLoRA
            if not eval_only:
                if not self._step_train_source_lora(results, source_model, dataset, progress_bar):
                    raise Exception("Ê∫êÊ®°ÂûãËÆ≠ÁªÉÂ§±Ë¥•")
            else:
                self._step_skip_with_reason("STEP 1/6: TRAIN SOURCE LORA", "‰ªÖËØÑ‰º∞Ê®°ÂºèÔºåË∑≥ËøáËÆ≠ÁªÉ", progress_bar)
            
            # Ê≠•È™§2: ËøÅÁßªLoRA
            if not eval_only:
                if not self._step_transfer_lora(results, source_model, target_model, dataset, progress_bar):
                    raise Exception("LoRAËøÅÁßªÂ§±Ë¥•")
            else:
                self._step_skip_with_reason("STEP 2/6: TRANSFER LORA", "‰ªÖËØÑ‰º∞Ê®°ÂºèÔºåË∑≥ËøáËøÅÁßª", progress_bar)
            
            # Ê≠•È™§3: ËØÑ‰º∞ÁõÆÊ†áÂü∫Á°ÄÊ®°Âûã
            if not self._step_eval_target_base(results, target_model, dataset, progress_bar):
                print("‚ö†Ô∏è ÁõÆÊ†áÂü∫Á°ÄÊ®°ÂûãËØÑ‰º∞Â§±Ë¥•Ôºå‰ΩÜÁªßÁª≠ÊâßË°å")
            
            # Ê≠•È™§4: ËØÑ‰º∞ËøÅÁßªLoRA
            if not eval_only:
                if not self._step_eval_transferred_lora(results, target_model, dataset, progress_bar):
                    print("‚ö†Ô∏è ËøÅÁßªLoRAËØÑ‰º∞Â§±Ë¥•Ôºå‰ΩÜÁªßÁª≠ÊâßË°å")
            else:
                self._step_skip_with_reason("STEP 4/6: EVAL TRANSFERRED LORA", "‰ªÖËØÑ‰º∞Ê®°ÂºèÔºåÊó†ËøÅÁßªLoRAÂèØËØÑ‰º∞", progress_bar)
            
            # Ê≠•È™§5: ËÆ≠ÁªÉÁõÆÊ†áLoRA
            if not eval_only:
                if not self._step_train_target_lora(results, target_model, dataset, progress_bar):
                    print("‚ö†Ô∏è ÁõÆÊ†áÊ®°ÂûãËÆ≠ÁªÉÂ§±Ë¥•Ôºå‰ΩÜÁªßÁª≠ÊâßË°å")
            else:
                self._step_skip_with_reason("STEP 5/6: TRAIN TARGET LORA", "‰ªÖËØÑ‰º∞Ê®°ÂºèÔºåË∑≥ËøáËÆ≠ÁªÉ", progress_bar)
            
            # Ê≠•È™§6: ËØÑ‰º∞Ê∫êÂü∫Á°ÄÊ®°Âûã
            if not self._step_eval_source_base(results, source_model, dataset, progress_bar):
                print("‚ö†Ô∏è Ê∫êÂü∫Á°ÄÊ®°ÂûãËØÑ‰º∞Â§±Ë¥•Ôºå‰ΩÜÁªßÁª≠ÊâßË°å")
            
            # ÊúÄÁªà‰øùÂ≠òÂÆåÊï¥ÁªìÊûú
            progress_bar.set_description("Saving Results")
            self.results.save_results(results)
            progress_bar.close()
            
            # ÊâìÂç∞ÊÄªÁªì
            self._print_summary(results)
            
            # ÊèêÁ§∫ÂèØÈÄâÂëΩ‰ª§
            if not eval_only:
                self._print_optional_commands(source_model, target_model, dataset)
            
            print("\nPipeline completed successfully!")
            return True
            
        except Exception as e:
            progress_bar.close()
            print(f"\nPipeline failed: {e}")
            # ‰øùÂ≠òÈÉ®ÂàÜÁªìÊûú
            self.results.save_partial_results(results, f"Â§±Ë¥•: {e}")
            return False
    
    def _step_skip_with_reason(self, step_title: str, reason: str, progress_bar: tqdm):
        """ÊòæÁ§∫Ë∑≥ËøáÁöÑÊ≠•È™§ÂèäÂéüÂõ†"""
        print(f"\n{'='*60}")
        print(step_title)
        print("="*60)
        print(f"üö´ Ë∑≥ËøáÂéüÂõ†: {reason}")
        print("="*60)
        progress_bar.update(1)
    
    def _step_train_source_lora(self, results: Dict[str, Any], source_model: str, 
                               dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§1: ËÆ≠ÁªÉÊ∫êLoRA"""
        print(f"\n{'='*60}")
        print("STEP 1/6: TRAIN SOURCE LORA")
        print("="*60)
        
        source_lora_path, source_lora_acc, status_msg = self.trainer.train_model(source_model, dataset)
        print(f"Áä∂ÊÄÅ: {status_msg}")
        print(f"üîç DEBUG: ËÆ≠ÁªÉÂô®ËøîÂõûÁöÑÂáÜÁ°ÆÁéá: {source_lora_acc}")
        if source_lora_path is None:
            return False
        
        results.update({
            'source_lora_path': source_lora_path,
            'source_lora_acc': source_lora_acc,
        })
        self.results.save_partial_results(results, "Ê∫êLoRAËÆ≠ÁªÉÂÆåÊàê")
        progress_bar.update(1)
        return True
    
    def _step_transfer_lora(self, results: Dict[str, Any], source_model: str,
                           target_model: str, dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§2: ËøÅÁßªLoRA"""
        print(f"\n{'='*60}")
        print("STEP 2/6: TRANSFER LORA")
        print("="*60)
        
        transferred_lora_path = self.transfer.transfer_lora(
            results['source_lora_path'], source_model, target_model, dataset
        )
        if transferred_lora_path is None:
            return False
        
        results['transferred_lora_path'] = transferred_lora_path
        self.results.save_partial_results(results, "LoRAËøÅÁßªÂÆåÊàê")
        progress_bar.update(1)
        return True
    
    def _step_eval_target_base(self, results: Dict[str, Any], target_model: str, 
                              dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§3: ËØÑ‰º∞ÁõÆÊ†áÂü∫Á°ÄÊ®°Âûã"""
        print(f"\n{'='*60}")
        print("STEP 3/6: EVAL TARGET BASE MODEL")
        print("="*60)
        
        target_acc = self.evaluator.evaluate_base_model(target_model, dataset)
        results['target_acc'] = target_acc
        self.results.save_partial_results(results, "ÁõÆÊ†áÂü∫Á°ÄÊ®°ÂûãËØÑ‰º∞ÂÆåÊàê")
        progress_bar.update(1)
        return target_acc is not None
    
    def _step_eval_transferred_lora(self, results: Dict[str, Any], target_model: str, 
                                   dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§4: ËØÑ‰º∞ËøÅÁßªLoRA"""
        print(f"\n{'='*60}")
        print("STEP 4/6: EVAL TRANSFERRED LORA")
        print("="*60)
        
        transferred_acc = self.evaluator.evaluate_lora_model(
            results['transferred_lora_path'], target_model, dataset
        )
        results['transferred_acc'] = transferred_acc
        self.results.save_partial_results(results, "ËøÅÁßªLoRAËØÑ‰º∞ÂÆåÊàê")
        progress_bar.update(1)
        return transferred_acc is not None
    
    def _step_train_target_lora(self, results: Dict[str, Any], target_model: str, 
                               dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§5: ËÆ≠ÁªÉÁõÆÊ†áLoRA"""
        print(f"\n{'='*60}")
        print("STEP 5/6: TRAIN TARGET LORA")
        print("="*60)
        
        target_lora_path, target_lora_acc, status_msg = self.trainer.train_model(target_model, dataset)
        print(f"Áä∂ÊÄÅ: {status_msg}")
        if target_lora_path is None:
            target_lora_acc = None
        
        results.update({
            'target_lora_path': target_lora_path,
            'target_lora_acc': target_lora_acc,
        })
        self.results.save_partial_results(results, "ÁõÆÊ†áLoRAËÆ≠ÁªÉÂÆåÊàê")
        progress_bar.update(1)
        return True  # Âç≥‰ΩøÂ§±Ë¥•‰πüÁªßÁª≠
    
    def _step_eval_source_base(self, results: Dict[str, Any], source_model: str, 
                              dataset: str, progress_bar: tqdm) -> bool:
        """Ê≠•È™§6: ËØÑ‰º∞Ê∫êÂü∫Á°ÄÊ®°Âûã"""
        print(f"\n{'='*60}")
        print("STEP 6/6: EVAL SOURCE BASE MODEL")
        print("="*60)
        
        source_acc = self.evaluator.evaluate_base_model(source_model, dataset)
        results['source_acc'] = source_acc
        self.results.save_partial_results(results, "Ê∫êÂü∫Á°ÄÊ®°ÂûãËØÑ‰º∞ÂÆåÊàê")
        progress_bar.update(1)
        return source_acc is not None
    
    def _print_summary(self, results: Dict[str, Any]):
        """Print experiment summary"""
        print(f"\n{'='*60}")
        print("EXPERIMENT SUMMARY")
        print("=" * 60)
        
        source_name = ModelUtils.get_model_short_name(results['source_model'])
        target_name = ModelUtils.get_model_short_name(results['target_model'])
        
        # Handle potentially None values
        source_acc = results.get('source_acc', 0) or 0
        target_acc = results.get('target_acc', 0) or 0
        source_lora_acc = results.get('source_lora_acc')
        target_lora_acc = results.get('target_lora_acc')
        transferred_acc = results.get('transferred_acc')
        
        print(f"Source Model ({source_name}):     {source_acc:.4f}")
        if source_lora_acc is not None:
            improvement = (source_lora_acc - source_acc) * 100
            print(f"Source + LoRA:              {source_lora_acc:.4f} (+{improvement:.2f}%)")
        
        print(f"Target Model ({target_name}):     {target_acc:.4f}")
        
        if transferred_acc is not None:
            improvement = (transferred_acc - target_acc) * 100
            print(f"Target + Transferred LoRA:  {transferred_acc:.4f} (+{improvement:.2f}%)")
        
        if target_lora_acc is not None:
            improvement = (target_lora_acc - target_acc) * 100
            print(f"Target + Direct LoRA:       {target_lora_acc:.4f} (+{improvement:.2f}%)")
        
        print("=" * 60)
        print(f"Detailed results: results/experiment_summary.md")
    
    def _print_optional_commands(self, source_model: str, target_model: str, dataset: str):
        """Print optional target model LoRA training commands"""
        target_name = ModelUtils.get_model_short_name(target_model)
        
        print(f"\n{'-'*60}")
        print(f"OPTIONAL: Train {target_name} LoRA for Comparison")
        print("-" * 60)
        
        # Training command
        train_cmd = f"python {self.config.get('paths.train_script')} " \
                   f"--dataset {dataset} " \
                   f"--base_model {target_model} " \
                   f"--bs {self.config.get('training.default_batch_size')} " \
                   f"--max_steps {self.config.get('training.default_max_steps')}"
        
        print(f"Train {target_name} LoRA:")
        print(f"  {train_cmd}")
        
        # Evaluation command 
        eval_cmd = f"python {self.config.get('paths.eval_script')} " \
                  f"--models_list [trained_model_path] " \
                  f"--dataset {dataset} " \
                  f"--sample_ratio {self.config.get('evaluation.sample_ratio')} " \
                  f"--base_model {target_model}"
        
        print(f"\nEvaluate {target_name} LoRA:")
        print(f"  {eval_cmd}")
