#!/usr/bin/env python3
"""
train_cs_lora_icoding.py
æœåŠ¡å™¨ç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬ - é’ˆå¯¹4Ã—H800ä¼˜åŒ–
æ”¯æŒç²¾ç¡®batchè¿½è¸ªå’Œprompt-checkpointé…å¯¹

é…ç½®ç‰¹ç‚¹:
- Batch size: 32 (é€‚é…æœåŠ¡å™¨å†…å­˜)
- Training steps: 125
- Checkpointé¢‘ç‡: æ¯50æ­¥ä¿å­˜
- æ•°æ®åŠ è½½: ä¸¥æ ¼é¡ºåºï¼Œå¯è¿½è¸ªæ¯ä¸ªbatchçš„sourceè¡Œå·

ä½¿ç”¨ç¤ºä¾‹:
# æœåŠ¡å™¨ç¯å¢ƒ - batch size 32
python train_cs_lora_icoding.py --dataset arc-challenge

# æœ¬åœ°æµ‹è¯• - batch size 4 
python train_cs_lora_icoding.py --dataset arc-challenge --test_mode

# è®­ç»ƒå•ä¸ªæ•°æ®é›†
python train_cs_lora_icoding.py --dataset arc-challenge

# è®­ç»ƒå¤šä¸ªæ•°æ®é›†
python train_cs_lora_icoding.py --dataset arc-challenge arc-easy

# å¯ç”¨è¯¦ç»†çš„batchè¿½è¸ªæ—¥å¿—
python train_cs_lora_icoding.py --dataset arc-challenge --track_batches

# å¹²è¿è¡ŒæŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
python train_cs_lora_icoding.py --dataset arc-challenge --dry_run

å…³é”®ç‰¹æ€§:
1. å›ºå®šbatch size=32ï¼Œä¸¥æ ¼é¡ºåºåŠ è½½æ•°æ®
2. æ¯ä¸ªcheckpointç²¾ç¡®è®°å½•å¯¹åº”çš„promptèŒƒå›´
3. è‡ªåŠ¨å¤„ç†epochå¾ªç¯ï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§
4. è¯¦ç»†çš„batch-checkpointæ˜ å°„æ—¥å¿—
"""

import os
import sys
import json
import yaml
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
import torch
from torch.utils.data import DataLoader, Dataset


def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œä¿æŒå­—å…¸ç»“æ„ä¸è¢«PyTorché»˜è®¤å¤„ç†ç ´å"""
    return batch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

try:
    from scripts.experiment_manager_enhanced import ExperimentManager
    from scripts.model_manager import ModelManager
    from utils.data_processor import DataProcessor
    from core.train import LoRATrainer
    from lora.checkpoint_utils import CheckpointManager
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)


class SequentialTrackingDataset(Dataset):
    """ä¸¥æ ¼é¡ºåºçš„å¯è¿½è¸ªæ•°æ®é›†"""
    
    def __init__(self, data_file: str, dataset_name: str):
        self.data_file = data_file
        self.dataset_name = dataset_name
        self.data = self._load_data()
        self.total_samples = len(self.data)
        
    def _load_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®ï¼Œä¿æŒåŸå§‹é¡ºåº"""
        data = []
        if not os.path.exists(self.data_file):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            
        with open(self.data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        # æ·»åŠ åŸå§‹è¡Œå·ä¿¡æ¯
                        item['_source_line'] = line_idx
                        item['_dataset_name'] = self.dataset_name
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {line_idx}: {e}")
                        
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›† {self.dataset_name}: {len(data)} æ ·æœ¬")
        return data
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        # æ”¯æŒè¶…è¿‡æ•°æ®é›†é•¿åº¦çš„å¾ªç¯è®¿é—®
        actual_idx = idx % self.total_samples
        item = self.data[actual_idx].copy()
        
        # æ·»åŠ å¾ªç¯ä¿¡æ¯
        epoch_num = idx // self.total_samples
        item['_actual_idx'] = actual_idx
        item['_global_idx'] = idx
        item['_epoch'] = epoch_num
        
        return item


class BatchTracker:
    """Batchè¿½è¸ªå™¨ï¼Œè®°å½•æ¯ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯"""
    
    def __init__(self, dataset_name: str, log_dir: str):
        self.dataset_name = dataset_name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¿½è¸ªæ—¥å¿—
        self.batch_log_file = self.log_dir / f"batch_tracking_{dataset_name}.jsonl"
        self.checkpoint_map_file = self.log_dir / f"checkpoint_mapping_{dataset_name}.json"
        
        self.batch_records = []
        self.checkpoint_mappings = {}
        
    def log_batch(self, step: int, batch_data: List[Dict[str, Any]]):
        """è®°å½•å•ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯"""
        batch_info = {
            "step": step,
            "dataset": self.dataset_name,
            "batch_size": len(batch_data),
            "timestamp": datetime.now().isoformat(),
            "samples": []
        }
        
        # è®°å½•æ¯ä¸ªæ ·æœ¬çš„è¿½è¸ªä¿¡æ¯
        for sample in batch_data:
            sample_info = {
                "source_line": sample.get('_source_line', -1),
                "actual_idx": sample.get('_actual_idx', -1),
                "global_idx": sample.get('_global_idx', -1),
                "epoch": sample.get('_epoch', 0),
                "id": sample.get('id', 'unknown'),
                "input_preview": sample.get('input', '')[:100] + '...' if len(sample.get('input', '')) > 100 else sample.get('input', '')
            }
            batch_info["samples"].append(sample_info)
        
        # è®¡ç®—batchèŒƒå›´
        source_lines = [s['source_line'] for s in batch_info["samples"]]
        batch_info["source_range"] = {
            "min_line": min(source_lines) if source_lines else -1,
            "max_line": max(source_lines) if source_lines else -1,
            "epochs_involved": list(set(s['epoch'] for s in batch_info["samples"]))
        }
        
        self.batch_records.append(batch_info)
        
        # å®æ—¶å†™å…¥æ—¥å¿—
        with open(self.batch_log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(batch_info, ensure_ascii=False) + '\n')
    
    def log_checkpoint(self, step: int, checkpoint_path: str, batch_range: Tuple[int, int]):
        """è®°å½•checkpointä¸batchçš„æ˜ å°„å…³ç³»"""
        start_step, end_step = batch_range
        
        # æ‰¾åˆ°å¯¹åº”çš„batchè®°å½•
        related_batches = [
            record for record in self.batch_records 
            if start_step <= record["step"] <= end_step
        ]
        
        checkpoint_info = {
            "checkpoint_step": step,
            "checkpoint_path": checkpoint_path,
            "batch_range": {
                "start_step": start_step,
                "end_step": end_step
            },
            "total_batches": len(related_batches),
            "total_samples": sum(b["batch_size"] for b in related_batches),
            "source_data_summary": self._summarize_source_data(related_batches),
            "timestamp": datetime.now().isoformat()
        }
        
        self.checkpoint_mappings[f"checkpoint_{step}"] = checkpoint_info
        
        # ä¿å­˜mappingæ–‡ä»¶
        with open(self.checkpoint_map_file, 'w', encoding='utf-8') as f:
            json.dump(self.checkpoint_mappings, f, indent=2, ensure_ascii=False)
    
    def _summarize_source_data(self, batches: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ€»ç»“source dataä¿¡æ¯"""
        all_source_lines = []
        all_epochs = set()
        
        for batch in batches:
            for sample in batch["samples"]:
                all_source_lines.append(sample["source_line"])
                all_epochs.add(sample["epoch"])
        
        return {
            "total_source_lines": len(all_source_lines),
            "source_line_range": {
                "min": min(all_source_lines) if all_source_lines else -1,
                "max": max(all_source_lines) if all_source_lines else -1
            },
            "epochs_involved": sorted(list(all_epochs)),
            "unique_source_lines": len(set(all_source_lines))
        }


def create_icoding_config(dataset_name: str, base_config: Dict[str, Any], batch_size: int = 32) -> Dict[str, Any]:
    """åˆ›å»ºicodingç¯å¢ƒçš„è®­ç»ƒé…ç½®"""
    config = base_config.copy()
    
    # æ›´æ–°æ•°æ®è·¯å¾„
    config['data']['train_file'] = f"data_to_lora/cs/{dataset_name}/{dataset_name}_train_formatted.jsonl"
    
    # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºå®éªŒåç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"icoding_{timestamp}_lora"
    
    # icodingä¸“ç”¨é…ç½®
    config['training'].update({
        'per_device_train_batch_size': batch_size,  # å¯é…ç½®batch size
        'gradient_accumulation_steps': 1,
        'max_steps': 125,  # å›ºå®šæ­¥æ•°
        'save_steps': 50,  # checkpointé¢‘ç‡
        'logging_steps': 1,
        'dataloader_num_workers': 4,  # æœåŠ¡å™¨ä¼˜åŒ–
        'dataloader_pin_memory': True,
        'remove_unused_columns': False,  # ä¿ç•™è¿½è¸ªä¿¡æ¯
    })
    
    # ç¡®ä¿æ•°æ®åŠ è½½çš„ä¸€è‡´æ€§
    config['data'].update({
        'shuffle': False,  # ä¸¥æ ¼ç¦ç”¨shuffle
        'seed': 42,  # å›ºå®šç§å­
        'drop_last': False  # ä¿ç•™æœ€åçš„ä¸å®Œæ•´batch
    })
    
    # æ›´æ–°è¾“å‡ºç›®å½•
    config['training']['output_dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/models"
    config['checkpoint']['dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/checkpoints"
    config['logging']['log_dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/logs"
    
    # å®éªŒä¿¡æ¯
    config['experiment']['name'] = experiment_name
    config['experiment']['description'] = f"iCoding LoRA training on {dataset_name} - batch_size={batch_size}, steps=125"
    config['experiment']['tags'] = ["icoding", "lora", "qwen2.5", dataset_name, f"batch{batch_size}", "trackable"]
    
    config['experiment_type'] = "icoding_lora"
    config['dataset_name'] = dataset_name
    
    return config


def run_icoding_experiment(dataset_name: str, base_config: Dict[str, Any], track_batches: bool = True, dry_run: bool = False, test_mode: bool = False, batch_size: int = 32) -> Dict[str, Any]:
    """è¿è¡Œicodingç¯å¢ƒçš„è®­ç»ƒå®éªŒ"""
    
    print(f"\n{'=' * 70}")
    print(f"ğŸš€ iCodingç¯å¢ƒè®­ç»ƒ: {dataset_name}")
    print(f"{'=' * 70}")
    
    # åˆ›å»ºé…ç½®
    config = create_icoding_config(dataset_name, base_config, batch_size)
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    data_file = config['data']['train_file']
    if not os.path.exists(data_file):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"ğŸ¯ è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: batch_size={batch_size}, steps=125, checkpoint_every=50")
    
    if dry_run:
        # åˆ†ææ•°æ®åˆ†å¸ƒ
        dataset = SequentialTrackingDataset(data_file, dataset_name)
        analyze_data_distribution(dataset, batch_size=batch_size, total_steps=125)
        return {"status": "dry_run_completed"}
    
    try:
        # è®¾ç½®æ—¥å¿—ç›®å½•
        log_dir = Path(config['logging']['log_dir'])
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨
        tracker = None
        if track_batches:
            tracker = BatchTracker(dataset_name, str(log_dir))
            print("ğŸ“ å¯ç”¨batchè¿½è¸ª")
        
        # åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
        dataset = SequentialTrackingDataset(data_file, dataset_name)
        
        # åˆ›å»ºDataLoader - å…³é”®é…ç½®ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,  # ä¸¥æ ¼ç¦ç”¨shuffle
            num_workers=0 if test_mode else 4,  # æµ‹è¯•æ¨¡å¼ä¸‹ä½¿ç”¨0ä¸ªworkeré¿å…å¤šè¿›ç¨‹é—®é¢˜
            pin_memory=False if test_mode else True,  # æµ‹è¯•æ¨¡å¼ä¸‹å…³é—­pin_memory
            drop_last=False,  # ä¿ç•™æœ€åçš„ä¸å®Œæ•´batch
            persistent_workers=False if test_mode else True,  # æµ‹è¯•æ¨¡å¼ä¸‹å…³é—­persistent workers
            collate_fn=custom_collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°ä¿æŒå­—å…¸ç»“æ„
        )
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨é…ç½®:")
        print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")
        print(f"  - Batchå¤§å°: {batch_size}")
        print(f"  - æ€»batchæ•°: {len(dataloader)}")
        print(f"  - Shuffle: False (ä¸¥æ ¼é¡ºåº)")
        
        # æ‰§è¡Œè®­ç»ƒæˆ–æ¨¡æ‹Ÿè®­ç»ƒ
        if test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šå®Œæ•´çš„batchè¿½è¸ªéªŒè¯ï¼Œä½†æ­¥æ•°è¾ƒå°‘ä»¥ä¾¿è§‚å¯Ÿ
            results = run_test_training_with_full_tracking(
                dataloader=dataloader,
                total_steps=20,  # æµ‹è¯•æ¨¡å¼ä½¿ç”¨è¾ƒå°‘æ­¥æ•°
                checkpoint_steps=[10, 20],  # æµ‹è¯•checkpoint
                tracker=tracker,
                config=config
            )
        else:
            # æ­£å¸¸æ¨¡å¼æˆ–æ¨¡æ‹Ÿè®­ç»ƒ
            results = simulate_training_with_tracking(
                dataloader=dataloader,
                total_steps=125,
                checkpoint_steps=[50, 100, 125],
                tracker=tracker,
                config=config
            )
        
        # ç”Ÿæˆline-to-checkpointæ˜ å°„æ–‡ä»¶
        if tracker:
            generate_line_to_checkpoint_mapping(tracker, str(log_dir))
        
        return results
        
    except Exception as e:
        print(f"âŒ {dataset_name} è®­ç»ƒå¤±è´¥: {e}")
        raise


def simulate_training_with_tracking(
    dataloader: DataLoader,
    total_steps: int,
    checkpoint_steps: List[int],
    tracker: Optional[BatchTracker],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå±•ç¤ºbatchè¿½è¸ªé€»è¾‘"""
    
    print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒæ¨¡æ‹Ÿ (å…±{total_steps}æ­¥)...")
    
    step = 0
    checkpoint_saved = []
    dataloader_iter = iter(dataloader)
    last_checkpoint_step = 0
    
    while step < total_steps:
        try:
            # è·å–ä¸‹ä¸€ä¸ªbatch
            batch = next(dataloader_iter)
            
            # è®°å½•batchä¿¡æ¯
            if tracker:
                tracker.log_batch(step, batch)
            
            # æ˜¾ç¤ºè¿›åº¦
            if step % 10 == 0 or step < 5:
                sample_info = batch[0] if batch else {}
                print(f"  Step {step}: batch_size={len(batch)}, "
                      f"source_lines=[{sample_info.get('_source_line', -1)}...], "
                      f"epoch={sample_info.get('_epoch', 0)}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜checkpoint
            if step + 1 in checkpoint_steps:
                checkpoint_path = f"checkpoint-{step + 1}"
                checkpoint_saved.append({
                    "step": step + 1,
                    "path": checkpoint_path,
                    "batch_range": (last_checkpoint_step, step)
                })
                
                if tracker:
                    tracker.log_checkpoint(step + 1, checkpoint_path, (last_checkpoint_step, step))
                
                print(f"ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path} (è¦†ç›–steps {last_checkpoint_step}-{step})")
                last_checkpoint_step = step + 1
            
            step += 1
            
        except StopIteration:
            # DataLoaderè€—å°½ï¼Œé‡æ–°å¼€å§‹ä¸‹ä¸€ä¸ªepoch
            print(f"  ğŸ“š æ•°æ®é›†éå†å®Œæ¯•ï¼Œå¼€å§‹æ–°epoch (å½“å‰step: {step})")
            dataloader_iter = iter(dataloader)
    
    results = {
        "status": "completed",
        "total_steps": step,
        "checkpoints_saved": checkpoint_saved,
        "dataset_cycles": step // len(dataloader) + (1 if step % len(dataloader) > 0 else 0)
    }
    
    print(f"\nâœ… è®­ç»ƒæ¨¡æ‹Ÿå®Œæˆ:")
    print(f"  - æ€»æ­¥æ•°: {step}")
    print(f"  - ä¿å­˜checkpoint: {len(checkpoint_saved)}ä¸ª")
    print(f"  - æ•°æ®é›†å¾ªç¯: {results['dataset_cycles']}æ¬¡")
    
    return results


def run_test_training_with_full_tracking(
    dataloader: DataLoader,
    total_steps: int,
    checkpoint_steps: List[int],
    tracker: Optional[BatchTracker],
    config: Dict[str, Any]
) -> Dict[str, Any]:
    """æµ‹è¯•æ¨¡å¼ï¼šè¿è¡Œå®Œæ•´çš„batchè¿½è¸ªéªŒè¯"""
    
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼è®­ç»ƒ (å…±{total_steps}æ­¥ï¼Œå®Œæ•´è¿½è¸ª)...")
    
    step = 0
    checkpoint_saved = []
    dataloader_iter = iter(dataloader)
    last_checkpoint_step = 0
    all_batches_data = []  # ä¿å­˜æ‰€æœ‰batchæ•°æ®ç”¨äºåˆ†æ
    
    while step < total_steps:
        try:
            # è·å–ä¸‹ä¸€ä¸ªbatch
            batch = next(dataloader_iter)
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            batch_list = []
            for item in batch:
                if isinstance(item, dict):
                    batch_list.append(item)
                elif hasattr(item, 'item'):  # å¤„ç†tensor
                    batch_list.append({'tensor_value': item.item()})
                else:
                    batch_list.append({'value': str(item)})
            
            all_batches_data.append({
                'step': step,
                'batch': batch_list
            })
            
            # è®°å½•batchä¿¡æ¯
            if tracker:
                tracker.log_batch(step, batch_list)
            
            # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
            sample_info = batch_list[0] if batch_list else {}
            source_lines = [item.get('_source_line', -1) for item in batch_list]
            epochs = list(set(item.get('_epoch', 0) for item in batch_list))
            
            print(f"  ğŸ” Step {step:2d}: batch_size={len(batch_list)}, "
                  f"source_lines=[{min(source_lines):3d}-{max(source_lines):3d}], "
                  f"epochs={epochs}")
            
            # æ˜¾ç¤ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ˆä»…å‰3ä¸ªæ ·æœ¬ä»¥é¿å…è¾“å‡ºè¿‡å¤šï¼‰
            for i, item in enumerate(batch_list[:3]):
                print(f"    ğŸ“„ Sample {i}: line={item.get('_source_line', -1)}, "
                      f"id={item.get('id', 'N/A')[:20]}..., "
                      f"epoch={item.get('_epoch', 0)}")
            
            if len(batch_list) > 3:
                print(f"    ğŸ“„ ... è¿˜æœ‰ {len(batch_list) - 3} ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜checkpoint
            if step + 1 in checkpoint_steps:
                checkpoint_path = f"checkpoint-{step + 1}"
                checkpoint_saved.append({
                    "step": step + 1,
                    "path": checkpoint_path,
                    "batch_range": (last_checkpoint_step, step)
                })
                
                if tracker:
                    tracker.log_checkpoint(step + 1, checkpoint_path, (last_checkpoint_step, step))
                
                print(f"ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path} (è¦†ç›–steps {last_checkpoint_step}-{step})")
                last_checkpoint_step = step + 1
            
            step += 1
            
        except StopIteration:
            # DataLoaderè€—å°½ï¼Œé‡æ–°å¼€å§‹ä¸‹ä¸€ä¸ªepoch
            print(f"  ğŸ“š æ•°æ®é›†éå†å®Œæ¯•ï¼Œå¼€å§‹æ–°epoch (å½“å‰step: {step})")
            dataloader_iter = iter(dataloader)
    
    results = {
        "status": "test_completed",
        "total_steps": step,
        "checkpoints_saved": checkpoint_saved,
        "dataset_cycles": step // len(dataloader) + (1 if step % len(dataloader) > 0 else 0),
        "all_batches_data": all_batches_data
    }
    
    print(f"\nâœ… æµ‹è¯•æ¨¡å¼è®­ç»ƒå®Œæˆ:")
    print(f"  - æ€»æ­¥æ•°: {step}")
    print(f"  - ä¿å­˜checkpoint: {len(checkpoint_saved)}ä¸ª")
    print(f"  - æ•°æ®é›†å¾ªç¯: {results['dataset_cycles']}æ¬¡")
    print(f"  - è¿½è¸ªbatch: {len(all_batches_data)}ä¸ª")
    
    return results


def generate_line_to_checkpoint_mapping(tracker: BatchTracker, log_dir: str):
    """ç”Ÿæˆline-to-checkpointçš„å®Œæ•´æ˜ å°„æ–‡ä»¶"""
    
    print(f"\nğŸ“‹ ç”Ÿæˆline-to-checkpointæ˜ å°„æ–‡ä»¶...")
    
    # æ„å»ºå®Œæ•´çš„æ˜ å°„å…³ç³»
    line_to_checkpoint = {}
    checkpoint_to_lines = {}
    
    # éå†æ‰€æœ‰checkpointæ˜ å°„
    for ckpt_name, ckpt_info in tracker.checkpoint_mappings.items():
        checkpoint_step = ckpt_info['checkpoint_step']
        start_step = ckpt_info['batch_range']['start_step']
        end_step = ckpt_info['batch_range']['end_step']
        
        # æ‰¾åˆ°å¯¹åº”æ­¥éª¤çš„batchè®°å½•
        related_batches = [
            record for record in tracker.batch_records
            if start_step <= record["step"] <= end_step
        ]
        
        lines_in_checkpoint = []
        
        for batch_record in related_batches:
            for sample in batch_record["samples"]:
                source_line = sample["source_line"]
                lines_in_checkpoint.append(source_line)
                
                # è®°å½•lineåˆ°checkpointçš„æ˜ å°„
                if source_line not in line_to_checkpoint:
                    line_to_checkpoint[source_line] = []
                line_to_checkpoint[source_line].append({
                    "checkpoint": ckpt_name,
                    "checkpoint_step": checkpoint_step,
                    "step": batch_record["step"],
                    "sample_id": sample["id"]
                })
        
        # è®°å½•checkpointåˆ°linesçš„æ˜ å°„
        checkpoint_to_lines[ckpt_name] = {
            "checkpoint_step": checkpoint_step,
            "lines": sorted(list(set(lines_in_checkpoint))),
            "line_count": len(set(lines_in_checkpoint)),
            "total_samples": len(lines_in_checkpoint),
            "step_range": [start_step, end_step]
        }
    
    # ä¿å­˜æ˜ å°„æ–‡ä»¶
    mapping_data = {
        "dataset": tracker.dataset_name,
        "generated_at": datetime.now().isoformat(),
        "summary": {
            "total_checkpoints": len(checkpoint_to_lines),
            "total_unique_lines": len(line_to_checkpoint),
            "total_line_checkpoint_pairs": sum(len(mappings) for mappings in line_to_checkpoint.values())
        },
        "line_to_checkpoint": line_to_checkpoint,
        "checkpoint_to_lines": checkpoint_to_lines
    }
    
    # ä¿å­˜è¯¦ç»†æ˜ å°„
    mapping_file = Path(log_dir) / f"line_checkpoint_mapping_{tracker.dataset_name}.json"
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping_data, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆç®€åŒ–çš„æ¦‚è§ˆæ–‡ä»¶
    summary_data = {
        "dataset": tracker.dataset_name,
        "generated_at": datetime.now().isoformat(),
        "checkpoints": []
    }
    
    for ckpt_name, info in checkpoint_to_lines.items():
        summary_data["checkpoints"].append({
            "name": ckpt_name,
            "step": info["checkpoint_step"],
            "line_range": [min(info["lines"]), max(info["lines"])] if info["lines"] else [0, 0],
            "unique_lines": info["line_count"],
            "total_samples": info["total_samples"]
        })
    
    summary_file = Path(log_dir) / f"checkpoint_summary_{tracker.dataset_name}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ˜ å°„æ–‡ä»¶å·²ç”Ÿæˆ:")
    print(f"  - è¯¦ç»†æ˜ å°„: {mapping_file}")
    print(f"  - æ¦‚è§ˆæ–‡ä»¶: {summary_file}")
    print(f"ğŸ“Š æ˜ å°„ç»Ÿè®¡:")
    print(f"  - æ€»checkpointæ•°: {len(checkpoint_to_lines)}")
    print(f"  - æ€»è¡Œæ•°: {len(line_to_checkpoint)}")
    print(f"  - è¡Œ-checkpointå¯¹æ•°: {sum(len(mappings) for mappings in line_to_checkpoint.values())}")


def analyze_data_distribution(dataset: SequentialTrackingDataset, batch_size: int, total_steps: int):
    """åˆ†ææ•°æ®åˆ†å¸ƒæƒ…å†µ"""
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
    print(f"{'=' * 50}")
    
    total_samples_needed = batch_size * total_steps
    dataset_size = len(dataset)
    epochs_needed = total_samples_needed / dataset_size
    
    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  - æ•°æ®é›†åç§°: {dataset.dataset_name}")
    print(f"  - æ ·æœ¬æ€»æ•°: {dataset_size}")
    print(f"  - æ•°æ®æ–‡ä»¶: {dataset.data_file}")
    
    print(f"\nè®­ç»ƒéœ€æ±‚:")
    print(f"  - Batchå¤§å°: {batch_size}")
    print(f"  - è®­ç»ƒæ­¥æ•°: {total_steps}")
    print(f"  - éœ€è¦æ ·æœ¬æ•°: {total_samples_needed}")
    print(f"  - éœ€è¦å¾ªç¯: {epochs_needed:.2f} epochs")
    
    print(f"\nBatchåˆ†å¸ƒé¢„æµ‹:")
    for step in range(min(10, total_steps)):  # æ˜¾ç¤ºå‰10ä¸ªbatch
        start_idx = step * batch_size
        end_idx = min(start_idx + batch_size, total_samples_needed)
        
        batch_samples = []
        for idx in range(start_idx, end_idx):
            actual_idx = idx % dataset_size
            epoch = idx // dataset_size
            sample = dataset[idx]
            batch_samples.append((actual_idx, epoch, sample['_source_line']))
        
        source_lines = [s[2] for s in batch_samples]
        epochs = list(set(s[1] for s in batch_samples))
        
        print(f"  Step {step:2d}: source_lines=[{min(source_lines):3d}-{max(source_lines):3d}], epochs={epochs}")
    
    if total_steps > 10:
        print(f"  ... (çœç•¥ä¸­é—´ {total_steps - 10} æ­¥)")
    
    # åˆ†æcheckpointè¦†ç›–èŒƒå›´
    print(f"\nCheckpointè¦†ç›–åˆ†æ (æ¯50æ­¥ä¿å­˜):")
    for ckpt_step in [50, 100, 125]:
        if ckpt_step <= total_steps:
            start_sample = (ckpt_step - 50) * batch_size if ckpt_step > 50 else 0
            end_sample = ckpt_step * batch_size - 1
            
            start_line = start_sample % dataset_size
            end_line = end_sample % dataset_size
            
            print(f"  Checkpoint-{ckpt_step}: samples[{start_sample}-{end_sample}] -> source_lines[{start_line}-{end_line}]")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="iCodingç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬")
    parser.add_argument("--dataset", type=str, required=True,
                       help="è¦è®­ç»ƒçš„æ•°æ®é›†åç§° (arc-challenge, arc-easy, boolq, hellaswag, openbookqa, piqa, winogrande)")
    parser.add_argument("--config", type=str, default="configs/training_config.yaml",
                       help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--track_batches", action="store_true",
                       help="å¯ç”¨è¯¦ç»†çš„batchè¿½è¸ªæ—¥å¿—")
    parser.add_argument("--dry_run", action="store_true",
                       help="å¹²è¿è¡Œï¼Œåˆ†ææ•°æ®åˆ†å¸ƒä½†ä¸å®é™…è®­ç»ƒ")
    parser.add_argument("--test_mode", action="store_true",
                       help="æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨æœ¬åœ°å¯æµ‹è¯•çš„batch size 4è¿›è¡Œå®Œæ•´çš„è¿½è¸ªéªŒè¯")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤32ï¼Œæµ‹è¯•æ¨¡å¼ä¸‹è‡ªåŠ¨è®¾ä¸º4)")
    
    args = parser.parse_args()
    
    # æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶è®¾ç½®å°batch size
    if args.test_mode:
        args.batch_size = 4
        args.track_batches = True  # æµ‹è¯•æ¨¡å¼è‡ªåŠ¨å¯ç”¨batchè¿½è¸ª
        print("ğŸ§ª å¯ç”¨æµ‹è¯•æ¨¡å¼: batch_size=4, å®Œæ•´è¿½è¸ªéªŒè¯")
    
    # éªŒè¯æ•°æ®é›†åç§°
    valid_datasets = ['arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'winogrande']
    if args.dataset not in valid_datasets:
        print(f"âŒ æ— æ•ˆçš„æ•°æ®é›†åç§°: {args.dataset}")
        print(f"âœ… å¯ç”¨æ•°æ®é›†: {', '.join(valid_datasets)}")
        return False
    
    print("ğŸš€ iCodingç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬")
    print("=" * 70)
    print(f"ç›®æ ‡æ•°æ®é›†: {args.dataset}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"Batchå¤§å°: {args.batch_size}")
    print(f"Batchè¿½è¸ª: {'å¯ç”¨' if args.track_batches else 'ç¦ç”¨'}")
    print(f"è¿è¡Œæ¨¡å¼: {'æµ‹è¯•æ¨¡å¼' if args.test_mode else 'Dry Run' if args.dry_run else 'å®é™…è®­ç»ƒ'}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    try:
        # åŠ è½½åŸºç¡€é…ç½®
        with open(args.config, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        
        # æ‰§è¡Œè®­ç»ƒ
        results = run_icoding_experiment(
            dataset_name=args.dataset,
            base_config=base_config,
            track_batches=args.track_batches,
            dry_run=args.dry_run,
            test_mode=args.test_mode,
            batch_size=args.batch_size
        )
        
        print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
        print(f"ğŸ“Š ç»“æœ: {results.get('status', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
