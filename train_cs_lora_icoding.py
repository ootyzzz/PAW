#!/usr/bin/env python3
"""
train_cs_lora_icoding.py
æœåŠ¡å™¨ç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬ - é’ˆå¯¹4Ã—H800ä¼˜åŒ–
æ”¯æŒç²¾ç¡®batchè¿½è¸ªå’Œprompt-checkpointé…å¯¹

======================================================================
ğŸš€ æœåŠ¡å™¨è¿è¡Œæ‰€æœ‰æ•°æ®é›†å‘½ä»¤ (batch_size=32, å®Œæ•´è®­ç»ƒ):
======================================================================

# è¿è¡Œæ‰€æœ‰7ä¸ªæ•°æ®é›†çš„å®Œæ•´è®­ç»ƒå‘½ä»¤:
python train_cs_lora_icoding.py --dataset arc-challenge --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset arc-easy --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset boolq --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset hellaswag --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset openbookqa --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset piqa --batch_size 32 --track_batches
python train_cs_lora_icoding.py --dataset winogrande --batch_size 32 --track_batches

# æˆ–è€…å¯ä»¥å†™æˆä¸€è¡Œè„šæœ¬:
for dataset in arc-challenge arc-easy boolq hellaswag openbookqa piqa winogrande; do python train_cs_lora_icoding.py --dataset $dataset --batch_size 32 --track_batches; done

======================================================================

é…ç½®ç‰¹ç‚¹:
- Batch size: 32 (æœåŠ¡å™¨) / 4 (æœ¬åœ°æµ‹è¯•)
- Training steps: 125 (æ­£å¸¸) / 20 (æµ‹è¯•)
- Checkpointé¢‘ç‡: æ¯50æ­¥ä¿å­˜
- æ•°æ®åŠ è½½: ä¸¥æ ¼é¡ºåºï¼Œå¯è¿½è¸ªæ¯ä¸ªbatchçš„sourceè¡Œå·
- çœŸå®è®­ç»ƒ: æœ¬åœ°å’ŒæœåŠ¡å™¨éƒ½æ‰§è¡ŒçœŸå®LoRAè®­ç»ƒ

ä½¿ç”¨ç¤ºä¾‹:
# æœåŠ¡å™¨ç¯å¢ƒ - batch size 32, å®Œæ•´125æ­¥è®­ç»ƒ
python train_cs_lora_icoding.py --dataset arc-challenge

# æœ¬åœ°æµ‹è¯• - batch size 4, 20æ­¥éªŒè¯è®­ç»ƒ
python train_cs_lora_icoding.py --dataset arc-challenge --test_mode

# è‡ªå®šä¹‰batch size
python train_cs_lora_icoding.py --dataset arc-challenge --batch_size 16

# å¯ç”¨è¯¦ç»†çš„batchè¿½è¸ªæ—¥å¿—
python train_cs_lora_icoding.py --dataset arc-challenge --track_batches

# å¹²è¿è¡ŒæŸ¥çœ‹æ•°æ®åˆ†å¸ƒ
python train_cs_lora_icoding.py --dataset arc-challenge --dry_run

å…³é”®ç‰¹æ€§:
1. ç»Ÿä¸€çš„çœŸå®LoRAè®­ç»ƒï¼šæœ¬åœ°æµ‹è¯•å’ŒæœåŠ¡å™¨éƒ¨ç½²ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒé€»è¾‘
2. å¯é…ç½®batch sizeï¼šæœåŠ¡å™¨ç”¨32ï¼Œæœ¬åœ°ç”¨4ï¼Œcheckpointæ ¼å¼å®Œå…¨ä¸€è‡´  
3. ç²¾ç¡®çš„batchè¿½è¸ªï¼šæ¯ä¸ªcheckpointéƒ½èƒ½è¿½æº¯åˆ°å…·ä½“çš„promptè¡Œå·
4. è‡ªåŠ¨å¤„ç†epochå¾ªç¯ï¼Œä¿æŒæ•°æ®ä¸€è‡´æ€§
"""

import os
import sys
import json
import yaml
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
import torch
from torch.utils.data import DataLoader, Dataset
from peft import TaskType


def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œä¿æŒå­—å…¸ç»“æ„ä¸è¢«PyTorché»˜è®¤å¤„ç†ç ´å"""
    return batch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

try:
    from scripts.experiment_manager_enhanced import ExperimentManager
    from scripts.model_manager import ModelManager
    from utils.data_processor import DataProcessor
    from core.train import LoRATrainer
    from lora.checkpoint_utils import CheckpointManager
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)


class SequentialTrackingDataset(Dataset):
    """ä¸¥æ ¼é¡ºåºçš„å¯è¿½è¸ªæ•°æ®é›†"""
    
    def __init__(self, data_file: str, dataset_name: str):
        self.data_file = data_file
        self.dataset_name = dataset_name
        self.data = self._load_data()
        self.total_samples = len(self.data)
        
    def _load_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®ï¼Œä¿æŒåŸå§‹é¡ºåº"""
        data = []
        if not os.path.exists(self.data_file):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            
        with open(self.data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        # æ·»åŠ åŸå§‹è¡Œå·ä¿¡æ¯
                        item['_source_line'] = line_idx
                        item['_dataset_name'] = self.dataset_name
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {line_idx}: {e}")
                        
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›† {self.dataset_name}: {len(data)} æ ·æœ¬")
        return data
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        # æ”¯æŒè¶…è¿‡æ•°æ®é›†é•¿åº¦çš„å¾ªç¯è®¿é—®
        actual_idx = idx % self.total_samples
        item = self.data[actual_idx].copy()
        
        # æ·»åŠ å¾ªç¯ä¿¡æ¯
        epoch_num = idx // self.total_samples
        item['_actual_idx'] = actual_idx
        item['_global_idx'] = idx
        item['_epoch'] = epoch_num
        
        return item


class BatchTracker:
    """Batchè¿½è¸ªå™¨ï¼Œè®°å½•æ¯ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯"""
    
    def __init__(self, dataset_name: str, log_dir: str):
        self.dataset_name = dataset_name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¿½è¸ªæ—¥å¿—
        self.batch_log_file = self.log_dir / f"batch_tracking_{dataset_name}.jsonl"
        self.checkpoint_map_file = self.log_dir / f"checkpoint_mapping_{dataset_name}.json"
        
        self.batch_records = []
        self.checkpoint_mappings = {}
        
    def log_batch(self, step: int, batch_data: List[Dict[str, Any]]):
        """è®°å½•å•ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯"""
        batch_info = {
            "step": step,
            "dataset": self.dataset_name,
            "batch_size": len(batch_data),
            "timestamp": datetime.now().isoformat(),
            "samples": []
        }
        
        # è®°å½•æ¯ä¸ªæ ·æœ¬çš„è¿½è¸ªä¿¡æ¯
        for sample in batch_data:
            sample_info = {
                "source_line": sample.get('_source_line', -1),
                "actual_idx": sample.get('_actual_idx', -1),
                "global_idx": sample.get('_global_idx', -1),
                "epoch": sample.get('_epoch', 0),
                "id": sample.get('id', 'unknown'),
                "input_preview": sample.get('input', '')[:100] + '...' if len(sample.get('input', '')) > 100 else sample.get('input', '')
            }
            batch_info["samples"].append(sample_info)
        
        # è®¡ç®—batchèŒƒå›´
        source_lines = [s['source_line'] for s in batch_info["samples"]]
        batch_info["source_range"] = {
            "min_line": min(source_lines) if source_lines else -1,
            "max_line": max(source_lines) if source_lines else -1,
            "epochs_involved": list(set(s['epoch'] for s in batch_info["samples"]))
        }
        
        self.batch_records.append(batch_info)
        
        # å®æ—¶å†™å…¥æ—¥å¿—
        with open(self.batch_log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(batch_info, ensure_ascii=False) + '\n')
    
    def log_checkpoint(self, step: int, checkpoint_path: str, batch_range: Tuple[int, int]):
        """è®°å½•checkpointä¸batchçš„æ˜ å°„å…³ç³»"""
        start_step, end_step = batch_range
        
        # æ‰¾åˆ°å¯¹åº”çš„batchè®°å½•
        related_batches = [
            record for record in self.batch_records 
            if start_step <= record["step"] <= end_step
        ]
        
        checkpoint_info = {
            "checkpoint_step": step,
            "checkpoint_path": checkpoint_path,
            "batch_range": {
                "start_step": start_step,
                "end_step": end_step
            },
            "total_batches": len(related_batches),
            "total_samples": sum(b["batch_size"] for b in related_batches),
            "source_data_summary": self._summarize_source_data(related_batches),
            "timestamp": datetime.now().isoformat()
        }
        
        self.checkpoint_mappings[f"checkpoint_{step}"] = checkpoint_info
        
        # ä¿å­˜mappingæ–‡ä»¶
        with open(self.checkpoint_map_file, 'w', encoding='utf-8') as f:
            json.dump(self.checkpoint_mappings, f, indent=2, ensure_ascii=False)
    
    def _summarize_source_data(self, batches: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ€»ç»“source dataä¿¡æ¯"""
        all_source_lines = []
        all_epochs = set()
        
        for batch in batches:
            for sample in batch["samples"]:
                all_source_lines.append(sample["source_line"])
                all_epochs.add(sample["epoch"])
        
        return {
            "total_source_lines": len(all_source_lines),
            "source_line_range": {
                "min": min(all_source_lines) if all_source_lines else -1,
                "max": max(all_source_lines) if all_source_lines else -1
            },
            "epochs_involved": sorted(list(all_epochs)),
            "unique_source_lines": len(set(all_source_lines))
        }


def create_icoding_config(dataset_name: str, base_config: Dict[str, Any], batch_size: int = 32) -> Dict[str, Any]:
    """åˆ›å»ºicodingç¯å¢ƒçš„è®­ç»ƒé…ç½®"""
    config = base_config.copy()
    
    # æ›´æ–°æ•°æ®è·¯å¾„
    config['data']['train_file'] = f"data_to_lora/cs/{dataset_name}/{dataset_name}_train_formatted.jsonl"
    
    # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºå®éªŒåç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"icoding_{timestamp}_lora"
    
    # icodingä¸“ç”¨é…ç½®
    config['training'].update({
        'per_device_train_batch_size': batch_size,  # å¯é…ç½®batch size
        'gradient_accumulation_steps': 1,
        'max_steps': 125,  # å›ºå®šæ­¥æ•°
        'save_steps': 50,  # checkpointé¢‘ç‡
        'logging_steps': 1,
        'dataloader_num_workers': 4,  # æœåŠ¡å™¨ä¼˜åŒ–
        'dataloader_pin_memory': True,
        'remove_unused_columns': False,  # ä¿ç•™è¿½è¸ªä¿¡æ¯
    })
    
    # ç¡®ä¿æ•°æ®åŠ è½½çš„ä¸€è‡´æ€§
    config['data'].update({
        'shuffle': False,  # ä¸¥æ ¼ç¦ç”¨shuffle
        'seed': 42,  # å›ºå®šç§å­
        'drop_last': False  # ä¿ç•™æœ€åçš„ä¸å®Œæ•´batch
    })
    
    # æ›´æ–°è¾“å‡ºç›®å½•
    config['training']['output_dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/models"
    config['checkpoint']['dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/checkpoints"
    config['logging']['log_dir'] = f"./experiments/icoding/{dataset_name}/{experiment_name}/logs"
    
    # å®éªŒä¿¡æ¯
    config['experiment']['name'] = experiment_name
    config['experiment']['description'] = f"iCoding LoRA training on {dataset_name} - batch_size={batch_size}, steps=125"
    config['experiment']['tags'] = ["icoding", "lora", "qwen2.5", dataset_name, f"batch{batch_size}", "trackable"]
    
    config['experiment_type'] = "icoding_lora"
    config['dataset_name'] = dataset_name
    
    return config


def run_icoding_experiment(dataset_name: str, base_config: Dict[str, Any], track_batches: bool = True, dry_run: bool = False, test_mode: bool = False, batch_size: int = 32) -> Dict[str, Any]:
    """è¿è¡Œicodingç¯å¢ƒçš„è®­ç»ƒå®éªŒ"""
    
    print(f"\n{'=' * 70}")
    print(f"ğŸš€ iCodingç¯å¢ƒè®­ç»ƒ: {dataset_name}")
    print(f"{'=' * 70}")
    
    # åˆ›å»ºé…ç½®
    config = create_icoding_config(dataset_name, base_config, batch_size)
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    data_file = config['data']['train_file']
    if not os.path.exists(data_file):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
    
    print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"ğŸ¯ è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: batch_size={batch_size}, steps=125, checkpoint_every=50")
    
    if dry_run:
        # åˆ†ææ•°æ®åˆ†å¸ƒ
        dataset = SequentialTrackingDataset(data_file, dataset_name)
        analyze_data_distribution(dataset, batch_size=batch_size, total_steps=125)
        return {"status": "dry_run_completed"}
    
    try:
        # è®¾ç½®æ—¥å¿—ç›®å½•
        log_dir = Path(config['logging']['log_dir'])
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨
        tracker = None
        if track_batches:
            tracker = BatchTracker(dataset_name, str(log_dir))
            print("ğŸ“ å¯ç”¨batchè¿½è¸ª")
        
        # åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
        dataset = SequentialTrackingDataset(data_file, dataset_name)
        
        # åˆ›å»ºDataLoader - å…³é”®é…ç½®ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,  # ä¸¥æ ¼ç¦ç”¨shuffle
            num_workers=0 if test_mode else 4,  # æµ‹è¯•æ¨¡å¼ä¸‹ä½¿ç”¨0ä¸ªworkeré¿å…å¤šè¿›ç¨‹é—®é¢˜
            pin_memory=False if test_mode else True,  # æµ‹è¯•æ¨¡å¼ä¸‹å…³é—­pin_memory
            drop_last=False,  # ä¿ç•™æœ€åçš„ä¸å®Œæ•´batch
            persistent_workers=False if test_mode else True,  # æµ‹è¯•æ¨¡å¼ä¸‹å…³é—­persistent workers
            collate_fn=custom_collate_fn  # ä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°ä¿æŒå­—å…¸ç»“æ„
        )
        
        print(f"ğŸ“Š æ•°æ®åŠ è½½å™¨é…ç½®:")
        print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")
        print(f"  - Batchå¤§å°: {batch_size}")
        print(f"  - æ€»batchæ•°: {len(dataloader)}")
        print(f"  - Shuffle: False (ä¸¥æ ¼é¡ºåº)")
        
        # æ‰§è¡ŒçœŸå®LoRAè®­ç»ƒ
        if test_mode:
            # æµ‹è¯•æ¨¡å¼ï¼šæœ¬åœ°éªŒè¯ï¼Œä½¿ç”¨è¾ƒå°‘æ­¥æ•°è§‚å¯Ÿæ•ˆæœ
            config['training']['max_steps'] = 20  # æµ‹è¯•æ¨¡å¼æ­¥æ•°
            checkpoint_steps = [10, 20]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: è®­ç»ƒ{config['training']['max_steps']}æ­¥ç”¨äºéªŒè¯")
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šå®Œæ•´è®­ç»ƒ125æ­¥
            checkpoint_steps = [50, 100, 125]
            print(f"ğŸš€ æ­£å¸¸æ¨¡å¼: è®­ç»ƒ{config['training']['max_steps']}æ­¥")
        
        # ç»Ÿä¸€ä½¿ç”¨çœŸå®LoRAè®­ç»ƒ
        results = run_actual_lora_training(
            dataloader=dataloader,
            config=config,
            tracker=tracker,
            checkpoint_steps=checkpoint_steps
        )
        
        # ç”Ÿæˆline-to-checkpointæ˜ å°„æ–‡ä»¶
        if tracker:
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†TrackingLoRATrainerï¼ˆæœ‰è‡ªå·±çš„æ˜ å°„æ–‡ä»¶ï¼‰
            tracking_mapping_file = Path(results.get("output_dir", "")) / "line_to_checkpoint_mapping.json"
            
            if tracking_mapping_file.exists():
                print(f"ğŸ“ å‘ç°TrackingLoRATraineræ˜ å°„æ–‡ä»¶: {tracking_mapping_file}")
                
                # å¤åˆ¶æ˜ å°„æ–‡ä»¶åˆ°logsç›®å½•ä»¥ä¿æŒä¸€è‡´æ€§
                log_mapping_file = log_dir / f"line_checkpoint_mapping_{dataset_name}.json"
                
                # è¯»å–TrackingLoRATrainerçš„æ˜ å°„æ•°æ®
                with open(tracking_mapping_file, 'r', encoding='utf-8') as f:
                    tracking_data = json.load(f)
                
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å¹¶ä¿å­˜åˆ°logsç›®å½•
                standard_mapping = {
                    "dataset": dataset_name,
                    "generated_at": tracking_data["generation_info"]["timestamp"],
                    "summary": {
                        "total_checkpoints": tracking_data["generation_info"]["total_checkpoints"],
                        "total_unique_lines": tracking_data["generation_info"]["total_tracked_lines"],
                        "total_line_checkpoint_pairs": tracking_data["generation_info"]["total_tracked_lines"]
                    },
                    "line_to_checkpoint": tracking_data["line_to_checkpoint"],
                    "checkpoint_to_lines": tracking_data["checkpoint_to_lines"]
                }
                
                with open(log_mapping_file, 'w', encoding='utf-8') as f:
                    json.dump(standard_mapping, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… æ˜ å°„æ–‡ä»¶å·²åŒæ­¥åˆ°logsç›®å½•: {log_mapping_file}")
                print(f"ğŸ“Š æ˜ å°„ç»Ÿè®¡:")
                print(f"  - æ€»checkpointæ•°: {standard_mapping['summary']['total_checkpoints']}")
                print(f"  - æ€»è¡Œæ•°: {standard_mapping['summary']['total_unique_lines']}")
                print(f"  - è¡Œ-checkpointå¯¹æ•°: {standard_mapping['summary']['total_line_checkpoint_pairs']}")
            else:
                # å›é€€åˆ°ä¼ ç»Ÿçš„æ˜ å°„ç”Ÿæˆæ–¹æ³•
                print("ğŸ“‹ ä½¿ç”¨ä¼ ç»ŸBatchTrackerç”Ÿæˆæ˜ å°„æ–‡ä»¶...")
                generate_line_to_checkpoint_mapping(tracker, str(log_dir))
        
        return results
        
    except Exception as e:
        print(f"âŒ {dataset_name} è®­ç»ƒå¤±è´¥: {e}")
        raise


def run_actual_lora_training(
    dataloader: DataLoader,
    config: Dict[str, Any],
    tracker: Optional[BatchTracker],
    checkpoint_steps: List[int]
) -> Dict[str, Any]:
    """è¿è¡Œå®é™…çš„LoRAè®­ç»ƒ"""
    
    print(f"\nğŸš€ å¼€å§‹LoRAè®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  - æ¨¡å‹: {config.get('model', {}).get('name', 'Qwen2.5-0.5B')}")
    print(f"  - æœ€å¤§æ­¥æ•°: {config['training']['max_steps']}")
    print(f"  - Batchå¤§å°: {config['training']['per_device_train_batch_size']}")
    print(f"  - ä¿å­˜æ­¥éª¤: {checkpoint_steps}")
    print(f"  - è¾“å‡ºç›®å½•: {config['training']['output_dir']}")
    
    try:
        # å‡†å¤‡LoRAè®­ç»ƒå™¨å‚æ•°
        model_path = config.get('model', {}).get('path', 'models/Qwen-Qwen2.5-0.5B')
        if not os.path.exists(model_path):
            # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹åç§°
            model_name = config.get('model', {}).get('name', 'Qwen/Qwen2.5-0.5B')
            model_path = model_name
            
        data_path = config['data']['train_file']
        output_dir = config['training']['output_dir']
        
        # LoRAé…ç½®
        lora_config = {
            'r': config.get('lora', {}).get('r', 16),
            'lora_alpha': config.get('lora', {}).get('alpha', 32),
            'target_modules': config.get('lora', {}).get('target_modules', ["q_proj", "v_proj"]),
            'lora_dropout': config.get('lora', {}).get('dropout', 0.1),
            'bias': config.get('lora', {}).get('bias', "none"),
            'task_type': TaskType.CAUSAL_LM
        }
        
        print(f"ğŸ“Š LoRAé…ç½®:")
        print(f"  - r: {lora_config['r']}")
        print(f"  - alpha: {lora_config['lora_alpha']}")
        print(f"  - target_modules: {lora_config['target_modules']}")
        
        # åˆ›å»ºLoRAè®­ç»ƒå™¨
        trainer = LoRATrainer(
            model_path=model_path,
            data_path=data_path,
            output_dir=output_dir,
            lora_config=lora_config
        )
        
        # ä½¿ç”¨tracking wrapperï¼ˆå¦‚æœéœ€è¦batchè¿½è¸ªï¼‰
        if tracker:
            print("ğŸ“ å¯ç”¨å®Œæ•´çš„batchè¿½è¸ªåŠŸèƒ½")
            
            from lora_trainer_wrapper import TrackingLoRATrainer
            
            # åˆ›å»ºè¿½è¸ªwrapper
            tracking_trainer = TrackingLoRATrainer(trainer, output_dir)
            
            # ä½¿ç”¨wrapperæ‰§è¡Œè®­ç»ƒ
            return tracking_trainer.train_with_tracking(dataloader, config, checkpoint_steps)
        
        # å¼€å§‹è®­ç»ƒï¼ˆæ— è¿½è¸ªæ¨¡å¼ï¼‰
        print(f"ğŸƒâ€â™‚ï¸ å¼€å§‹æ‰§è¡Œæ ‡å‡†LoRAè®­ç»ƒ...")
        
        # è°ƒç”¨è®­ç»ƒï¼Œä¼ å…¥è®­ç»ƒå‚æ•°
        training_result = trainer.train(
            batch_size=config['training']['per_device_train_batch_size'],
            max_length=config.get('data', {}).get('max_length', 512),
            gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
            warmup_steps=0,
            logging_steps=config['training'].get('logging_steps', 1),
            save_steps=config['training'].get('save_steps', 50)
        )
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨å¹¶ä¿å­˜æ¨¡å‹
        output_dir = Path(config['training']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if hasattr(trainer, 'save_model'):
            trainer.save_model(str(output_dir))
        
        results = {
            "status": "training_completed",
            "output_dir": str(output_dir),
            "total_steps": config['training']['max_steps'],
            "checkpoint_steps": checkpoint_steps,
            "model_saved": True,
            "training_result": training_result
        }
        
        print(f"\nâœ… LoRAè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {output_dir}")
        print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {config['training']['max_steps']}")
        print(f"ğŸ’¾ Checkpointä¿å­˜: {len(checkpoint_steps)}ä¸ª")
        
        return results
        
    except Exception as e:
        print(f"âŒ LoRAè®­ç»ƒå¤±è´¥: {e}")
        print(f"ğŸ’¡ é”™è¯¯è¯¦æƒ…: {str(e)}")
        
        # è®°å½•é”™è¯¯ä½†ä¸å›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
        results = {
            "status": "training_failed",
            "error": str(e),
            "output_dir": config['training']['output_dir'],
            "total_steps": config['training']['max_steps']
        }
        
        raise Exception(f"LoRAè®­ç»ƒå¤±è´¥: {e}")


def generate_line_to_checkpoint_mapping(tracker: BatchTracker, log_dir: str):
    """ç”Ÿæˆline-to-checkpointçš„å®Œæ•´æ˜ å°„æ–‡ä»¶"""
    
    print(f"\nğŸ“‹ ç”Ÿæˆline-to-checkpointæ˜ å°„æ–‡ä»¶...")
    
    # æ„å»ºå®Œæ•´çš„æ˜ å°„å…³ç³»
    line_to_checkpoint = {}
    checkpoint_to_lines = {}
    
    # éå†æ‰€æœ‰checkpointæ˜ å°„
    for ckpt_name, ckpt_info in tracker.checkpoint_mappings.items():
        checkpoint_step = ckpt_info['checkpoint_step']
        start_step = ckpt_info['batch_range']['start_step']
        end_step = ckpt_info['batch_range']['end_step']
        
        # æ‰¾åˆ°å¯¹åº”æ­¥éª¤çš„batchè®°å½•
        related_batches = [
            record for record in tracker.batch_records
            if start_step <= record["step"] <= end_step
        ]
        
        lines_in_checkpoint = []
        
        for batch_record in related_batches:
            for sample in batch_record["samples"]:
                source_line = sample["source_line"]
                lines_in_checkpoint.append(source_line)
                
                # è®°å½•lineåˆ°checkpointçš„æ˜ å°„
                if source_line not in line_to_checkpoint:
                    line_to_checkpoint[source_line] = []
                line_to_checkpoint[source_line].append({
                    "checkpoint": ckpt_name,
                    "checkpoint_step": checkpoint_step,
                    "step": batch_record["step"],
                    "sample_id": sample["id"]
                })
        
        # è®°å½•checkpointåˆ°linesçš„æ˜ å°„
        checkpoint_to_lines[ckpt_name] = {
            "checkpoint_step": checkpoint_step,
            "lines": sorted(list(set(lines_in_checkpoint))),
            "line_count": len(set(lines_in_checkpoint)),
            "total_samples": len(lines_in_checkpoint),
            "step_range": [start_step, end_step]
        }
    
    # ä¿å­˜æ˜ å°„æ–‡ä»¶
    mapping_data = {
        "dataset": tracker.dataset_name,
        "generated_at": datetime.now().isoformat(),
        "summary": {
            "total_checkpoints": len(checkpoint_to_lines),
            "total_unique_lines": len(line_to_checkpoint),
            "total_line_checkpoint_pairs": sum(len(mappings) for mappings in line_to_checkpoint.values())
        },
        "line_to_checkpoint": line_to_checkpoint,
        "checkpoint_to_lines": checkpoint_to_lines
    }
    
    # ä¿å­˜è¯¦ç»†æ˜ å°„
    mapping_file = Path(log_dir) / f"line_checkpoint_mapping_{tracker.dataset_name}.json"
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping_data, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆç®€åŒ–çš„æ¦‚è§ˆæ–‡ä»¶
    summary_data = {
        "dataset": tracker.dataset_name,
        "generated_at": datetime.now().isoformat(),
        "checkpoints": []
    }
    
    for ckpt_name, info in checkpoint_to_lines.items():
        summary_data["checkpoints"].append({
            "name": ckpt_name,
            "step": info["checkpoint_step"],
            "line_range": [min(info["lines"]), max(info["lines"])] if info["lines"] else [0, 0],
            "unique_lines": info["line_count"],
            "total_samples": info["total_samples"]
        })
    
    summary_file = Path(log_dir) / f"checkpoint_summary_{tracker.dataset_name}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ˜ å°„æ–‡ä»¶å·²ç”Ÿæˆ:")
    print(f"  - è¯¦ç»†æ˜ å°„: {mapping_file}")
    print(f"  - æ¦‚è§ˆæ–‡ä»¶: {summary_file}")
    print(f"ğŸ“Š æ˜ å°„ç»Ÿè®¡:")
    print(f"  - æ€»checkpointæ•°: {len(checkpoint_to_lines)}")
    print(f"  - æ€»è¡Œæ•°: {len(line_to_checkpoint)}")
    print(f"  - è¡Œ-checkpointå¯¹æ•°: {sum(len(mappings) for mappings in line_to_checkpoint.values())}")


def analyze_data_distribution(dataset: SequentialTrackingDataset, batch_size: int, total_steps: int):
    """åˆ†ææ•°æ®åˆ†å¸ƒæƒ…å†µ"""
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
    print(f"{'=' * 50}")
    
    total_samples_needed = batch_size * total_steps
    dataset_size = len(dataset)
    epochs_needed = total_samples_needed / dataset_size
    
    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  - æ•°æ®é›†åç§°: {dataset.dataset_name}")
    print(f"  - æ ·æœ¬æ€»æ•°: {dataset_size}")
    print(f"  - æ•°æ®æ–‡ä»¶: {dataset.data_file}")
    
    print(f"\nè®­ç»ƒéœ€æ±‚:")
    print(f"  - Batchå¤§å°: {batch_size}")
    print(f"  - è®­ç»ƒæ­¥æ•°: {total_steps}")
    print(f"  - éœ€è¦æ ·æœ¬æ•°: {total_samples_needed}")
    print(f"  - éœ€è¦å¾ªç¯: {epochs_needed:.2f} epochs")
    
    print(f"\nBatchåˆ†å¸ƒé¢„æµ‹:")
    for step in range(min(10, total_steps)):  # æ˜¾ç¤ºå‰10ä¸ªbatch
        start_idx = step * batch_size
        end_idx = min(start_idx + batch_size, total_samples_needed)
        
        batch_samples = []
        for idx in range(start_idx, end_idx):
            actual_idx = idx % dataset_size
            epoch = idx // dataset_size
            sample = dataset[idx]
            batch_samples.append((actual_idx, epoch, sample['_source_line']))
        
        source_lines = [s[2] for s in batch_samples]
        epochs = list(set(s[1] for s in batch_samples))
        
        print(f"  Step {step:2d}: source_lines=[{min(source_lines):3d}-{max(source_lines):3d}], epochs={epochs}")
    
    if total_steps > 10:
        print(f"  ... (çœç•¥ä¸­é—´ {total_steps - 10} æ­¥)")
    
    # åˆ†æcheckpointè¦†ç›–èŒƒå›´
    print(f"\nCheckpointè¦†ç›–åˆ†æ (æ¯50æ­¥ä¿å­˜):")
    for ckpt_step in [50, 100, 125]:
        if ckpt_step <= total_steps:
            start_sample = (ckpt_step - 50) * batch_size if ckpt_step > 50 else 0
            end_sample = ckpt_step * batch_size - 1
            
            start_line = start_sample % dataset_size
            end_line = end_sample % dataset_size
            
            print(f"  Checkpoint-{ckpt_step}: samples[{start_sample}-{end_sample}] -> source_lines[{start_line}-{end_line}]")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="iCodingç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬")
    parser.add_argument("--dataset", type=str, required=True,
                       help="è¦è®­ç»ƒçš„æ•°æ®é›†åç§° (arc-challenge, arc-easy, boolq, hellaswag, openbookqa, piqa, winogrande)")
    parser.add_argument("--config", type=str, default="configs/training_config.yaml",
                       help="åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--track_batches", action="store_true",
                       help="å¯ç”¨è¯¦ç»†çš„batchè¿½è¸ªæ—¥å¿—")
    parser.add_argument("--dry_run", action="store_true",
                       help="å¹²è¿è¡Œï¼Œåˆ†ææ•°æ®åˆ†å¸ƒä½†ä¸å®é™…è®­ç»ƒ")
    parser.add_argument("--test_mode", action="store_true",
                       help="æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨batch size 4è¿›è¡Œè¾ƒå°‘æ­¥æ•°çš„è®­ç»ƒéªŒè¯ï¼ˆ20æ­¥vs125æ­¥ï¼‰")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤32é€‚åˆæœåŠ¡å™¨ï¼Œæœ¬åœ°æµ‹è¯•å»ºè®®4)")
    
    args = parser.parse_args()
    
    # æµ‹è¯•æ¨¡å¼ä¸‹å¼ºåˆ¶è®¾ç½®å°batch size
    if args.test_mode:
        args.batch_size = 4
        args.track_batches = True  # æµ‹è¯•æ¨¡å¼è‡ªåŠ¨å¯ç”¨batchè¿½è¸ª
        print("ğŸ§ª å¯ç”¨æµ‹è¯•æ¨¡å¼: batch_size=4, è®­ç»ƒ20æ­¥ç”¨äºéªŒè¯")
    
    # éªŒè¯æ•°æ®é›†åç§°
    valid_datasets = ['arc-challenge', 'arc-easy', 'boolq', 'hellaswag', 'openbookqa', 'piqa', 'winogrande']
    if args.dataset not in valid_datasets:
        print(f"âŒ æ— æ•ˆçš„æ•°æ®é›†åç§°: {args.dataset}")
        print(f"âœ… å¯ç”¨æ•°æ®é›†: {', '.join(valid_datasets)}")
        return False
    
    print("ğŸš€ iCodingç¯å¢ƒLoRAè®­ç»ƒè„šæœ¬")
    print("=" * 70)
    print(f"ç›®æ ‡æ•°æ®é›†: {args.dataset}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"Batchå¤§å°: {args.batch_size}")
    print(f"Batchè¿½è¸ª: {'å¯ç”¨' if args.track_batches else 'ç¦ç”¨'}")
    print(f"è¿è¡Œæ¨¡å¼: {'æµ‹è¯•æ¨¡å¼(20æ­¥)' if args.test_mode else 'Dry Run' if args.dry_run else 'å®Œæ•´è®­ç»ƒ(125æ­¥)'}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    try:
        # åŠ è½½åŸºç¡€é…ç½®
        with open(args.config, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        
        # æ‰§è¡Œè®­ç»ƒ
        results = run_icoding_experiment(
            dataset_name=args.dataset,
            base_config=base_config,
            track_batches=args.track_batches,
            dry_run=args.dry_run,
            test_mode=args.test_mode,
            batch_size=args.batch_size
        )
        
        print(f"\nğŸ‰ å®éªŒå®Œæˆ!")
        print(f"ğŸ“Š ç»“æœ: {results.get('status', 'unknown')}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
