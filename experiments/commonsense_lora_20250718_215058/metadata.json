{
  "name": "commonsense_lora_20250718_215058",
  "description": "Two-stage LoRA training on Qwen2.5-0.5B with commonsense dataset",
  "tags": [
    "lora",
    "commonsense",
    "qwen2.5"
  ],
  "created_at": "2025-07-18T21:51:00.048615",
  "status": "failed",
  "config": {
    "experiment_type": "commonsense_lora",
    "model_path": "C:/Users/feifa/GitHub/P2W/models/Qwen-Qwen2.5-0.5B",
    "data_path": "C:/Users/feifa/GitHub/P2W/raw_datasets/commonsense/cs_all_unbalanced.jsonl",
    "training": {
      "experiment_type": "commonsense_lora",
      "output_dir": "./experiments/commonsense_lora_qwen25/models",
      "stage1": {
        "steps": 75,
        "learning_rate": 0.0001,
        "save_checkpoints": false,
        "description": "High learning rate stage"
      },
      "stage2": {
        "steps": 50,
        "learning_rate": 1e-05,
        "save_checkpoints": true,
        "save_every_step": true,
        "description": "Low learning rate stage with checkpointing"
      },
      "per_device_train_batch_size": 4,
      "per_device_eval_batch_size": 4,
      "gradient_accumulation_steps": 1,
      "weight_decay": 0.01,
      "warmup_steps": 0,
      "logging_steps": 1,
      "save_total_limit": 50,
      "remove_unused_columns": false,
      "fp16": true,
      "gradient_checkpointing": true,
      "dataloader_num_workers": 0,
      "optimizer": "adamw",
      "adam_beta1": 0.9,
      "adam_beta2": 0.999,
      "adam_epsilon": 1e-08,
      "max_grad_norm": 1.0
    },
    "lora": {
      "r": 16,
      "alpha": 32,
      "dropout": 0.1,
      "target_modules": [
        "q_proj",
        "v_proj",
        "k_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
      ],
      "bias": "none",
      "task_type": "CAUSAL_LM"
    },
    "hardware": {
      "use_cuda": true,
      "mixed_precision": true,
      "compile_model": false
    },
    "timestamp": "2025-07-18T21:51:00.045122"
  },
  "type": "commonsense_lora",
  "updated_at": "2025-07-18T21:51:00.421165"
}