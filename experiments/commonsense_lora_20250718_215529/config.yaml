data_path: C:/Users/feifa/GitHub/P2W/raw_datasets/commonsense/cs_all_unbalanced.jsonl
experiment_type: commonsense_lora
hardware:
  compile_model: false
  mixed_precision: true
  use_cuda: true
lora:
  bias: none
  lora_alpha: 32
  lora_dropout: 0.1
  r: 16
  target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  task_type: CAUSAL_LM
model_path: C:/Users/feifa/GitHub/P2W/models/Qwen-Qwen2.5-0.5B
timestamp: '2025-07-18T21:55:31.144014'
training:
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-08
  dataloader_num_workers: 0
  experiment_type: commonsense_lora
  fp16: true
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  logging_steps: 1
  max_grad_norm: 1.0
  optimizer: adamw
  output_dir: ./experiments/commonsense_lora_qwen25/models
  per_device_eval_batch_size: 4
  per_device_train_batch_size: 4
  remove_unused_columns: false
  save_total_limit: 50
  stage1:
    description: High learning rate stage
    learning_rate: 0.0001
    save_checkpoints: false
    steps: 75
  stage2:
    description: Low learning rate stage with checkpointing
    learning_rate: 1.0e-05
    save_checkpoints: true
    save_every_step: true
    steps: 50
  warmup_steps: 0
  weight_decay: 0.01
