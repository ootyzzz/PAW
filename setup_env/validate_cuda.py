import torch
import platform
import subprocess
import re
import sys
import os

def check_environment_consistency():
    """å¿«é€Ÿæ£€æŸ¥ç¯å¢ƒä¸€è‡´æ€§"""
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if not conda_env:
        return False, "æœªæ¿€æ´»condaç¯å¢ƒ"
    
    # æ£€æŸ¥Pythonè·¯å¾„
    expected_path = f"C:\\Users\\feifa\\.conda\\envs\\{conda_env}\\python.exe"
    if sys.executable.lower() != expected_path.lower():
        return False, f"Pythonè·¯å¾„ä¸åŒ¹é…ï¼Œå½“å‰: {sys.executable}"
    
    # æ£€æŸ¥å‘½ä»¤è¡Œä¸€è‡´æ€§
    try:
        cmd_version = subprocess.check_output("python --version", shell=True).decode("utf-8").strip()
        script_version = f"Python {platform.python_version()}"
        if cmd_version != script_version:
            return False, f"ç‰ˆæœ¬ä¸ä¸€è‡´: å‘½ä»¤è¡Œ({cmd_version}) vs è„šæœ¬({script_version})"
    except:
        return False, "æ— æ³•æ£€æŸ¥å‘½ä»¤è¡Œpythonç‰ˆæœ¬"
    
    return True, "ç¯å¢ƒä¸€è‡´"

def check_cuda_compatibility():
    """æ£€æŸ¥RTX 5060 GPUçš„CUDAå…¼å®¹æ€§å’Œå¯ç”¨æ€§"""
    print("===== RTX 5060 CUDAç¯å¢ƒéªŒè¯è„šæœ¬ v2.0 =====")
    
    # ç¯å¢ƒä¸€è‡´æ€§å¿«é€Ÿæ£€æŸ¥
    print("ã€ç¯å¢ƒä¸€è‡´æ€§æ£€æŸ¥ã€‘")
    is_consistent, message = check_environment_consistency()
    if is_consistent:
        print(f"âœ… {message}")
    else:
        print(f"âŒ {message}")
        print("ğŸ’¡ å»ºè®®: conda deactivate && conda activate cuda312")
        print("=" * 50)
        return  # ç¯å¢ƒæœ‰é—®é¢˜å°±ç›´æ¥è¿”å›
    
    # ç³»ç»Ÿä¿¡æ¯
    print(f"ç³»ç»Ÿä¿¡æ¯: {platform.system()} {platform.version()} ({platform.machine()})")
    print("=" * 50)
    
    # Pythonç¯å¢ƒè¯¦ç»†ä¿¡æ¯
    print("ã€Pythonç¯å¢ƒä¿¡æ¯ã€‘")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"Pythonå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„: {sys.executable}")
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_env = os.environ.get('CONDA_DEFAULT_ENV')
    if conda_env:
        print(f"å½“å‰Condaç¯å¢ƒ: {conda_env}")
        
        # æ£€æŸ¥ç¯å¢ƒä¸€è‡´æ€§
        expected_path = f"C:\\Users\\feifa\\.conda\\envs\\{conda_env}\\python.exe"
        if sys.executable.lower() != expected_path.lower():
            print(f"âš ï¸  è­¦å‘Š: Pythonè·¯å¾„ä¸condaç¯å¢ƒä¸åŒ¹é…!")
            print(f"    æœŸæœ›è·¯å¾„: {expected_path}")
            print(f"    å®é™…è·¯å¾„: {sys.executable}")
            print(f"    å»ºè®®: è¿è¡Œ 'conda deactivate && conda activate {conda_env}' é‡æ–°æ¿€æ´»ç¯å¢ƒ")
        else:
            print("âœ… Pythonè·¯å¾„ä¸condaç¯å¢ƒåŒ¹é…")
    else:
        print("å½“å‰ç¯å¢ƒ: ç³»ç»ŸPython (éCondaç¯å¢ƒ)")
    
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
    venv_path = os.environ.get('VIRTUAL_ENV')
    if venv_path:
        print(f"è™šæ‹Ÿç¯å¢ƒè·¯å¾„: {venv_path}")
    
    # æ˜¾ç¤ºPythonè·¯å¾„å’Œå‘½ä»¤è¡Œæ£€æŸ¥
    print(f"Pythonåº“è·¯å¾„: {sys.path[0] if sys.path else 'æœªçŸ¥'}")
    
    # æ£€æŸ¥å‘½ä»¤è¡Œpythonç‰ˆæœ¬ä¸€è‡´æ€§
    try:
        cmd_python_version = subprocess.check_output("python --version", shell=True).decode("utf-8").strip()
        script_python_version = f"Python {platform.python_version()}"
        
        print(f"å‘½ä»¤è¡Œpythonç‰ˆæœ¬: {cmd_python_version}")
        print(f"è„šæœ¬è¿è¡Œpythonç‰ˆæœ¬: {script_python_version}")
        
        if cmd_python_version != script_python_version:
            print("âš ï¸  è­¦å‘Š: å‘½ä»¤è¡Œpythonç‰ˆæœ¬ä¸è„šæœ¬è¿è¡Œç‰ˆæœ¬ä¸ä¸€è‡´!")
            print("    è¿™å¯èƒ½å¯¼è‡´ç¯å¢ƒé…ç½®é—®é¢˜ï¼Œå»ºè®®é‡æ–°æ¿€æ´»condaç¯å¢ƒ")
        else:
            print("âœ… å‘½ä»¤è¡Œpythonç‰ˆæœ¬ä¸è„šæœ¬è¿è¡Œç‰ˆæœ¬ä¸€è‡´")
    except Exception as e:
        print(f"æ— æ³•æ£€æŸ¥å‘½ä»¤è¡Œpythonç‰ˆæœ¬: {e}")
    print("=" * 50)
    
    # è·å–NVIDIAé©±åŠ¨ä¿¡æ¯
    print("ã€NVIDIAé©±åŠ¨ä¿¡æ¯ã€‘")
    try:
        if platform.system() == "Windows":
            result = subprocess.check_output("nvidia-smi", shell=True).decode("utf-8")
        else:
            result = subprocess.check_output(["nvidia-smi", "--query-gpu=driver_version,compute_cap", "--format=csv,noheader"]).decode("utf-8")
        
        driver_version = re.search(r"Driver Version: (\d+\.\d+)", result)
        if driver_version:
            driver_version = driver_version.group(1)
            print(f"é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
            
            # æå–CUDAç‰ˆæœ¬
            cuda_version_match = re.search(r"CUDA Version: (\d+\.\d+)", result)
            cuda_version = cuda_version_match.group(1) if cuda_version_match else "æœªçŸ¥"
            print(f"æ”¯æŒçš„æœ€é«˜CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            print("æ— æ³•è·å–é©±åŠ¨ç‰ˆæœ¬ä¿¡æ¯")
    except Exception as e:
        print(f"è·å–é©±åŠ¨ä¿¡æ¯å¤±è´¥: {e}")
    
    # æ‰“å°GPUåˆ—è¡¨
    try:
        gpu_info = subprocess.check_output("nvidia-smi --query-gpu=name,uuid,memory.total --format=csv,noheader,nounits", shell=True).decode("utf-8").strip()
        print("GPUåˆ—è¡¨:")
        gpu_list = gpu_info.split('\n')
        for i, gpu in enumerate(gpu_list):
            name, uuid, memory = gpu.strip().split(', ')
            print(f"  - GPU{i}: {name} | æ˜¾å­˜: {memory} MiB | UUID: {uuid}")
    except:
        print("  - æ— æ³•è·å–GPUè¯¦ç»†ä¿¡æ¯")
    print("=" * 50)
    
    # CUDAå·¥å…·åŒ…ç‰ˆæœ¬
    print("ã€CUDAå·¥å…·åŒ…ç‰ˆæœ¬ã€‘")
    try:
        nvcc_output = subprocess.check_output("nvcc --version", shell=True).decode("utf-8")
        cuda_version = re.search(r"release (\d+\.\d+)", nvcc_output).group(1)
        print(f"release {cuda_version}")
    except Exception as e:
        print(f"æ— æ³•è·å–CUDAå·¥å…·åŒ…ç‰ˆæœ¬: {e}")
    print("=" * 50)
    
    # PyTorch CUDAæ”¯æŒéªŒè¯
    print("ã€PyTorch CUDAæ”¯æŒéªŒè¯ã€‘")
    try:
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"PyTorchç¼–è¯‘æ—¶çš„CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
            
            # æ£€æŸ¥æ¯ä¸ªGPU
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
                compute_capability = torch.cuda.get_device_properties(i).major * 10 + torch.cuda.get_device_properties(i).minor
                
                print(f"GPU {i} è¯¦æƒ…:")
                print(f"  - åç§°: {gpu_name}")
                print(f"  - æ˜¾å­˜: {gpu_memory:.2f} GB")
                print(f"  - è®¡ç®—èƒ½åŠ›: sm_{compute_capability}")
                
                # æ£€æŸ¥RTX 5060ç‰¹å®šä¿¡æ¯
                if "RTX 5060" in gpu_name:
                    print("  - âœ… æ£€æµ‹åˆ°RTX 5060 GPU")
                    
                    # éªŒè¯CUDAåŠŸèƒ½
                    try:
                        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¼ é‡å¹¶åœ¨GPUä¸Šæ‰§è¡Œæ“ä½œ
                        x = torch.tensor([1.0], device=f"cuda:{i}")
                        y = torch.tensor([2.0], device=f"cuda:{i}")
                        z = x + y
                        
                        # æ£€æŸ¥è®¡ç®—ç»“æœ
                        if z.item() == 3.0:
                            print("  - âœ… CUDAè®¡ç®—éªŒè¯æˆåŠŸ")
                        else:
                            print(f"  - âŒ CUDAè®¡ç®—ç»“æœå¼‚å¸¸: {z.item()} (é¢„æœŸ3.0)")
                    except Exception as e:
                        print(f"  - âŒ GPUè¿ç®—å¤±è´¥: {e}")
                        print("    æç¤º: å°è¯•è®¾ç½®ç¯å¢ƒå˜é‡ CUDA_LAUNCH_BLOCKING=1 ä»¥è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯")
    except Exception as e:
        print(f"éªŒè¯PyTorch CUDAæ”¯æŒæ—¶å‡ºé”™: {e}")
    print("=" * 50)
    
    # ç»“è®º
    if torch.cuda.is_available():
        print("ã€éªŒè¯å®Œæˆã€‘")
        print("ç»“è®º: CUDAç¯å¢ƒæ­£å¸¸ï¼ŒGPUåŠ é€Ÿå¯ç”¨")
    else:
        print("ã€éªŒè¯å®Œæˆã€‘")
        print("ç»“è®º: CUDAç¯å¢ƒé…ç½®å­˜åœ¨é—®é¢˜ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
        print("å»ºè®®: æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦ä¸CUDAå·¥å…·åŒ…å…¼å®¹ï¼Œæˆ–æ›´æ–°NVIDIAé©±åŠ¨")

if __name__ == "__main__":
    check_cuda_compatibility()