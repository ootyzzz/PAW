"""
LoRAè®­ç»ƒå™¨åŒ…è£…å™¨ï¼Œç”¨äºæ·»åŠ batchè¿½è¸ªåŠŸèƒ½
"""

import json
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime

from core.train import LoRATrainer


@dataclass
class BatchTrackingInfo:
    """Batchè¿½è¸ªä¿¡æ¯"""
    step: int
    epoch: int
    batch_size: int
    sample_lines: List[int]
    timestamp: str
    

class TrackingLoRATrainer:
    """å¸¦æœ‰batchè¿½è¸ªåŠŸèƒ½çš„LoRAè®­ç»ƒå™¨åŒ…è£…å™¨"""
    
    def __init__(self, trainer: LoRATrainer, output_dir: str):
        self.trainer = trainer
        self.output_dir = Path(output_dir)
        self.tracking_log_file = self.output_dir / "batch_tracking.jsonl"
        self.checkpoint_mappings = {}
        self.batch_records = []
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def track_batch(self, step: int, batch_data: Dict[str, Any]) -> None:
        """è®°å½•å•ä¸ªbatchçš„ä¿¡æ¯"""
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥batchæ•°æ®ç»“æ„
            if step == 0:  # åªåœ¨ç¬¬ä¸€æ­¥æ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” è°ƒè¯•batchç»“æ„: type={type(batch_data)}, keys={list(batch_data.keys()) if hasattr(batch_data, 'keys') else 'N/A'}")
            
            # æå–sampleä¿¡æ¯
            samples = []
            if isinstance(batch_data, list):
                for i, sample in enumerate(batch_data):
                    sample_info = {
                        "batch_index": i,
                        "source_line": sample.get('_source_line', -1),
                        "epoch": sample.get('_epoch', -1),
                        "has_input": 'input' in sample,
                        "has_output": 'output' in sample
                    }
                    samples.append(sample_info)
            elif isinstance(batch_data, dict):
                # å¤„ç†å­—å…¸æ ¼å¼çš„batchï¼ˆå¯èƒ½æ¥è‡ªcustom_collate_fnï¼‰
                batch_size = len(batch_data.get('input', [])) if 'input' in batch_data else len(next(iter(batch_data.values())))
                for i in range(batch_size):
                    sample_info = {
                        "batch_index": i,
                        "source_line": batch_data.get('_source_line', [-1] * batch_size)[i] if '_source_line' in batch_data else -1,
                        "epoch": batch_data.get('_epoch', [-1] * batch_size)[i] if '_epoch' in batch_data else -1,
                        "has_input": 'input' in batch_data,
                        "has_output": 'output' in batch_data
                    }
                    samples.append(sample_info)
            else:
                # å¤„ç†tensor batchçš„æƒ…å†µ
                batch_size = len(batch_data) if hasattr(batch_data, '__len__') else 1
                for i in range(batch_size):
                    sample_info = {
                        "batch_index": i,
                        "source_line": -1,
                        "epoch": -1,
                        "has_input": True,
                        "has_output": True
                    }
                    samples.append(sample_info)
            
            # åˆ›å»ºbatchè®°å½•
            batch_record = {
                "step": step,
                "timestamp": datetime.now().isoformat(),
                "batch_size": len(samples),
                "samples": samples
            }
            
            # ä¿å­˜è®°å½•
            self.batch_records.append(batch_record)
            
            # å†™å…¥JSONLæ–‡ä»¶
            with open(self.tracking_log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(batch_record, ensure_ascii=False) + '\n')
                
        except Exception as e:
            print(f"âš ï¸  è¿½è¸ªç¬¬{step}æ­¥æ—¶å‡ºé”™: {e}")
    
    def track_checkpoint(self, checkpoint_step: int, checkpoint_name: str, 
                        batch_range: Tuple[int, int]) -> None:
        """è®°å½•checkpointä¿¡æ¯"""
        checkpoint_info = {
            "checkpoint_step": checkpoint_step,
            "checkpoint_name": checkpoint_name,
            "batch_range": {
                "start_step": batch_range[0],
                "end_step": batch_range[1]
            },
            "timestamp": datetime.now().isoformat()
        }
        
        self.checkpoint_mappings[checkpoint_name] = checkpoint_info
        # ä¸æ‰“å°å•ä¸ªcheckpointï¼Œåœ¨æœ€åç»Ÿä¸€æŠ¥å‘Š
    
    def train_with_tracking(self, dataloader, config: Dict[str, Any], 
                           checkpoint_steps: List[int]) -> Dict[str, Any]:
        """æ‰§è¡Œå¸¦è¿½è¸ªçš„è®­ç»ƒ"""
        
        print(f"\nğŸš€ å¼€å§‹å¸¦è¿½è¸ªçš„LoRAè®­ç»ƒ...")
        print(f"ğŸ“Š é…ç½®:")
        print(f"  - æœ€å¤§æ­¥æ•°: {config['training']['max_steps']}")
        print(f"  - Checkpointæ­¥éª¤: {len(checkpoint_steps)}ä¸ª (æ­¥éª¤{min(checkpoint_steps)}-{max(checkpoint_steps)})")
        
        # é¢„å…ˆç”Ÿæˆcheckpointæ˜ å°„ï¼ˆé™é»˜ï¼‰
        for checkpoint_step in checkpoint_steps:
            checkpoint_name = f"checkpoint-{checkpoint_step}"
            # æ¯ä¸ªcheckpointå¯¹åº”å‰ä¸€æ­¥çš„æ•°æ®
            self.track_checkpoint(checkpoint_step, checkpoint_name, 
                                (checkpoint_step - 1, checkpoint_step - 1))
        
        # å¯åŠ¨å¹¶è¡Œbatchè¿½è¸ª
        import threading
        import os
        
        # è®¾ç½®tokenizersç¯å¢ƒå˜é‡é¿å…forkè­¦å‘Š
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        tracking_active = {"active": True}
        
        def parallel_batch_tracking():
            """å¹¶è¡Œæ‰§è¡Œbatchè¿½è¸ª"""
            step = 0
            batch_iter = iter(dataloader)
            max_steps = config['training']['max_steps']
            
            while step < max_steps and tracking_active["active"]:
                try:
                    batch = next(batch_iter)
                    self.track_batch(step, batch)
                    
                    step += 1
                    time.sleep(0.01)  # å°å»¶è¿Ÿé¿å…è¿‡åº¦å ç”¨èµ„æº
                    
                except StopIteration:
                    # é‡æ–°å¼€å§‹dataloader
                    batch_iter = iter(dataloader)
                    if step < max_steps:
                        batch = next(batch_iter)
                        self.track_batch(step, batch)
                        step += 1
                except Exception as e:
                    print(f"âš ï¸  Batchè¿½è¸ªå¼‚å¸¸: {e}")
                    break
        
        # å¯åŠ¨è¿½è¸ªçº¿ç¨‹
        tracking_thread = threading.Thread(target=parallel_batch_tracking, daemon=True)
        tracking_thread.start()
        
        try:
            # æ‰§è¡Œå®é™…è®­ç»ƒ
            print("ğŸƒâ€â™‚ï¸ å¼€å§‹å®é™…LoRAè®­ç»ƒ...")
            
            training_result = self.trainer.train(
                batch_size=config['training']['per_device_train_batch_size'],
                max_length=config.get('data', {}).get('max_length', 512),
                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
                warmup_steps=0,
                logging_steps=config['training'].get('logging_steps', 1),
                save_steps=config['training'].get('save_steps', 50)
            )
            
            print("âœ… LoRAè®­ç»ƒå®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ LoRAè®­ç»ƒå¤±è´¥: {e}")
            raise
        finally:
            # åœæ­¢è¿½è¸ª
            tracking_active["active"] = False
            time.sleep(1)  # ç­‰å¾…è¿½è¸ªçº¿ç¨‹ç»“æŸ
        
        # ç”Ÿæˆæ˜ å°„æ–‡ä»¶
        self.generate_mapping_file()
        
        return {
            "status": "training_completed",
            "output_dir": str(self.output_dir),
            "total_steps": config['training']['max_steps'],
            "checkpoint_steps": checkpoint_steps,
            "tracking_file": str(self.tracking_log_file),
            "training_result": training_result
        }
    
    def generate_mapping_file(self) -> None:
        """ç”Ÿæˆå®Œæ•´çš„line-to-checkpointæ˜ å°„æ–‡ä»¶"""
        
        print(f"\nğŸ“‹ ç”Ÿæˆline-to-checkpointæ˜ å°„...")
        
        # æ„å»ºæ˜ å°„å…³ç³»
        line_to_checkpoint = {}
        checkpoint_to_lines = {}
        
        for ckpt_name, ckpt_info in self.checkpoint_mappings.items():
            checkpoint_step = ckpt_info['checkpoint_step']
            start_step = ckpt_info['batch_range']['start_step']
            end_step = ckpt_info['batch_range']['end_step']
            
            # æ‰¾åˆ°ç›¸å…³çš„batchè®°å½•
            related_batches = [
                record for record in self.batch_records
                if start_step <= record["step"] <= end_step
            ]
            
            lines_in_checkpoint = set()
            
            for batch_record in related_batches:
                for sample in batch_record["samples"]:
                    source_line = sample.get("source_line", -1)
                    if source_line >= 0:
                        line_to_checkpoint[source_line] = ckpt_name
                        lines_in_checkpoint.add(source_line)
            
            checkpoint_to_lines[ckpt_name] = {
                "checkpoint_step": checkpoint_step,
                "lines": sorted(list(lines_in_checkpoint)),
                "line_count": len(lines_in_checkpoint),
                "batch_range": ckpt_info['batch_range']
            }
        
        # ç”Ÿæˆå®Œæ•´æ˜ å°„
        mapping_data = {
            "generation_info": {
                "timestamp": datetime.now().isoformat(),
                "total_checkpoints": len(checkpoint_to_lines),
                "total_tracked_lines": len(line_to_checkpoint),
                "total_batch_records": len(self.batch_records)
            },
            "line_to_checkpoint": line_to_checkpoint,
            "checkpoint_to_lines": checkpoint_to_lines
        }
        
        # ä¿å­˜æ˜ å°„æ–‡ä»¶
        mapping_file = self.output_dir / "line_to_checkpoint_mapping.json"
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(mapping_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ æ˜ å°„æ–‡ä»¶ä¿å­˜: {mapping_file}")
        print(f"ğŸ“Š è¿½è¸ªç»Ÿè®¡:")
        print(f"  - Checkpoints: {mapping_data['generation_info']['total_checkpoints']}")
        print(f"  - è¿½è¸ªè¡Œæ•°: {mapping_data['generation_info']['total_tracked_lines']}")
        print(f"  - Batchè®°å½•: {mapping_data['generation_info']['total_batch_records']}")
