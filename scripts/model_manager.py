#!/usr/bin/env python3
"""
model_manager.py å·²åºŸå¼ƒï¼Œä»…ä¿ç•™åº•å±‚å·¥å…·æ”¯æŒ
æ‰€æœ‰æ¨¡å‹ç®¡ç†æ“ä½œè¯·é€šè¿‡ baidu_gpu_lora_training.ipynb notebook å®Œæˆ
å¦‚éœ€åº•å±‚å‘½ä»¤è¡Œè°ƒç”¨ï¼Œå¯åœ¨ notebook ä¸­ä½¿ç”¨ !python scripts/model_manager.py æˆ– subprocess
"""

import os
import argparse
from pathlib import Path

def download_model(model_name, cache_dir="./models"):
    """ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°"""
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"ğŸ“ ä¿å­˜è·¯å¾„: {cache_dir}")
    
    # åˆ›å»ºç›®å½•
    Path(cache_dir).mkdir(parents=True, exist_ok=True)
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        print("ğŸ“¦ ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        print("ğŸ¤– ä¸‹è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True,
            torch_dtype="auto"
        )
        
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ!")
        
        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"  - åç§°: {model_name}")
        print(f"  - å‚æ•°é‡: {model.num_parameters():,}")
        print(f"  - ç¼“å­˜è·¯å¾„: {cache_dir}")
        
        return True
        
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£…transformers: pip install transformers")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def list_models(cache_dir="./models"):
    """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
    cache_path = Path(cache_dir)
    
    if not cache_path.exists():
        print("ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return
    
    print(f"ğŸ“š å·²ä¸‹è½½çš„æ¨¡å‹ ({cache_dir}):")
    
    # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
    model_dirs = []
    for item in cache_path.iterdir():
        if item.is_dir():
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
            if any(item.glob("*.bin")) or any(item.glob("*.safetensors")):
                model_dirs.append(item)
    
    if not model_dirs:
        print("  æš‚æ— å·²ä¸‹è½½çš„æ¨¡å‹")
        return
    
    for i, model_dir in enumerate(model_dirs, 1):
        print(f"  {i}. {model_dir.name}")
        
        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶å¤§å°
        total_size = sum(f.stat().st_size for f in model_dir.rglob("*") if f.is_file())
        size_mb = total_size / (1024 * 1024)
        print(f"     å¤§å°: {size_mb:.1f} MB")

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹ç®¡ç†å·¥å…·")
    parser.add_argument("--action", choices=["download", "list"], default="list",
                       help="æ“ä½œç±»å‹")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-0.5B",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--cache_dir", type=str, default="./models",
                       help="æ¨¡å‹ç¼“å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸ—ï¸  Foundation Model Manager")
    print("=" * 50)
    
    if args.action == "download":
        download_model(args.model, args.cache_dir)
    elif args.action == "list":
        list_models(args.cache_dir)

if __name__ == "__main__":
    main()
