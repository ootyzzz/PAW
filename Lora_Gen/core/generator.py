"""
generator.py
å®Œæ•´çš„ LoRA å‚æ•°ç”Ÿæˆå™¨
Text Encoder (HuggingFace Transformers) -> HyperConv Decoder -> Tokenized Parameters
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from typing import List, Optional, Tuple, Union
import numpy as np

from .hyperconv_decoder import HyperConvDecoder


class HuggingFaceTextEncoder(nn.Module):
    """
    åŸºäºHuggingFace Transformersçš„æ–‡æœ¬ç¼–ç å™¨
    ä½¿ç”¨å®˜æ–¹æ–¹æ³•å®ç°all-MiniLM-L6-v2
    """
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2'):
        super().__init__()
        self.model_name = model_name
        print(f"ğŸ”„ åŠ è½½HuggingFaceæ¨¡å‹: {model_name}")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        
        # è·å–embeddingç»´åº¦
        self.embedding_dim = self.model.config.hidden_size  # 384 for all-MiniLM-L6-v2
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œembeddingç»´åº¦: {self.embedding_dim}")
    
    def mean_pooling(self, model_output, attention_mask):
        """
        Mean Pooling - è€ƒè™‘attention maskçš„æ­£ç¡®å¹³å‡
        è¿™æ˜¯å®˜ç½‘æä¾›çš„æ ‡å‡†å®ç°
        """
        token_embeddings = model_output[0]  # ç¬¬ä¸€ä¸ªå…ƒç´ åŒ…å«æ‰€æœ‰token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def encode(self, sentences):
        """
        ç¼–ç å¥å­ä¸ºembeddings
        
        Args:
            sentences: stræˆ–List[str] - è¾“å…¥å¥å­
            
        Returns:
            sentence_embeddings: torch.Tensor [batch_size, embedding_dim]
        """
        if isinstance(sentences, str):
            sentences = [sentences]
        
        # Tokenizeå¥å­
        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„device
        device = next(self.parameters()).device
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
        
        # è®¡ç®—token embeddings
        with torch.no_grad():
            model_output = self.model(**encoded_input)
        
        # æ‰§è¡Œpooling
        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
        
        # æ ‡å‡†åŒ–embeddings
        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)
        
        return sentence_embeddings
    
    def forward(self, sentences):
        """å‰å‘ä¼ æ’­"""
        return self.encode(sentences)


class LoRAParameterGenerator(nn.Module):
    """
    å®Œæ•´çš„LoRAå‚æ•°ç”Ÿæˆå™¨
    
    æ¶æ„ï¼š
    Text Input -> HuggingFace Text Encoder -> HyperConv Decoder -> Tokenized LoRA Parameters
    """
    def __init__(
        self, 
        text_encoder_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # HuggingFace model
        hidden_dim: int = 384,
        max_seq_len: int = 512,
        num_hyperconv_blocks: int = 3,
        output_dim: int = None,
        freeze_text_encoder: bool = True
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.max_seq_len = max_seq_len
        self.output_dim = output_dim or hidden_dim
        
        # Text Encoder (HuggingFace Transformers)
        self.text_encoder = HuggingFaceTextEncoder(text_encoder_name)
        self.encoder_dim = self.text_encoder.embedding_dim
        
        # å†»ç»“text encoderå‚æ•°ä»¥èŠ‚çœæ˜¾å­˜
        if freeze_text_encoder:
            for param in self.text_encoder.parameters():
                param.requires_grad = False
        
        # è¾“å…¥æŠ•å½±å±‚ï¼šå°†HuggingFaceè¾“å‡ºæ˜ å°„åˆ°hidden_dim
        if self.encoder_dim != hidden_dim:
            self.input_proj = nn.Sequential(
                nn.Linear(self.encoder_dim, hidden_dim * 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim * 2, hidden_dim)
            )
        else:
            self.input_proj = nn.Identity()
        
        # åºåˆ—æ‰©å±•å±‚ï¼šå°†å•ä¸ªembeddingæ‰©å±•ä¸ºåºåˆ—
        self.sequence_expander = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * max_seq_len),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # Parameter Generator (HyperConv Decoder)
        self.parameter_generator = HyperConvDecoder(
            num_blocks=num_hyperconv_blocks,
            dim=hidden_dim,
            max_seq_len=max_seq_len,
            output_dim=self.output_dim
        )
        
        # æœ€ç»ˆè¾“å‡ºå½’ä¸€åŒ–
        self.output_norm = nn.LayerNorm(self.output_dim)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Conv1d):
                torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
    
    def forward(self, prompts: Union[List[str], torch.Tensor], return_embeddings: bool = False):
        """
        Args:
            prompts: List[str] æˆ–å·²ç¼–ç çš„tensor [B, encoder_dim]
            return_embeddings: bool - æ˜¯å¦è¿”å›ä¸­é—´embeddings
        
        Returns:
            generated_params: torch.Tensor [B, max_seq_len, output_dim] - ç”Ÿæˆçš„å‚æ•°tokens
        """
        if isinstance(prompts, list):
            # 1. Text Encoding (HuggingFace Transformers)
            with torch.no_grad():
                text_embeddings = self.text_encoder.encode(prompts)
        else:
            text_embeddings = prompts
        
        batch_size = text_embeddings.shape[0]
        
        # 2. Input Projection
        embeddings = self.input_proj(text_embeddings)  # [B, hidden_dim]
        
        # 3. æ‰©å±•åˆ°åºåˆ—ç»´åº¦
        expanded = self.sequence_expander(embeddings)  # [B, hidden_dim * max_seq_len]
        expanded = expanded.view(batch_size, self.max_seq_len, self.hidden_dim)  # [B, max_seq_len, hidden_dim]
        
        # 4. Parameter Generation
        generated_params = self.parameter_generator(expanded)  # [B, max_seq_len, output_dim]
        generated_params = self.output_norm(generated_params)
        
        if return_embeddings:
            return generated_params, text_embeddings
        return generated_params
    
    def encode_text(self, prompts: List[str]) -> torch.Tensor:
        """å•ç‹¬çš„æ–‡æœ¬ç¼–ç æ–¹æ³•"""
        with torch.no_grad():
            return self.text_encoder.encode(prompts)


class LoRATokenizer:
    """
    LoRAå‚æ•°çš„Tokenizer
    å°†checkpointå‚æ•°è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„tokenåºåˆ—
    """
    def __init__(self, max_tokens: int = 512, token_dim: int = 384):
        self.max_tokens = max_tokens
        self.token_dim = token_dim
        
    def tokenize_checkpoint(self, checkpoint_path: str, layer_names: List[str] = None) -> torch.Tensor:
        """
        å°†checkpointçš„LoRAå‚æ•°tokenize
        
        Args:
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
            layer_names: è¦æå–çš„å±‚åç§°åˆ—è¡¨
            
        Returns:
            tokens: torch.Tensor [max_tokens, token_dim] - tokenizedå‚æ•°
        """
        # åŠ è½½checkpoint - å¤„ç†PyTorch 2.6çš„å®‰å…¨æ¨¡å¼
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
        except Exception:
            # å›é€€åˆ°æ—§ç‰ˆæœ¬åŠ è½½æ–¹å¼
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æå–å‚æ•° - å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
        if isinstance(checkpoint, dict):
            if 'state_dict' in checkpoint:
                params_dict = checkpoint['state_dict']
            else:
                params_dict = checkpoint
        else:
            params_dict = checkpoint
        
        # æå–LoRAå‚æ•°
        lora_params = []
        for name, param in params_dict.items():
            if 'lora' in name.lower() and (layer_names is None or any(ln in name for ln in layer_names)):
                lora_params.append(param.flatten())
        
        if not lora_params:
            raise ValueError(f"No LoRA parameters found in {checkpoint_path}")
        
        # æ‹¼æ¥æ‰€æœ‰å‚æ•°
        all_params = torch.cat(lora_params, dim=0)  # [total_params]
        
        # åˆ†å—å¹¶é‡å¡‘ä¸ºtokens
        total_elements = len(all_params)
        elements_per_token = self.token_dim
        
        # è®¡ç®—éœ€è¦çš„tokenæ•°é‡
        num_tokens = (total_elements + elements_per_token - 1) // elements_per_token
        
        # Paddingåˆ°åˆé€‚çš„å¤§å°
        if total_elements % elements_per_token != 0:
            padding_size = elements_per_token - (total_elements % elements_per_token)
            all_params = torch.cat([all_params, torch.zeros(padding_size)], dim=0)
        
        # é‡å¡‘ä¸ºtokenæ ¼å¼
        tokens = all_params.view(-1, elements_per_token)  # [num_tokens, token_dim]
        
        # Paddingæˆ–æˆªæ–­åˆ°max_tokens
        if num_tokens > self.max_tokens:
            tokens = tokens[:self.max_tokens]
        elif num_tokens < self.max_tokens:
            padding = torch.zeros(self.max_tokens - num_tokens, self.token_dim)
            tokens = torch.cat([tokens, padding], dim=0)
        
        return tokens
    
    def detokenize(self, tokens: torch.Tensor) -> torch.Tensor:
        """
        å°†tokensè½¬æ¢å›å‚æ•°å‘é‡
        
        Args:
            tokens: [max_tokens, token_dim] or [batch_size, max_tokens, token_dim]
            
        Returns:
            params: æ‰å¹³åŒ–çš„å‚æ•°å‘é‡
        """
        if tokens.dim() == 3:
            # æ‰¹å¤„ç†æƒ…å†µ
            return tokens.view(tokens.shape[0], -1)
        else:
            # å•ä¸ªæ ·æœ¬
            return tokens.view(-1)
