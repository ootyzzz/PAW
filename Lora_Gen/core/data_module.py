"""
data_module.py
æ•°æ®æ¨¡å—ï¼šå¤„ç†promptå’Œcheckpointçš„é…å¯¹æ•°æ®
"""
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
import json
import random
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
import numpy as np

from .generator import LoRATokenizer


class PromptCheckpointDataset(Dataset):
    """
    Prompt-Checkpointé…å¯¹æ•°æ®é›†
    """
    def __init__(
        self,
        prompt_file: str,
        checkpoint_dir: str,
        tokenizer: LoRATokenizer,
        samples_per_prompt: int = 4,
        max_checkpoints: int = 50,
        cache_tokenized: bool = True
    ):
        self.prompt_file = prompt_file
        self.checkpoint_dir = Path(checkpoint_dir)
        self.tokenizer = tokenizer
        self.samples_per_prompt = samples_per_prompt
        self.max_checkpoints = max_checkpoints
        self.cache_tokenized = cache_tokenized
        
        # åŠ è½½æ•°æ®
        self.prompts = self._load_prompts()
        self.checkpoint_paths = self._load_checkpoint_paths()
        self.tokenized_cache = {} if cache_tokenized else None
        
        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"  - Prompts: {len(self.prompts)}")
        print(f"  - Checkpoints: {len(self.checkpoint_paths)}")
        print(f"  - æ€»æ ·æœ¬æ•°: {len(self)}")
    
    def _load_prompts(self) -> List[str]:
        """åŠ è½½ç»„åˆprompts"""
        prompts = []
        with open(self.prompt_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line.strip())
                    prompts.append(item['prompt'])
        
        return prompts
    
    def _load_checkpoint_paths(self) -> List[str]:
        """åŠ è½½checkpointè·¯å¾„"""
        checkpoint_files = []
        
        # æŸ¥æ‰¾æ‰€æœ‰.ckptæˆ–.ptæ–‡ä»¶
        for ext in ['*.ckpt', '*.pt', '*.pth']:
            checkpoint_files.extend(list(self.checkpoint_dir.glob(ext)))
        
        # é™åˆ¶æ•°é‡
        if len(checkpoint_files) > self.max_checkpoints:
            checkpoint_files = checkpoint_files[:self.max_checkpoints]
        
        return [str(f) for f in checkpoint_files]
    
    def _tokenize_checkpoint(self, checkpoint_path: str) -> torch.Tensor:
        """tokenize checkpointï¼Œä½¿ç”¨ç¼“å­˜"""
        if self.cache_tokenized and checkpoint_path in self.tokenized_cache:
            return self.tokenized_cache[checkpoint_path]
        
        try:
            tokens = self.tokenizer.tokenize_checkpoint(checkpoint_path)
            
            if self.cache_tokenized:
                self.tokenized_cache[checkpoint_path] = tokens
            
            return tokens
        except Exception as e:
            print(f"âš ï¸ æ— æ³•tokenize checkpoint {checkpoint_path}: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
            return torch.zeros(self.tokenizer.max_tokens, self.tokenizer.token_dim)
    
    def __len__(self):
        return len(self.prompts) * len(self.checkpoint_paths)
    
    def __getitem__(self, idx):
        # è®¡ç®—promptå’Œcheckpointçš„ç´¢å¼•
        prompt_idx = idx // len(self.checkpoint_paths)
        checkpoint_idx = idx % len(self.checkpoint_paths)
        
        # è·å–æ•°æ®
        prompt = self.prompts[prompt_idx]
        checkpoint_path = self.checkpoint_paths[checkpoint_idx]
        
        # Tokenize checkpoint
        tokenized_params = self._tokenize_checkpoint(checkpoint_path)
        
        return {
            'prompt': prompt,
            'target_params': tokenized_params,
            'checkpoint_path': checkpoint_path,
            'prompt_idx': prompt_idx,
            'checkpoint_idx': checkpoint_idx
        }


class LoRAGeneratorDataModule(pl.LightningDataModule):
    """
    Lightningæ•°æ®æ¨¡å—
    """
    def __init__(
        self,
        train_prompt_file: str,
        val_prompt_file: str,
        checkpoint_dir: str,
        batch_size: int = 8,
        num_workers: int = 4,
        samples_per_prompt: int = 4,
        max_checkpoints: int = 50,
        max_seq_len: int = 512,
        token_dim: int = 384,
        cache_tokenized: bool = True,
        train_ratio: float = 0.9
    ):
        super().__init__()
        self.train_prompt_file = train_prompt_file
        self.val_prompt_file = val_prompt_file
        self.checkpoint_dir = checkpoint_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.samples_per_prompt = samples_per_prompt
        self.max_checkpoints = max_checkpoints
        self.max_seq_len = max_seq_len
        self.token_dim = token_dim
        self.cache_tokenized = cache_tokenized
        self.train_ratio = train_ratio
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = LoRATokenizer(
            max_tokens=max_seq_len,
            token_dim=token_dim
        )
    
    def setup(self, stage: Optional[str] = None):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "fit" or stage is None:
            # å¦‚æœåªæœ‰ä¸€ä¸ªpromptæ–‡ä»¶ï¼Œéœ€è¦åˆ†å‰²
            if self.val_prompt_file is None or not os.path.exists(self.val_prompt_file):
                # ä½¿ç”¨è®­ç»ƒæ–‡ä»¶åˆ›å»ºæ•°æ®é›†ï¼Œç„¶ååˆ†å‰²
                full_dataset = PromptCheckpointDataset(
                    prompt_file=self.train_prompt_file,
                    checkpoint_dir=self.checkpoint_dir,
                    tokenizer=self.tokenizer,
                    samples_per_prompt=self.samples_per_prompt,
                    max_checkpoints=self.max_checkpoints,
                    cache_tokenized=self.cache_tokenized
                )
                
                # åˆ†å‰²æ•°æ®é›†
                train_size = int(len(full_dataset) * self.train_ratio)
                val_size = len(full_dataset) - train_size
                
                self.train_dataset, self.val_dataset = torch.utils.data.random_split(
                    full_dataset, [train_size, val_size]
                )
            else:
                # åˆ†åˆ«åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
                self.train_dataset = PromptCheckpointDataset(
                    prompt_file=self.train_prompt_file,
                    checkpoint_dir=self.checkpoint_dir,
                    tokenizer=self.tokenizer,
                    samples_per_prompt=self.samples_per_prompt,
                    max_checkpoints=self.max_checkpoints,
                    cache_tokenized=self.cache_tokenized
                )
                
                self.val_dataset = PromptCheckpointDataset(
                    prompt_file=self.val_prompt_file,
                    checkpoint_dir=self.checkpoint_dir,
                    tokenizer=self.tokenizer,
                    samples_per_prompt=self.samples_per_prompt,
                    max_checkpoints=self.max_checkpoints,
                    cache_tokenized=self.cache_tokenized
                )
        
        if stage == "test" or stage is None:
            self.test_dataset = self.val_dataset if hasattr(self, 'val_dataset') else None
    
    def collate_fn(self, batch):
        """è‡ªå®šä¹‰collateå‡½æ•°"""
        prompts = [item['prompt'] for item in batch]
        target_params = torch.stack([item['target_params'] for item in batch])
        
        return prompts, target_params
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True,
            drop_last=True
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True
        )
    
    def test_dataloader(self):
        if self.test_dataset is not None:
            return DataLoader(
                self.test_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=self.num_workers,
                collate_fn=self.collate_fn,
                pin_memory=True
            )
        return None


def create_prompt_splits(
    input_file: str,
    train_output: str,
    val_output: str,
    samples_per_prompt: int = 4,
    test_ratio: float = 0.1
):
    """
    ä»å•ä¸ªæ•°æ®æ–‡ä»¶åˆ›å»ºè®­ç»ƒ/éªŒè¯split
    
    Args:
        input_file: è¾“å…¥çš„jsonlæ–‡ä»¶
        train_output: è®­ç»ƒé›†è¾“å‡ºæ–‡ä»¶
        val_output: éªŒè¯é›†è¾“å‡ºæ–‡ä»¶
        samples_per_prompt: æ¯ä¸ªpromptåŒ…å«çš„æ ·æœ¬æ•°
        test_ratio: éªŒè¯é›†æ¯”ä¾‹
    """
    # åŠ è½½æ•°æ®
    data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line.strip()))
    
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {len(data)} æ ·æœ¬")
    
    # éšæœºæ‰“ä¹±
    random.shuffle(data)
    
    # åˆ›å»ºnon-overlappingçš„ç»„åˆ
    prompts = []
    for i in range(0, len(data) - samples_per_prompt + 1, samples_per_prompt):
        batch = data[i:i + samples_per_prompt]
        
        # æ‹¼æ¥æˆç»„åˆprompt
        combined_prompt = ""
        for item in batch:
            prompt_text = f"Question: {item['input']}\n"
            for option in item['options']:
                prompt_text += f"{option}\n"
            prompt_text += f"Answer: {item['target']}\n\n"
            combined_prompt += prompt_text
        
        combined_prompt = combined_prompt.strip()
        prompts.append({
            "prompt": combined_prompt,
            "sample_count": len(batch),
            "batch_info": [{"id": item["id"], "target": item["target"]} for item in batch]
        })
    
    print(f"ğŸ“Š åˆ›å»ºç»„åˆ: {len(prompts)} ä¸ªpromptç»„åˆ")
    
    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    split_idx = int(len(prompts) * (1 - test_ratio))
    train_prompts = prompts[:split_idx]
    val_prompts = prompts[split_idx:]
    
    # ä¿å­˜è®­ç»ƒé›†
    with open(train_output, 'w', encoding='utf-8') as f:
        for prompt_item in train_prompts:
            f.write(json.dumps(prompt_item, ensure_ascii=False) + '\n')
    
    # ä¿å­˜éªŒè¯é›†
    with open(val_output, 'w', encoding='utf-8') as f:
        for prompt_item in val_prompts:
            f.write(json.dumps(prompt_item, ensure_ascii=False) + '\n')
    
    print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
    print(f"  - è®­ç»ƒé›†: {len(train_prompts)} ç»„åˆ -> {train_output}")
    print(f"  - éªŒè¯é›†: {len(val_prompts)} ç»„åˆ -> {val_output}")


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šåˆ›å»ºæ•°æ®åˆ†å‰²
    create_prompt_splits(
        input_file="data_to_lora/cs/arc-challenge/arc-challenge_train_formatted.jsonl",
        train_output="Lora_Gen/data/train_prompts.jsonl",
        val_output="Lora_Gen/data/val_prompts.jsonl",
        samples_per_prompt=4,
        test_ratio=0.1
    )
