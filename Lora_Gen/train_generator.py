#!/usr/bin/env python3
"""
train_generator.py
LoRAå‚æ•°ç”Ÿæˆå™¨è®­ç»ƒè„šæœ¬
Lightning + SwanLab ç‰ˆæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python train_generator.py --config config/generator_config.yaml
python train_generator.py --config config/generator_config.yaml --checkpoint_dir runs/arc-challenge_lora_20250721_005053/checkpoints
python train_generator.py --dry_run  # éªŒè¯é…ç½®å’Œæ•°æ®
"""
import os
import sys
import yaml
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
import warnings

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import torch
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger
import swanlab

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from core.lightning_module import (
    LoRAGeneratorLightningModule, 
    setup_callbacks, 
    SwanLabLogger
)
from core.data_module import LoRAGeneratorDataModule, create_prompt_splits
from core.generator import LoRATokenizer

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def validate_config(config: Dict[str, Any]) -> bool:
    """éªŒè¯é…ç½®æ–‡ä»¶"""
    required_keys = [
        'model', 'data', 'training', 'lightning', 'logging'
    ]
    
    for key in required_keys:
        if key not in config:
            print(f"âŒ é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€é”®: {key}")
            return False
    
    # éªŒè¯è·¯å¾„
    checkpoint_dir = Path(config['data']['checkpoint_dir'])
    if not checkpoint_dir.exists():
        print(f"âŒ Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return False
    
    return True

def setup_experiment_paths(config: Dict[str, Any], experiment_name: str) -> Dict[str, Any]:
    """è®¾ç½®å®éªŒè·¯å¾„"""
    base_dir = Path("Lora_Gen/experiments") / experiment_name
    
    config['paths'] = {
        'experiment_dir': str(base_dir),
        'checkpoints_dir': str(base_dir / "checkpoints"),
        'tensorboard_dir': str(base_dir / "tensorboard_logs"),
        'swanlab_dir': str(base_dir / "swanlab_logs"),
        'results_dir': str(base_dir / "results"),
        'config_file': str(base_dir / "config.yaml")
    }
    
    # åˆ›å»ºç›®å½•
    for path in config['paths'].values():
        if path.endswith('.yaml'):
            Path(path).parent.mkdir(parents=True, exist_ok=True)
        else:
            Path(path).mkdir(parents=True, exist_ok=True)
    
    return config

def prepare_data(config: Dict[str, Any], force_recreate: bool = False) -> bool:
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    data_config = config['data']
    
    # æ£€æŸ¥promptæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    train_file = Path(data_config['train_prompt_file'])
    val_file = Path(data_config['val_prompt_file'])
    
    if not train_file.exists() or not val_file.exists() or force_recreate:
        print("ğŸ“Š åˆ›å»ºè®­ç»ƒ/éªŒè¯æ•°æ®split...")
        
        # æºæ•°æ®æ–‡ä»¶
        source_file = "data_to_lora/cs/arc-challenge/arc-challenge_train_formatted.jsonl"
        if not Path(source_file).exists():
            print(f"âŒ æºæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
            return False
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        train_file.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºsplit
        create_prompt_splits(
            input_file=source_file,
            train_output=str(train_file),
            val_output=str(val_file),
            samples_per_prompt=data_config['samples_per_prompt'],
            test_ratio=1.0 - data_config['train_ratio']
        )
    
    # éªŒè¯checkpointç›®å½•
    checkpoint_dir = Path(data_config['checkpoint_dir'])
    if not checkpoint_dir.exists():
        print(f"âŒ Checkpointç›®å½•ä¸å­˜åœ¨: {checkpoint_dir}")
        return False
    
    # ç»Ÿè®¡checkpointæ–‡ä»¶
    checkpoint_files = []
    for ext in ['*.ckpt', '*.pt', '*.pth']:
        checkpoint_files.extend(list(checkpoint_dir.glob(ext)))
    
    print(f"ğŸ“Š å‘ç° {len(checkpoint_files)} ä¸ªcheckpointæ–‡ä»¶")
    
    if len(checkpoint_files) == 0:
        print(f"âŒ åœ¨ {checkpoint_dir} ä¸­æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
        return False
    
    return True

def run_training(config: Dict[str, Any], dry_run: bool = False) -> bool:
    """è¿è¡Œè®­ç»ƒ"""
    try:
        # ç”Ÿæˆå®éªŒåç§°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        experiment_name = f"lora_generator_{timestamp}"
        
        # è®¾ç½®å®éªŒè·¯å¾„
        config = setup_experiment_paths(config, experiment_name)
        
        # æ›´æ–°å®éªŒé…ç½®
        config['experiment'] = {
            'name': experiment_name,
            'created_at': datetime.now().isoformat(),
            'description': config.get('experiment', {}).get('description', 
                                                          "LoRA Parameter Generator training"),
            'framework': 'lightning_swanlab',
            'tags': config.get('experiment', {}).get('tags', [])
        }
        
        # æ›´æ–°SwanLabé…ç½®
        if config['logging']['swanlab']['experiment_name'] is None:
            config['logging']['swanlab']['experiment_name'] = experiment_name
        
        print(f"\\n{'=' * 70}")
        print(f"ğŸš€ LoRA Parameter Generator è®­ç»ƒ")
        print(f"{'=' * 70}")
        print(f"å®éªŒåç§°: {experiment_name}")
        print(f"å®éªŒç›®å½•: {config['paths']['experiment_dir']}")
        print(f"Checkpointæº: {config['data']['checkpoint_dir']}")
        print(f"æ¨¡å‹é…ç½®: {config['model']}")
        print(f"è®­ç»ƒé…ç½®: batch_size={config['data']['batch_size']}, lr={config['training']['learning_rate']}")
        
        if dry_run:
            print("ğŸƒ Dry Run æ¨¡å¼ - éªŒè¯å®Œæˆ!")
            return True
        
        # ä¿å­˜é…ç½®
        with open(config['paths']['config_file'], 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, indent=2, allow_unicode=True)
        
        # å‡†å¤‡æ•°æ®
        if not prepare_data(config):
            return False
        
        # åˆå§‹åŒ–SwanLab
        print("ğŸ“Š åˆå§‹åŒ– SwanLab...")
        swanlab_config = config['logging']['swanlab']
        swanlab_params = {
            "project": swanlab_config['project'],
            "experiment_name": swanlab_config['experiment_name'],
            "config": config,
            "logdir": config['paths']['swanlab_dir']
        }
        
        if swanlab_config.get('workspace'):
            swanlab_params["workspace"] = swanlab_config['workspace']
        
        swanlab_run = swanlab.init(**swanlab_params)
        
        # åˆ›å»ºæ•°æ®æ¨¡å—
        print("ğŸ“Š åˆ›å»ºæ•°æ®æ¨¡å—...")
        data_module = LoRAGeneratorDataModule(
            train_prompt_file=config['data']['train_prompt_file'],
            val_prompt_file=config['data']['val_prompt_file'],
            checkpoint_dir=config['data']['checkpoint_dir'],
            batch_size=config['data']['batch_size'],
            num_workers=config['data']['num_workers'],
            samples_per_prompt=config['data']['samples_per_prompt'],
            max_checkpoints=config['data']['max_checkpoints'],
            max_seq_len=config['model']['max_seq_len'],
            token_dim=config['model']['output_dim'],
            cache_tokenized=config['data']['cache_tokenized'],
            train_ratio=config['data']['train_ratio']
        )
        
        # åˆ›å»ºLightningæ¨¡å—
        print("ğŸ§  åˆ›å»ºLightningæ¨¡å—...")
        lightning_module = LoRAGeneratorLightningModule(
            # Model config
            text_encoder_name=config['model']['text_encoder_name'],
            hidden_dim=config['model']['hidden_dim'],
            max_seq_len=config['model']['max_seq_len'],
            num_hyperconv_blocks=config['model']['num_hyperconv_blocks'],
            output_dim=config['model']['output_dim'],
            
            # Training config
            learning_rate=float(config['training']['learning_rate']),
            weight_decay=float(config['training']['weight_decay']),
            warmup_steps=int(config['training']['warmup_steps']),
            
            # Loss config
            mse_weight=float(config['training']['mse_weight']),
            l1_weight=float(config['training']['l1_weight']),
            
            # Optimizer config
            optimizer_type=config['training']['optimizer_type'],
            scheduler_type=config['training']['scheduler_type']
        )
        
        # å°†SwanLab runæ·»åŠ åˆ°æ¨¡å—
        lightning_module._swanlab_run = swanlab_run
        
        # è®¾ç½®å›è°ƒ
        callbacks = setup_callbacks(config)
        
        # è®¾ç½®TensorBoardæ—¥å¿—è®°å½•å™¨
        tensorboard_logger = TensorBoardLogger(
            save_dir=config['paths']['tensorboard_dir'],
            name=config['logging']['tensorboard']['name'],
            version=config['logging']['tensorboard']['version']
        )
        
        # åˆ›å»ºTrainer
        trainer_config = config['lightning']
        trainer = Trainer(
            max_epochs=config['training']['max_epochs'],
            max_steps=config['training']['max_steps'],
            callbacks=callbacks,
            logger=tensorboard_logger,
            accelerator=trainer_config['accelerator'],
            devices=trainer_config['devices'],
            strategy=trainer_config['strategy'],
            precision=config['training']['precision'],
            gradient_clip_val=config['training']['gradient_clip_val'],
            accumulate_grad_batches=config['training']['accumulate_grad_batches'],
            val_check_interval=config['training']['val_check_interval'],
            check_val_every_n_epoch=config['training']['check_val_every_n_epoch'],
            enable_progress_bar=trainer_config['enable_progress_bar'],
            log_every_n_steps=trainer_config['log_every_n_steps'],
            deterministic=trainer_config['deterministic']
        )
        
        print(f"\\nğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“Š è®­ç»ƒå™¨é…ç½®:")
        print(f"  - æœ€å¤§epochs: {config['training']['max_epochs']}")
        print(f"  - ç²¾åº¦: {config['training']['precision']}")
        print(f"  - åŠ é€Ÿå™¨: {trainer.accelerator}")
        print(f"  - è®¾å¤‡: {trainer.num_devices}")
        
        # å¼€å§‹è®­ç»ƒ
        trainer.fit(lightning_module, datamodule=data_module)
        
        # æµ‹è¯•ï¼ˆå¦‚æœæœ‰æµ‹è¯•æ•°æ®ï¼‰
        if data_module.test_dataloader() is not None:
            print(f"\\nğŸ§ª å¼€å§‹æµ‹è¯•...")
            test_results = trainer.test(lightning_module, datamodule=data_module)
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {test_results}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = Path(config['paths']['results_dir']) / "final_model"
        final_model_dir.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': lightning_module.generator.state_dict(),
            'config': config,
            'experiment_name': experiment_name
        }, final_model_dir / "generator_model.pt")
        
        print(f"\\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ å®éªŒç›®å½•: {config['paths']['experiment_dir']}")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹: {final_model_dir}")
        
        # å…³é—­SwanLab
        swanlab.finish()
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        if 'swanlab_run' in locals():
            swanlab.finish()
        return False

def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    parser = argparse.ArgumentParser(description="LoRA Parameter Generator è®­ç»ƒè„šæœ¬")
    parser.add_argument("--config", type=str, default="config/generator_config.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint_dir", type=str, default=None,
                       help="Checkpointç›®å½•è·¯å¾„ (è¦†ç›–é…ç½®æ–‡ä»¶)")
    parser.add_argument("--dry_run", action="store_true",
                       help="å¹²è¿è¡Œæ¨¡å¼: éªŒè¯é…ç½®ä½†ä¸è®­ç»ƒ")
    parser.add_argument("--force_recreate_data", action="store_true",
                       help="å¼ºåˆ¶é‡æ–°åˆ›å»ºè®­ç»ƒæ•°æ®")
    
    args = parser.parse_args()
    
    print("ğŸš€ LoRA Parameter Generator è®­ç»ƒè„šæœ¬")
    print("=" * 70)
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"è¿è¡Œæ¨¡å¼: {'ğŸƒ Dry Run' if args.dry_run else 'ğŸš€ å®Œæ•´è®­ç»ƒ'}")
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    try:
        # åŠ è½½é…ç½®
        config_path = Path(args.config)
        if not config_path.exists():
            # å°è¯•ç›¸å¯¹äºè„šæœ¬ç›®å½•çš„è·¯å¾„
            config_path = Path(__file__).parent / args.config
        
        if not config_path.exists():
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
            return False
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        if args.checkpoint_dir:
            config['data']['checkpoint_dir'] = args.checkpoint_dir
            print(f"ğŸ”„ ä½¿ç”¨å‘½ä»¤è¡Œcheckpointç›®å½•: {args.checkpoint_dir}")
        
        # éªŒè¯é…ç½®
        if not validate_config(config):
            return False
        
        # è¿è¡Œè®­ç»ƒ
        return run_training(config, dry_run=args.dry_run)
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
