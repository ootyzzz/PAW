# LoRA Parameter Generator

åŸºäº HyperConv çš„ LoRA å‚æ•°ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ PyTorch Lightning + SwanLab è®­ç»ƒæ¡†æ¶ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
Lora_Gen/
â”œâ”€â”€ core/                           # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ hyperconv_decoder.py       # HyperConv è§£ç å™¨
â”‚   â”œâ”€â”€ generator.py                # LoRA å‚æ•°ç”Ÿæˆå™¨
â”‚   â”œâ”€â”€ lightning_module.py         # Lightning è®­ç»ƒæ¨¡å—  
â”‚   â”œâ”€â”€ data_module.py              # æ•°æ®æ¨¡å—
â”‚   â””â”€â”€ utils/                      # å·¥å…·å‡½æ•°
â”œâ”€â”€ config/                         # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ generator_config.yaml       # é»˜è®¤é…ç½®
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train_prompts.jsonl         # è®­ç»ƒ prompts
â”‚   â””â”€â”€ val_prompts.jsonl           # éªŒè¯ prompts
â”œâ”€â”€ experiments/                    # å®éªŒç»“æœ
â”‚   â””â”€â”€ lora_generator_YYYYMMDD_HHMMSS/
â”‚       â”œâ”€â”€ checkpoints/            # Lightning checkpoints
â”‚       â”œâ”€â”€ tensorboard_logs/       # TensorBoard æ—¥å¿—
â”‚       â”œâ”€â”€ swanlab_logs/           # SwanLab æ—¥å¿—
â”‚       â”œâ”€â”€ results/                # æœ€ç»ˆç»“æœ
â”‚       â””â”€â”€ config.yaml             # å®éªŒé…ç½®
â”œâ”€â”€ scripts/                        # å·¥å…·è„šæœ¬
â”‚   â””â”€â”€ prepare_data.py             # æ•°æ®å‡†å¤‡å·¥å…·
â”œâ”€â”€ train_generator.py              # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py                    # æ¨ç†è„šæœ¬
â””â”€â”€ README.md                       # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install pytorch-lightning>=2.0.0
pip install swanlab>=0.3.0
pip install sentence-transformers
pip install transformers>=4.20.0
pip install peft
pip install PyYAML
```

### 2. æ•°æ®å‡†å¤‡

```bash
# å‡†å¤‡è®­ç»ƒæ•°æ®
python scripts/prepare_data.py prepare \
    --source data_to_lora/cs/arc-challenge/arc-challenge_train_formatted.jsonl \
    --output_dir Lora_Gen/data \
    --samples_per_prompt 4 \
    --test_ratio 0.1

# åˆ†æcheckpointæ–‡ä»¶
python scripts/prepare_data.py analyze \
    --checkpoint_dir runs/arc-challenge_lora_20250721_005053/checkpoints
```

### 3. è®­ç»ƒæ¨¡å‹

```bash
# åŸºç¡€è®­ç»ƒ
python train_generator.py --config config/generator_config.yaml

# æŒ‡å®šcheckpointç›®å½•
python train_generator.py \
    --config config/generator_config.yaml \
    --checkpoint_dir runs/arc-challenge_lora_20250721_005053/checkpoints

# å¹²è¿è¡Œï¼ˆéªŒè¯é…ç½®ï¼‰
python train_generator.py --config config/generator_config.yaml --dry_run
```

### 4. æ¨¡å‹æ¨ç†

```bash
# å•ä¸ªpromptæ¨ç†
python inference.py \
    --model experiments/lora_generator_xxx/results/final_model/generator_model.pt \
    --prompt "Question: What is the capital of France?"

# æ‰¹é‡æ¨ç†
python inference.py \
    --model experiments/lora_generator_xxx/results/final_model/generator_model.pt \
    --prompt_file test_prompts.txt \
    --output results/generated_params.pt
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ¨¡å‹æ¶æ„

```
Input Prompts
    â†“
Text Encoder (Sentence-BERT)
    â†“
Input Projection
    â†“
Sequence Expander  
    â†“
HyperConv Block 1 (ConvW â†’ ConvH â†’ ConvL)
    â†“
HyperConv Block 2 (ConvW â†’ ConvH â†’ ConvL)
    â†“  
HyperConv Block 3 (ConvW â†’ ConvH â†’ ConvL)
    â†“
Output Projection
    â†“
Tokenized LoRA Parameters [B, 512, 384]
```

### è®­ç»ƒæµç¨‹

1. **æ•°æ®å‡†å¤‡**: å°† ARC Challenge æ•°æ®éšæœºç»„åˆæˆ non-overlapping prompts
2. **Checkpoint Tokenization**: å°† 50 ä¸ª LoRA checkpoints tokenize æˆå›ºå®šæ ¼å¼
3. **ç›‘ç£è®­ç»ƒ**: ä½¿ç”¨ MSE loss è®­ç»ƒç”Ÿæˆå™¨ç”Ÿæˆç›®æ ‡å‚æ•°tokens
4. **å®éªŒç®¡ç†**: Lightning + SwanLab è‡ªåŠ¨è®°å½•è®­ç»ƒè¿‡ç¨‹å’ŒæŒ‡æ ‡

### æŸå¤±å‡½æ•°

```python
# MSE Loss (ä¸»è¦æŸå¤±)
mse_loss = F.mse_loss(generated_params, target_params)

# L1 Loss (ç¨€ç–æ€§æ­£åˆ™åŒ–) 
l1_loss = F.l1_loss(generated_params, target_params)

# æ€»æŸå¤±
total_loss = mse_weight * mse_loss + l1_weight * l1_loss
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

```yaml
model:
  text_encoder_name: "all-MiniLM-L6-v2"  # Sentence-BERTæ¨¡å‹
  hidden_dim: 384                         # éšè—å±‚ç»´åº¦
  max_seq_len: 512                        # æœ€å¤§åºåˆ—é•¿åº¦
  num_hyperconv_blocks: 3                 # HyperConvå—æ•°é‡
  output_dim: 384                         # è¾“å‡ºç»´åº¦
  freeze_text_encoder: true               # å†»ç»“æ–‡æœ¬ç¼–ç å™¨
```

### è®­ç»ƒé…ç½®

```yaml
training:
  max_epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  mse_weight: 1.0
  l1_weight: 0.1
  optimizer_type: "adamw"
  scheduler_type: "cosine"
```

### æ•°æ®é…ç½®

```yaml
data:
  checkpoint_dir: "runs/arc-challenge_lora_20250721_005053/checkpoints"
  batch_size: 8
  samples_per_prompt: 4
  max_checkpoints: 50
  cache_tokenized: true
```

## ğŸ“Š SwanLab é›†æˆ

### ä¸ªäººä½¿ç”¨

```bash
# é»˜è®¤ä½¿ç”¨ä¸ªäººé¡¹ç›®ï¼ˆè‡ªåŠ¨ç”Ÿæˆé¡¹ç›®åï¼‰
python train_generator.py --config config/generator_config.yaml
```

### å›¢é˜Ÿåä½œ

```yaml
# ä¿®æ”¹é…ç½®æ–‡ä»¶
logging:
  swanlab:
    project: "team-lora-experiments"    # å›¢é˜Ÿé¡¹ç›®å
    workspace: "your-team-workspace"    # å›¢é˜Ÿå·¥ä½œåŒº
```

æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```bash
export SWANLAB_PROJECT="team-lora-experiments"
export SWANLAB_WORKSPACE="your-team-workspace"
python train_generator.py --config config/generator_config.yaml
```

## ğŸ”§ å·¥å…·è„šæœ¬

### æ•°æ®åˆ†æ

```bash
# åˆ†æcheckpointç»Ÿè®¡ä¿¡æ¯
python scripts/prepare_data.py analyze --checkpoint_dir your_checkpoint_dir

# æµ‹è¯•tokenization
python scripts/prepare_data.py test \
    --checkpoint_file your_checkpoint.ckpt \
    --max_tokens 512 \
    --token_dim 384
```

### æ‰¹é‡è®­ç»ƒ

```bash
# è®­ç»ƒå¤šä¸ªé…ç½®
for config in config/*.yaml; do
    echo "è®­ç»ƒé…ç½®: $config"
    python train_generator.py --config $config
done
```

## ğŸ“ˆ å®éªŒç®¡ç†

### å®éªŒç›®å½•ç»“æ„

æ¯ä¸ªå®éªŒä¼šè‡ªåŠ¨åˆ›å»ºç‹¬ç«‹çš„ç›®å½•ï¼š

```
experiments/lora_generator_20250721_143052/
â”œâ”€â”€ checkpoints/                    # Lightningæ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ epoch=10-val_loss=0.1234.ckpt
â”‚   â””â”€â”€ last.ckpt
â”œâ”€â”€ tensorboard_logs/               # TensorBoardæ—¥å¿—
â”œâ”€â”€ swanlab_logs/                   # SwanLabæ—¥å¿—  
â”œâ”€â”€ results/                        # æœ€ç»ˆç»“æœ
â”‚   â””â”€â”€ final_model/
â”‚       â””â”€â”€ generator_model.pt
â””â”€â”€ config.yaml                     # å®éªŒé…ç½®å¿«ç…§
```

### æŒ‡æ ‡ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è®°å½•çš„æŒ‡æ ‡ï¼š

- **Loss**: MSE loss, L1 loss, Total loss
- **Metrics**: MAE, RMSE, Relative error
- **Learning**: Learning rate, Epoch, Step
- **Hardware**: GPU utilization, Memory usage

## ğŸ¯ ä½¿ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1: åŸºç¡€è®­ç»ƒ

```bash
# 1. å‡†å¤‡æ•°æ®
python scripts/prepare_data.py prepare \
    --source data_to_lora/cs/arc-challenge/arc-challenge_train_formatted.jsonl \
    --output_dir Lora_Gen/data

# 2. è®­ç»ƒæ¨¡å‹
python train_generator.py --config config/generator_config.yaml

# 3. æ¨ç†æµ‹è¯•
python inference.py \
    --model experiments/lora_generator_xxx/results/final_model/generator_model.pt \
    --prompt "Question: What is photosynthesis?"
```

### æ¡ˆä¾‹2: è‡ªå®šä¹‰é…ç½®

```bash
# 1. å¤åˆ¶é»˜è®¤é…ç½®
cp config/generator_config.yaml config/my_config.yaml

# 2. ä¿®æ”¹é…ç½®
# ç¼–è¾‘ my_config.yamlï¼Œè°ƒæ•´è¶…å‚æ•°

# 3. è®­ç»ƒ
python train_generator.py --config config/my_config.yaml
```

### æ¡ˆä¾‹3: å›¢é˜Ÿåä½œ

```yaml
# team_config.yaml
logging:
  swanlab:
    project: "lora-research-2025"
    workspace: "ai-lab"
    experiment_name: "hyperconv-v1"
```

```bash
python train_generator.py --config team_config.yaml
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å†…å­˜ä½¿ç”¨**: æ¯ä¸ªcheckpointçº¦1GBï¼Œå»ºè®®ä½¿ç”¨ç¼“å­˜æœºåˆ¶
2. **è®¡ç®—èµ„æº**: æ¨èä½¿ç”¨GPUè®­ç»ƒï¼ŒCPUè®­ç»ƒä¼šå¾ˆæ…¢
3. **æ•°æ®è´¨é‡**: ç¡®ä¿promptså’ŒcheckpointsåŒ¹é…ä¸”æœ‰æ•ˆ
4. **å®éªŒç®¡ç†**: å»ºè®®ä¸ºä¸åŒå®éªŒä½¿ç”¨æè¿°æ€§çš„é…ç½®æ–‡ä»¶å

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   ```bash
   # å‡å°batch size
   # åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®: batch_size: 4
   ```

2. **CheckpointåŠ è½½å¤±è´¥**
   ```bash
   # æ£€æŸ¥checkpointæ ¼å¼
   python scripts/prepare_data.py test --checkpoint_file your_file.ckpt
   ```

3. **SwanLabè¿æ¥é—®é¢˜**
   ```bash
   # æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPI key
   swanlab login
   ```

## ğŸ“š ç›¸å…³è®ºæ–‡

- HyperConv: [è®ºæ–‡é“¾æ¥]
- LoRA: Low-Rank Adaptation of Large Language Models
- Parameter Generation for Few-shot Learning

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issues å’Œ Pull Requestsï¼

## ğŸ“„ è®¸å¯è¯

MIT License
