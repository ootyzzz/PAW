#!/usr/bin/env python3
"""
inference.py
LoRAå‚æ•°ç”Ÿæˆå™¨æ¨ç†è„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python inference.py --model experiments/lora_generator_xxx/results/final_model/generator_model.pt --prompt "Question: What is..."
python inference.py --model experiments/lora_generator_xxx/results/final_model/generator_model.pt --prompt_file test_prompts.txt
"""
import os
import sys
from pathlib import Path
import argparse
import torch
import yaml
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from core.generator import LoRAParameterGenerator, LoRATokenizer


class LoRAGeneratorInference:
    """LoRAå‚æ•°ç”Ÿæˆå™¨æ¨ç†ç±»"""
    
    def __init__(self, model_path: str, device: str = 'auto'):
        self.device = self._setup_device(device)
        self.model, self.config = self._load_model(model_path)
        self.tokenizer = LoRATokenizer(
            max_tokens=self.config['model']['max_seq_len'],
            token_dim=self.config['model']['output_dim']
        )
        
    def _setup_device(self, device: str) -> torch.device:
        """è®¾ç½®è®¾å¤‡"""
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        return torch.device(device)
    
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint['config']
        
        # åˆ›å»ºæ¨¡å‹
        model = LoRAParameterGenerator(
            text_encoder_name=config['model']['text_encoder_name'],
            hidden_dim=config['model']['hidden_dim'],
            max_seq_len=config['model']['max_seq_len'],
            num_hyperconv_blocks=config['model']['num_hyperconv_blocks'],
            output_dim=config['model']['output_dim'],
            freeze_text_encoder=True
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ - è®¾å¤‡: {self.device}")
        
        return model, config
    
    def generate_lora_parameters(
        self, 
        prompts: List[str],
        return_raw: bool = False
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆLoRAå‚æ•°
        
        Args:
            prompts: è¾“å…¥promptåˆ—è¡¨
            return_raw: æ˜¯å¦è¿”å›åŸå§‹tokenæ ¼å¼
            
        Returns:
            ç»“æœå­—å…¸
        """
        with torch.no_grad():
            # ç”Ÿæˆå‚æ•°tokens
            generated_tokens = self.model(prompts)  # [B, max_seq_len, output_dim]
            
            results = {
                'prompts': prompts,
                'generated_tokens': generated_tokens.cpu() if return_raw else None,
                'batch_size': len(prompts),
                'token_shape': generated_tokens.shape,
                'statistics': self._compute_statistics(generated_tokens)
            }
            
            # å¦‚æœéœ€è¦ï¼Œå°†tokensè½¬æ¢ä¸ºå‚æ•°å‘é‡
            if not return_raw:
                param_vectors = []
                for i in range(len(prompts)):
                    param_vec = self.tokenizer.detokenize(generated_tokens[i])
                    param_vectors.append(param_vec.cpu())
                
                results['parameter_vectors'] = param_vectors
                results['parameter_shapes'] = [vec.shape for vec in param_vectors]
            
            return results
    
    def _compute_statistics(self, tokens: torch.Tensor) -> Dict[str, float]:
        """è®¡ç®—ç”Ÿæˆå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯"""
        with torch.no_grad():
            stats = {
                'mean': tokens.mean().item(),
                'std': tokens.std().item(),
                'min': tokens.min().item(),
                'max': tokens.max().item(),
                'l1_norm': tokens.abs().mean().item(),
                'l2_norm': torch.norm(tokens).item() / tokens.numel()**0.5
            }
        return stats
    
    def save_generated_parameters(
        self, 
        results: Dict[str, Any], 
        output_path: str,
        save_format: str = 'pt'
    ):
        """ä¿å­˜ç”Ÿæˆçš„å‚æ•°"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if save_format == 'pt':
            torch.save(results, output_path.with_suffix('.pt'))
        elif save_format == 'yaml':
            # ç§»é™¤tensoræ•°æ®ï¼Œåªä¿å­˜å…ƒæ•°æ®
            save_data = {
                'prompts': results['prompts'],
                'batch_size': results['batch_size'],
                'token_shape': list(results['token_shape']),
                'statistics': results['statistics'],
                'config': self.config
            }
            with open(output_path.with_suffix('.yaml'), 'w') as f:
                yaml.dump(save_data, f, default_flow_style=False)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LoRAå‚æ•°ç”Ÿæˆå™¨æ¨ç†è„šæœ¬")
    parser.add_argument("--model", type=str, required=True,
                       help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„")
    parser.add_argument("--prompt", type=str, default=None,
                       help="å•ä¸ªè¾“å…¥prompt")
    parser.add_argument("--prompt_file", type=str, default=None,
                       help="åŒ…å«å¤šä¸ªpromptçš„æ–‡ä»¶")
    parser.add_argument("--output", type=str, default=None,
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ (cuda/cpu/auto)")
    parser.add_argument("--save_format", type=str, default="pt",
                       choices=["pt", "yaml"], help="ä¿å­˜æ ¼å¼")
    parser.add_argument("--return_raw", action="store_true",
                       help="è¿”å›åŸå§‹tokenæ ¼å¼è€Œä¸æ˜¯å‚æ•°å‘é‡")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    if not args.prompt and not args.prompt_file:
        print("âŒ å¿…é¡»æä¾› --prompt æˆ– --prompt_file")
        return False
    
    if not Path(args.model).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        return False
    
    try:
        # åˆå§‹åŒ–æ¨ç†å™¨
        print("ğŸš€ LoRAå‚æ•°ç”Ÿæˆå™¨æ¨ç†")
        print("=" * 50)
        
        inference = LoRAGeneratorInference(args.model, args.device)
        
        # å‡†å¤‡prompts
        prompts = []
        if args.prompt:
            prompts = [args.prompt]
        elif args.prompt_file:
            with open(args.prompt_file, 'r', encoding='utf-8') as f:
                prompts = [line.strip() for line in f if line.strip()]
        
        print(f"ğŸ“ è¾“å…¥prompts: {len(prompts)} ä¸ª")
        for i, prompt in enumerate(prompts[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"  {i+1}. {prompt[:100]}{'...' if len(prompt) > 100 else ''}")
        
        if len(prompts) > 3:
            print(f"  ... (è¿˜æœ‰ {len(prompts)-3} ä¸ª)")
        
        # ç”Ÿæˆå‚æ•°
        print("\\nğŸ§  ç”ŸæˆLoRAå‚æ•°...")
        results = inference.generate_lora_parameters(prompts, return_raw=args.return_raw)
        
        # æ˜¾ç¤ºç»“æœ
        print("\\nğŸ“Š ç”Ÿæˆç»“æœ:")
        print(f"  - Tokenå½¢çŠ¶: {results['token_shape']}")
        print(f"  - ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in results['statistics'].items():
            print(f"    {key}: {value:.6f}")
        
        # ä¿å­˜ç»“æœ
        if args.output:
            inference.save_generated_parameters(
                results, args.output, args.save_format
            )
        else:
            # é»˜è®¤è¾“å‡ºè·¯å¾„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"Lora_Gen/results/generated_params_{timestamp}.pt"
            inference.save_generated_parameters(results, output_path, args.save_format)
        
        print("\\nâœ… æ¨ç†å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    from datetime import datetime
    success = main()
    sys.exit(0 if success else 1)
