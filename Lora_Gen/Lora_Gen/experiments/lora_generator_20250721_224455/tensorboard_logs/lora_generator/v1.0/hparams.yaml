hidden_dim: 384
l1_weight: 0.1
learning_rate: 0.0001
max_seq_len: 512
mse_weight: 1.0
num_hyperconv_blocks: 3
optimizer_type: adamw
output_dim: 384
scheduler_type: cosine
text_encoder_name: sentence-transformers/all-MiniLM-L6-v2
warmup_steps: 100
weight_decay: 1.0e-05
