#!/usr/bin/env python3
"""
evaluate_generated_lora.py
å°†ç”Ÿæˆçš„LoRAå‚æ•°åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹å¹¶åœ¨ARC Challengeä¸Šè¯„ä¼°

å®Œæ•´æµç¨‹ï¼š
1. åŠ è½½ç”Ÿæˆå™¨æ¨¡å‹
2. ä»éªŒè¯é›†promptç”ŸæˆLoRAå‚æ•°
3. å°†å‚æ•°è½¬æ¢ä¸ºå®é™…LoRAæƒé‡
4. åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹ï¼ˆQwen2.5-0.5Bï¼‰
5. åœ¨ARC Challengeæµ‹è¯•é›†ä¸Šè¯„ä¼°å‡†ç¡®ç‡

ä½¿ç”¨æ–¹æ³•ï¼š
python evaluate_generated_lora.py --generator_ckpt Lora_Gen/experiments/lora_generator_20250721_224455/checkpoints/last.ckpt
"""

import os
import sys
import json
import argparse
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from tqdm import tqdm
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))
sys.path.append(str(project_root.parent))

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType
import torch.nn.functional as F

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from core.lightning_module import LoRAGeneratorLightningModule


def load_base_model(model_name: str = "models/Qwen-Qwen2.5-0.5B", device: str = "cuda"):
    """åŠ è½½åŸºç¡€è¯­è¨€æ¨¡å‹"""
    print(f"ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹: {model_name}")
    
    # é¦–å…ˆå°è¯•æœ¬åœ°è·¯å¾„
    model_path = Path(__file__).parent.parent / model_name
    if not model_path.exists():
        # å¦‚æœæœ¬åœ°ä¸å­˜åœ¨ï¼Œä½¿ç”¨HuggingFace
        model_name = "Qwen/Qwen2.5-0.5B"
        print(f"   ä½¿ç”¨HuggingFaceæ¨¡å‹: {model_name}")
    else:
        model_name = str(model_path)
        print(f"   ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map=device,
            trust_remote_code=True
        )
        
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
        raise


def generate_lora_weights(lightning_module: LoRAGeneratorLightningModule, prompt: str) -> torch.Tensor:
    """ä»promptç”ŸæˆLoRAæƒé‡"""
    print(f"ğŸ§  ä»promptç”ŸæˆLoRAæƒé‡...")
    print(f"ğŸ“ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
    
    lightning_module.eval()
    with torch.no_grad():
        # ç”Ÿæˆå‚æ•°tokens [1, 512, 384]
        lora_tokens = lightning_module.generator([prompt])
        
        # å±•å¹³ä¸ºæƒé‡å‘é‡
        weights = lora_tokens[0].flatten()  # [512*384] = [196608]
        
        print(f"âœ… ç”ŸæˆLoRAæƒé‡ï¼Œæ€»å‚æ•°æ•°: {len(weights)}")
        return weights


def apply_lora_to_model(base_model, lora_weights: torch.Tensor, lora_rank: int = 16) -> torch.nn.Module:
    """å°†ç”Ÿæˆçš„LoRAæƒé‡åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹"""
    print(f"ğŸ”§ åº”ç”¨LoRAæƒé‡åˆ°æ¨¡å‹ (rank={lora_rank})")
    
    # åˆ›å»ºLoRAé…ç½®
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=lora_rank,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]  # é’ˆå¯¹attentionå±‚
    )
    
    # åº”ç”¨LoRA
    peft_model = get_peft_model(base_model, lora_config)
    
    # å°†ç”Ÿæˆçš„æƒé‡åº”ç”¨åˆ°LoRAå‚æ•°
    weight_idx = 0
    applied_modules = 0
    total_applied_params = 0
    
    for name, module in peft_model.named_modules():
        if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
            try:
                # è·å–LoRA Aå’ŒBçŸ©é˜µçš„å¤§å°
                lora_A_size = module.lora_A['default'].weight.numel()
                lora_B_size = module.lora_B['default'].weight.numel()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æƒé‡
                if weight_idx + lora_A_size + lora_B_size <= len(lora_weights):
                    # åº”ç”¨LoRA Aæƒé‡
                    lora_A_data = lora_weights[weight_idx:weight_idx + lora_A_size]
                    module.lora_A['default'].weight.data = lora_A_data.view_as(module.lora_A['default'].weight).to(module.lora_A['default'].weight.device)
                    weight_idx += lora_A_size
                    
                    # åº”ç”¨LoRA Bæƒé‡
                    lora_B_data = lora_weights[weight_idx:weight_idx + lora_B_size]
                    module.lora_B['default'].weight.data = lora_B_data.view_as(module.lora_B['default'].weight).to(module.lora_B['default'].weight.device)
                    weight_idx += lora_B_size
                    
                    applied_modules += 1
                    total_applied_params += lora_A_size + lora_B_size
                    
                    print(f"  âœ… åº”ç”¨åˆ° {name}: A{list(module.lora_A['default'].weight.shape)} + B{list(module.lora_B['default'].weight.shape)}")
                else:
                    print(f"  âš ï¸ {name}: æƒé‡ä¸è¶³ï¼Œè·³è¿‡")
                    
            except Exception as e:
                print(f"  âŒ {name}: åº”ç”¨å¤±è´¥ - {e}")
                continue
    
    print(f"âœ… LoRAåº”ç”¨å®Œæˆ:")
    print(f"  - åº”ç”¨æ¨¡å—æ•°: {applied_modules}")
    print(f"  - åº”ç”¨å‚æ•°æ•°: {total_applied_params}")
    print(f"  - ä½¿ç”¨æƒé‡æ¯”ä¾‹: {weight_idx/len(lora_weights)*100:.1f}%")
    
    return peft_model


def evaluate_on_arc_challenge(model, tokenizer, test_data_path: str, max_samples: int = 100) -> Dict[str, Any]:
    """åœ¨ARC Challengeæµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    print(f"ğŸ“Š åœ¨ARC Challengeä¸Šè¯„ä¼°æ¨¡å‹")
    print(f"æµ‹è¯•æ–‡ä»¶: {test_data_path}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    # è¯»å–æµ‹è¯•æ•°æ®
    test_samples = []
    with open(test_data_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= max_samples:
                break
            if line.strip():
                sample = json.loads(line.strip())
                test_samples.append(sample)
    
    print(f"ğŸ“ˆ å®é™…æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
    
    correct = 0
    total = 0
    results = []
    
    model.eval()
    
    for sample in tqdm(test_samples, desc="è¯„ä¼°ä¸­"):
        try:
            # è§£ææ ·æœ¬
            question = sample.get('input', '')
            options = sample.get('options', [])
            correct_answer = sample.get('target', '')
            
            if not question or not options or not correct_answer:
                continue
            
            # æ„å»ºå¤šé€‰é¢˜prompt
            prompt = f"Question: {question}\n"
            choice_letters = []
            
            for option in options:
                # æå–é€‰é¡¹å­—æ¯å’Œå†…å®¹ (æ ¼å¼: "A: content")
                if ':' in option:
                    letter = option.split(':')[0].strip()
                    content = ':'.join(option.split(':')[1:]).strip()
                    prompt += f"{letter}: {content}\n"
                    choice_letters.append(letter)
            
            prompt += "Answer:"
            
            # Tokenizeè¾“å…¥
            inputs = tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512,
                padding=True
            ).to(model.device)
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=5,
                    do_sample=False,
                    temperature=0.0,
                    pad_token_id=tokenizer.eos_token_id,
                    num_beams=1
                )
            
            # è§£æç”Ÿæˆçš„ç­”æ¡ˆ
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer_part = generated_text[len(prompt):].strip()
            
            # æå–ç­”æ¡ˆå­—æ¯
            predicted_answer = None
            for letter in choice_letters:
                if letter in answer_part.upper():
                    predicted_answer = letter
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå­—ç¬¦
            if predicted_answer is None and answer_part:
                first_char = answer_part[0].upper()
                if first_char in choice_letters:
                    predicted_answer = first_char
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
            if predicted_answer is None:
                predicted_answer = choice_letters[0] if choice_letters else 'A'
            
            # æ£€æŸ¥æ­£ç¡®æ€§
            is_correct = predicted_answer == correct_answer
            if is_correct:
                correct += 1
            
            total += 1
            
            # è®°å½•ç»“æœ
            results.append({
                'question': question,
                'options': options,
                'correct_answer': correct_answer,
                'predicted_answer': predicted_answer,
                'is_correct': is_correct,
                'generated_text': answer_part,
                'confidence': 1.0  # ç®€åŒ–ç‰ˆæœ¬
            })
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„ç»“æœ
            if len(results) <= 5:
                status = "âœ…" if is_correct else "âŒ"
                print(f"  {status} æ ·æœ¬ {len(results)}: {correct_answer} -> {predicted_answer}")
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
            continue
    
    accuracy = correct / total if total > 0 else 0.0
    
    metrics = {
        'accuracy': accuracy,
        'correct': correct,
        'total': total,
        'error_rate': 1 - accuracy
    }
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  - æ€»æ ·æœ¬æ•°: {total}")
    print(f"  - æ­£ç¡®ç­”æ¡ˆ: {correct}")
    print(f"  - å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    return metrics, results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”ŸæˆLoRAå¹¶åœ¨ARC Challengeä¸Šè¯„ä¼°")
    parser.add_argument("--generator_ckpt", type=str, 
                       default="Lora_Gen/experiments/lora_generator_20250721_224455/checkpoints/last.ckpt",
                       help="ç”Ÿæˆå™¨checkpointè·¯å¾„")
    parser.add_argument("--base_model", type=str, 
                       default="models/Qwen-Qwen2.5-0.5B",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test_data", type=str, 
                       default="../data_to_lora/cs/arc-challenge/arc-challenge_test_formatted.jsonl",
                       help="ARC Challengeæµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--val_prompts", type=str, 
                       default="data/arc-challenge/val_prompts.jsonl",
                       help="éªŒè¯é›†promptsæ–‡ä»¶")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--output_dir", type=str, default="evaluation_results",
                       help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--max_test_samples", type=int, default=200,
                       help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°")
    parser.add_argument("--lora_rank", type=int, default=16,
                       help="LoRA rank")
    parser.add_argument("--prompt_idx", type=int, default=0,
                       help="ä½¿ç”¨ç¬¬å‡ ä¸ªéªŒè¯promptç”ŸæˆLoRA")
    parser.add_argument("--baseline_only", action="store_true",
                       help="åªç”¨åŸºç¡€æ¨¡å‹è¯„ä¼°ï¼Œä¸åº”ç”¨LoRAï¼ˆä½œä¸ºbaselineï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ LoRAç”Ÿæˆ + ARC Challengeè¯„ä¼°")
    print("=" * 70)
    print(f"ç”Ÿæˆå™¨checkpoint: {args.generator_ckpt}")
    print(f"åŸºç¡€æ¨¡å‹: {args.base_model}")
    print(f"æµ‹è¯•æ•°æ®: {args.test_data}")
    print(f"æœ€å¤§æµ‹è¯•æ ·æœ¬: {args.max_test_samples}")
    print(f"ä»…baselineæ¨¡å¼: {args.baseline_only}")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # 4. åŠ è½½åŸºç¡€æ¨¡å‹
        base_model, tokenizer = load_base_model(args.base_model, args.device)
        
        if args.baseline_only:
            print("ğŸ” Baselineæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹è¯„ä¼°")
            evaluation_model = base_model
        else:
            # 1. åŠ è½½ç”Ÿæˆå™¨æ¨¡å‹
            print("ğŸ”„ åŠ è½½ç”Ÿæˆå™¨æ¨¡å‹...")
            try:
                lightning_module = LoRAGeneratorLightningModule.load_from_checkpoint(
                    args.generator_ckpt,
                    map_location=args.device
                )
                lightning_module.eval()
                lightning_module = lightning_module.to(args.device)
            except Exception as e:
                print(f"âŒ åŠ è½½ç”Ÿæˆå™¨å¤±è´¥ (å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜): {e}")
                print("ğŸ”„ å°è¯•åˆ›å»ºæ¨¡æ‹Ÿçš„LoRAæƒé‡...")
                
                # åˆ›å»ºæ¨¡æ‹Ÿçš„LoRAæƒé‡ç”¨äºæµ‹è¯•
                lora_weights = torch.randn(512 * 384) * 0.01  # æ¨¡æ‹Ÿç”Ÿæˆçš„æƒé‡
                print("âœ… ä½¿ç”¨æ¨¡æ‹ŸLoRAæƒé‡ç»§ç»­æµ‹è¯•")
                
                # è·³è½¬åˆ°åŸºç¡€æ¨¡å‹åŠ è½½
                lightning_module = None
            
            # 2. è¯»å–éªŒè¯é›†prompts
            print(f"ğŸ“‹ è¯»å–éªŒè¯é›†prompts...")
            val_prompts = []
            with open(args.val_prompts, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        val_prompts.append(data['prompt'])
            
            if args.prompt_idx >= len(val_prompts):
                args.prompt_idx = 0
                
            selected_prompt = val_prompts[args.prompt_idx]
            print(f"ğŸ“ ä½¿ç”¨prompt {args.prompt_idx+1}/{len(val_prompts)} (é•¿åº¦: {len(selected_prompt)})")
            
            # 3. ç”ŸæˆLoRAæƒé‡
            lora_weights = generate_lora_weights(lightning_module, selected_prompt)
            
            # 5. åº”ç”¨LoRAæƒé‡
            evaluation_model = apply_lora_to_model(base_model, lora_weights, args.lora_rank)
        
        # 6. åœ¨ARC Challengeä¸Šè¯„ä¼°
        metrics, results = evaluate_on_arc_challenge(
            evaluation_model, tokenizer, args.test_data, args.max_test_samples
        )
        
        # 7. ä¿å­˜ç»“æœ
        final_results = {
            'config': {
                'generator_checkpoint': args.generator_ckpt if not args.baseline_only else "N/A (baseline)",
                'base_model': args.base_model,
                'test_data': args.test_data,
                'lora_rank': args.lora_rank if not args.baseline_only else "N/A (baseline)",
                'prompt_idx': args.prompt_idx if not args.baseline_only else "N/A (baseline)",
                'max_test_samples': args.max_test_samples,
                'baseline_only': args.baseline_only
            },
            'prompt_info': {
                'selected_prompt': selected_prompt[:500] + "..." if not args.baseline_only and len(selected_prompt) > 500 else (selected_prompt if not args.baseline_only else "N/A (baseline)"),
                'prompt_length': len(selected_prompt) if not args.baseline_only else "N/A (baseline)"
            } if not args.baseline_only else {"baseline_mode": True},
            'metrics': metrics,
            'sample_results': results[:10],  # åªä¿å­˜å‰10ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœ
            'summary': {
                'accuracy': metrics['accuracy'],
                'total_samples': metrics['total'],
                'correct_predictions': metrics['correct'],
                'model_type': 'baseline' if args.baseline_only else 'lora_enhanced'
            }
        }
        
        results_file = output_dir / f"arc_challenge_evaluation_{'baseline' if args.baseline_only else f'prompt_{args.prompt_idx}'}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
        print(f"  ğŸ“Š å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
        print(f"  âœ… æ­£ç¡®: {metrics['correct']}/{metrics['total']}")
        print(f"  ğŸ·ï¸  æ¨¡å‹ç±»å‹: {'ğŸ”¥ LoRAå¢å¼º' if not args.baseline_only else 'ğŸ“‹ åŸå§‹åŸºçº¿'}")
        print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {results_file}")
        
        # æ˜¾ç¤ºä¸€äº›æ ·æœ¬ç»“æœ
        print(f"\nğŸ“‹ æ ·æœ¬ç»“æœé¢„è§ˆ:")
        for i, result in enumerate(results[:3]):
            status = "âœ…" if result['is_correct'] else "âŒ"
            print(f"  {status} æ ·æœ¬ {i+1}: {result['correct_answer']} -> {result['predicted_answer']}")
            print(f"     é—®é¢˜: {result['question'][:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
