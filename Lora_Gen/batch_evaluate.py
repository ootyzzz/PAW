#!/usr/bin/env python3
"""
batch_evaluate.py
æ‰¹é‡è¯„ä¼°ä¸åŒpromptç”Ÿæˆçš„LoRAæƒé‡æ€§èƒ½

è¿è¡Œå¤šä¸ªvalidation promptså¹¶æ¯”è¾ƒç»“æœ
"""

import subprocess
import json
from pathlib import Path

def run_evaluation(prompt_idx, max_samples=100):
    """è¿è¡Œå•ä¸ªpromptçš„è¯„ä¼°"""
    cmd = [
        "python", "evaluate_generated_lora.py",
        "--prompt_idx", str(prompt_idx),
        "--max_test_samples", str(max_samples)
    ]
    
    print(f"ğŸ”„ è¿è¡Œprompt {prompt_idx}çš„è¯„ä¼°...")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"âœ… Prompt {prompt_idx} è¯„ä¼°å®Œæˆ")
        return True
    else:
        print(f"âŒ Prompt {prompt_idx} è¯„ä¼°å¤±è´¥: {result.stderr}")
        return False

def collect_results():
    """æ”¶é›†æ‰€æœ‰ç»“æœ"""
    results_dir = Path("evaluation_results")
    results = []
    
    # åŸºçº¿ç»“æœ
    baseline_file = results_dir / "arc_challenge_evaluation_baseline.json"
    if baseline_file.exists():
        with open(baseline_file, 'r', encoding='utf-8') as f:
            baseline_data = json.load(f)
            results.append({
                'type': 'baseline',
                'prompt_idx': -1,
                'accuracy': baseline_data['metrics']['accuracy'],
                'correct': baseline_data['metrics']['correct'],
                'total': baseline_data['metrics']['total']
            })
    
    # LoRAç»“æœ
    for i in range(10):  # å‡è®¾æœ€å¤š10ä¸ªprompts
        lora_file = results_dir / f"arc_challenge_evaluation_prompt_{i}.json"
        if lora_file.exists():
            with open(lora_file, 'r', encoding='utf-8') as f:
                lora_data = json.load(f)
                results.append({
                    'type': 'lora',
                    'prompt_idx': i,
                    'accuracy': lora_data['metrics']['accuracy'],
                    'correct': lora_data['metrics']['correct'],
                    'total': lora_data['metrics']['total']
                })
    
    return results

def plot_results(results):
    """ç»˜åˆ¶ç»“æœå¯¹æ¯”å›¾"""
    baseline_acc = None
    lora_results = []
    
    for r in results:
        if r['type'] == 'baseline':
            baseline_acc = r['accuracy']
        else:
            lora_results.append((r['prompt_idx'], r['accuracy']))
    
    if not lora_results:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°LoRAè¯„ä¼°ç»“æœ")
        return
    
    # æ’åº
    lora_results.sort()
    prompt_indices, accuracies = zip(*lora_results)
    
    # ç»˜å›¾
    plt.figure(figsize=(12, 6))
    
    # LoRAç»“æœ
    bars = plt.bar(range(len(prompt_indices)), accuracies, alpha=0.7, label='LoRA Enhanced')
    
    # åŸºçº¿
    if baseline_acc is not None:
        plt.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label=f'Baseline ({baseline_acc:.1%})')
    
    plt.xlabel('Validation Prompt Index')
    plt.ylabel('Accuracy')
    plt.title('LoRA Generator Performance Across Different Validation Prompts')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, accuracies)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, 
                f'{acc:.1%}', ha='center', va='bottom')
    
    plt.xticks(range(len(prompt_indices)), [f'P{i}' for i in prompt_indices])
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig('evaluation_results/lora_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: evaluation_results/lora_performance_comparison.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ‰¹é‡LoRAè¯„ä¼°å®éªŒ")
    print("=" * 50)
    
    # ç¡®ä¿baselineå­˜åœ¨
    baseline_file = Path("evaluation_results/arc_challenge_evaluation_baseline.json")
    if not baseline_file.exists():
        print("âš ï¸ æœªæ‰¾åˆ°baselineç»“æœï¼Œå…ˆè¿è¡Œbaselineè¯„ä¼°...")
        subprocess.run(["python", "evaluate_generated_lora.py", "--baseline_only", "--max_test_samples", "100"])
    
    # æ‰¹é‡è¿è¡Œä¸åŒpromptçš„è¯„ä¼°
    max_prompts = 5  # æµ‹è¯•å‰5ä¸ªprompts
    successful_runs = 0
    
    for i in range(max_prompts):
        if run_evaluation(i, max_samples=100):
            successful_runs += 1
    
    print(f"\nğŸ“ˆ å®Œæˆ {successful_runs}/{max_prompts} ä¸ªpromptçš„è¯„ä¼°")
    
    # æ”¶é›†å¹¶åˆ†æç»“æœ
    results = collect_results()
    
    if results:
        print("\nğŸ“Š ç»“æœæ±‡æ€»:")
        print("-" * 60)
        for r in results:
            if r['type'] == 'baseline':
                print(f"ğŸ“‹ Baseline:        {r['accuracy']:.1%} ({r['correct']}/{r['total']})")
            else:
                print(f"ğŸ”¥ LoRA Prompt {r['prompt_idx']:2d}: {r['accuracy']:.1%} ({r['correct']}/{r['total']})")
        
        # æ‰¾å‡ºæœ€ä½³LoRAç»“æœ
        lora_results = [r for r in results if r['type'] == 'lora']
        if lora_results:
            best_lora = max(lora_results, key=lambda x: x['accuracy'])
            baseline = next((r for r in results if r['type'] == 'baseline'), None)
            
            if baseline:
                improvement = best_lora['accuracy'] - baseline['accuracy']
                relative_improvement = improvement / baseline['accuracy'] * 100
                
                print(f"\nğŸ¯ æœ€ä½³ç»“æœ:")
                print(f"   æœ€ä½³LoRA (Prompt {best_lora['prompt_idx']}): {best_lora['accuracy']:.1%}")
                print(f"   ç›¸æ¯”Baselineæå‡: +{improvement:.1%} (ç›¸å¯¹+{relative_improvement:.1f}%)")
        
        # ç»˜åˆ¶ç»“æœ
        plot_results(results)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")

if __name__ == "__main__":
    main()
