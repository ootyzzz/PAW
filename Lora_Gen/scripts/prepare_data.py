#!/usr/bin/env python3
"""
scripts/prepare_data.py
æ•°æ®å‡†å¤‡è„šæœ¬

ä½¿ç”¨æ–¹æ³•ï¼š
python scripts/prepare_data.py --source data_to_lora/cs/arc-challenge/arc-challenge_train_formatted.jsonl --output_dir Lora_Gen/data
python scripts/prepare_data.py --checkpoint_dir runs/arc-challenge_lora_20250721_005053/checkpoints --analyze
"""
import os
import sys
import argparse
import json
import random
from pathlib import Path
from typing import List, Dict, Any
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from Lora_Gen.core.data_module import create_prompt_splits
from Lora_Gen.core.generator import LoRATokenizer


def analyze_checkpoints(checkpoint_dir: str):
    """åˆ†æcheckpointæ–‡ä»¶"""
    checkpoint_dir = Path(checkpoint_dir)
    
    print(f"ğŸ“Š åˆ†æCheckpointç›®å½•: {checkpoint_dir}")
    print("=" * 60)
    
    # æŸ¥æ‰¾checkpointæ–‡ä»¶
    checkpoint_files = []
    for ext in ['*.ckpt', '*.pt', '*.pth']:
        checkpoint_files.extend(list(checkpoint_dir.glob(ext)))
    
    if not checkpoint_files:
        print("âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
        return
    
    print(f"ğŸ“ å‘ç° {len(checkpoint_files)} ä¸ªcheckpointæ–‡ä»¶:")
    
    # åˆ†ææ¯ä¸ªcheckpoint
    total_params = 0
    lora_params = 0
    file_sizes = []
    
    for i, ckpt_file in enumerate(checkpoint_files[:10]):  # åªåˆ†æå‰10ä¸ª
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = ckpt_file.stat().st_size / (1024**3)  # GB
            file_sizes.append(file_size)
            
            # åŠ è½½checkpoint - å¤„ç†PyTorch 2.6
            try:
                checkpoint = torch.load(ckpt_file, map_location='cpu', weights_only=False)
            except Exception:
                checkpoint = torch.load(ckpt_file, map_location='cpu')
            
            # ç»Ÿè®¡å‚æ•°
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    params = checkpoint['state_dict']
                else:
                    params = checkpoint
                
                file_total = 0
                file_lora = 0
                
                for name, param in params.items():
                    param_count = param.numel()
                    file_total += param_count
                    
                    if 'lora' in name.lower():
                        file_lora += param_count
                
                total_params += file_total
                lora_params += file_lora
                
                print(f"  {i+1:2d}. {ckpt_file.name}")
                print(f"      å¤§å°: {file_size:.2f} GB")
                print(f"      æ€»å‚æ•°: {file_total:,}")
                print(f"      LoRAå‚æ•°: {file_lora:,}")
                
        except Exception as e:
            print(f"  {i+1:2d}. {ckpt_file.name} - âŒ åŠ è½½å¤±è´¥: {e}")
    
    print(f"\\nğŸ“ˆ ç»Ÿè®¡æ‘˜è¦:")
    print(f"  - æ–‡ä»¶æ•°é‡: {len(checkpoint_files)}")
    print(f"  - å¹³å‡æ–‡ä»¶å¤§å°: {sum(file_sizes)/len(file_sizes):.2f} GB")
    print(f"  - å¹³å‡æ€»å‚æ•°: {total_params//len(file_sizes):,}")
    print(f"  - å¹³å‡LoRAå‚æ•°: {lora_params//len(file_sizes):,}")


def test_tokenization(checkpoint_file: str, max_tokens: int = 512, token_dim: int = 384):
    """æµ‹è¯•checkpointçš„tokenization"""
    print(f"ğŸ§ª æµ‹è¯•Checkpoint Tokenization")
    print(f"æ–‡ä»¶: {checkpoint_file}")
    print(f"é…ç½®: max_tokens={max_tokens}, token_dim={token_dim}")
    print("=" * 50)
    
    try:
        tokenizer = LoRATokenizer(max_tokens=max_tokens, token_dim=token_dim)
        
        # Tokenize
        tokens = tokenizer.tokenize_checkpoint(checkpoint_file)
        
        print(f"âœ… TokenizationæˆåŠŸ!")
        print(f"  - Tokenå½¢çŠ¶: {tokens.shape}")
        print(f"  - Tokenç»Ÿè®¡:")
        print(f"    Mean: {tokens.mean():.6f}")
        print(f"    Std: {tokens.std():.6f}")
        print(f"    Min: {tokens.min():.6f}")
        print(f"    Max: {tokens.max():.6f}")
        
        # æµ‹è¯•detokenization
        param_vec = tokenizer.detokenize(tokens)
        print(f"  - Detokenizationå½¢çŠ¶: {param_vec.shape}")
        
    except Exception as e:
        print(f"âŒ Tokenizationå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def prepare_training_data(
    source_file: str,
    output_dir: str,
    samples_per_prompt: int = 4,
    test_ratio: float = 0.1
):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print(f"ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®")
    print(f"æºæ–‡ä»¶: {source_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ¯ä¸ªpromptæ ·æœ¬æ•°: {samples_per_prompt}")
    print(f"æµ‹è¯•æ¯”ä¾‹: {test_ratio}")
    print("=" * 50)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    train_file = output_dir / "train_prompts.jsonl"
    val_file = output_dir / "val_prompts.jsonl"
    
    try:
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        create_prompt_splits(
            input_file=source_file,
            train_output=str(train_file),
            val_output=str(val_file),
            samples_per_prompt=samples_per_prompt,
            test_ratio=test_ratio
        )
        
        # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
        train_count = 0
        val_count = 0
        
        with open(train_file, 'r', encoding='utf-8') as f:
            train_count = sum(1 for line in f if line.strip())
        
        with open(val_file, 'r', encoding='utf-8') as f:
            val_count = sum(1 for line in f if line.strip())
        
        print(f"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
        print(f"  - è®­ç»ƒæ ·æœ¬: {train_count}")
        print(f"  - éªŒè¯æ ·æœ¬: {val_count}")
        print(f"  - è®­ç»ƒæ–‡ä»¶: {train_file}")
        print(f"  - éªŒè¯æ–‡ä»¶: {val_file}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®å‡†å¤‡å·¥å…·")
    
    # å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    prep_parser = subparsers.add_parser('prepare', help='å‡†å¤‡è®­ç»ƒæ•°æ®')
    prep_parser.add_argument('--source', type=str, required=True,
                            help='æºæ•°æ®æ–‡ä»¶è·¯å¾„')
    prep_parser.add_argument('--output_dir', type=str, default='Lora_Gen/data',
                            help='è¾“å‡ºç›®å½•')
    prep_parser.add_argument('--samples_per_prompt', type=int, default=4,
                            help='æ¯ä¸ªpromptçš„æ ·æœ¬æ•°')
    prep_parser.add_argument('--test_ratio', type=float, default=0.1,
                            help='æµ‹è¯•é›†æ¯”ä¾‹')
    
    # åˆ†æcheckpoint
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†æcheckpointæ–‡ä»¶')
    analyze_parser.add_argument('--checkpoint_dir', type=str, required=True,
                               help='Checkpointç›®å½•è·¯å¾„')
    
    # æµ‹è¯•tokenization
    test_parser = subparsers.add_parser('test', help='æµ‹è¯•tokenization')
    test_parser.add_argument('--checkpoint_file', type=str, required=True,
                            help='å•ä¸ªcheckpointæ–‡ä»¶è·¯å¾„')
    test_parser.add_argument('--max_tokens', type=int, default=512,
                            help='æœ€å¤§tokenæ•°')
    test_parser.add_argument('--token_dim', type=int, default=384,
                            help='Tokenç»´åº¦')
    
    # å…¼å®¹æ—§çš„å‘½ä»¤è¡Œå‚æ•°æ ¼å¼
    parser.add_argument('--source', type=str, help='æºæ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='Lora_Gen/data',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--checkpoint_dir', type=str, help='Checkpointç›®å½•è·¯å¾„')
    parser.add_argument('--analyze', action='store_true', help='åˆ†æcheckpointæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ”§ LoRA Generator æ•°æ®å‡†å¤‡å·¥å…·")
    print("=" * 50)
    
    try:
        if args.command == 'prepare':
            prepare_training_data(
                source_file=args.source,
                output_dir=args.output_dir,
                samples_per_prompt=args.samples_per_prompt,
                test_ratio=args.test_ratio
            )
        elif args.command == 'analyze':
            analyze_checkpoints(args.checkpoint_dir)
        elif args.command == 'test':
            test_tokenization(
                checkpoint_file=args.checkpoint_file,
                max_tokens=args.max_tokens,
                token_dim=args.token_dim
            )
        else:
            # å…¼å®¹æ—§æ ¼å¼
            if args.analyze and args.checkpoint_dir:
                analyze_checkpoints(args.checkpoint_dir)
            elif args.source:
                prepare_training_data(args.source, args.output_dir)
            else:
                parser.print_help()
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
