#!/usr/bin/env python3
"""
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
åŒ…å«è®­ç»ƒæ‰§è¡Œã€å›è°ƒè®¾ç½®ã€SwanLabé›†æˆç­‰åŠŸèƒ½
"""

import os
import yaml
from pathlib import Path
from typing import Dict, Any, List

import torch
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
import swanlab

from peft import TaskType
from .model import LoRALightningModule
from .data import TrainTestDataModule


class SwanLabLogger:
    """SwanLab Logger for Lightning"""
    
    def __init__(self, project_name: str, experiment_name: str, config: Dict[str, Any]):
        self.project_name = project_name
        self.experiment_name = experiment_name 
        self.config = config
        self._run = None
        
    def initialize_run(self):
        """åˆå§‹åŒ– SwanLab run"""
        if self._run is None:
            self._run = swanlab.init(
                project=self.project_name,
                experiment_name=self.experiment_name,
                config=self.config
            )
        return self._run


def setup_callbacks(config: Dict[str, Any]) -> List[pl.Callback]:
    """è®¾ç½®Lightningå›è°ƒï¼ˆå­¦ä¹ ç‡ç›‘æ§ã€æ—©åœï¼‰"""
    callbacks = []
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # æ—©åœå›è°ƒ - éªŒè¯å‡†ç¡®ç‡50æ­¥æ— æå‡åˆ™åœæ­¢
    early_stopping = EarlyStopping(
        monitor='val_accuracy',
        patience=50,
        mode='max',
        verbose=True,
        min_delta=0.001  # è‡³å°‘æå‡0.1%æ‰ç®—æ”¹å–„
    )
    callbacks.append(early_stopping)
    
    # æ³¨æ„ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨å°†åœ¨æ¨¡å‹å†…éƒ¨é…ç½®
    
    return callbacks


def run_lightning_training(
    dataset_name: str,
    config: Dict[str, Any],
    dry_run: bool = False
) -> Dict[str, Any]:
    """è¿è¡ŒLightningè®­ç»ƒ"""
    
    print(f"\n{'=' * 70}")
    print(f"ğŸš€ Lightning LoRA è®­ç»ƒ: {dataset_name}")
    print(f"{'=' * 70}")
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    train_file = config['data']['train_file']
    test_file = config['data']['test_file']
    using_validation_as_test = config['data'].get('using_validation_as_test', False)
    
    for file_path, file_type in [(train_file, "è®­ç»ƒ"), (test_file, "æµ‹è¯•")]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_type}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"ğŸ“ è®­ç»ƒæ–‡ä»¶: {train_file}")
    if using_validation_as_test:
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file} (ä½¿ç”¨validationä½œä¸ºtest)")
        print("ğŸ’¡ æ•°æ®é›†é…ç½®: é€‚é…åªæœ‰train/validationçš„æ•°æ®é›†")
    else:
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ¯ å®éªŒç›®å½•: {config['paths']['experiment_dir']}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: batch_size={config['training']['batch_size']}, steps={config['training']['max_steps']} ({config['training']['stage1_steps']}+{config['training']['stage2_steps']}) - ä¿å­˜æœ€å{config['training']['save_steps']}æ­¥")
    
    if dry_run:
        print("ğŸƒ Dry Run å®Œæˆ - å·²éªŒè¯:")
        print("  âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®")
        print("  âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®")
        print("  âœ… å®éªŒç›®å½•ç»“æ„åˆ›å»º")
        print("  âœ… LoRAé…ç½®æœ‰æ•ˆ")
        print("  âœ… è®­ç»ƒå‚æ•°åˆç†")
        print("  ğŸ’¡ è¦å®é™…è®­ç»ƒè¯·ç§»é™¤ --dry_run å‚æ•°")
        return {"status": "dry_run_completed"}
    
    try:
        # åˆ›å»ºå®éªŒç›®å½•
        experiment_dir = Path(config['paths']['experiment_dir'])
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        with open(config['paths']['config_file'], 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, indent=2, allow_unicode=True)
        
        # åˆå§‹åŒ– SwanLab
        print("ğŸ“Š åˆå§‹åŒ– SwanLab...")
        swanlab_run = swanlab.init(
            project=f"lora-training",
            experiment_name=config['experiment']['name'],
            config=config,
            logdir=config['paths']['swanlab_dir']
        )
        
        # åˆ›å»ºæ•°æ®æ¨¡å—
        batch_size = config['training']['batch_size']
            
        data_module = TrainTestDataModule(
            dataset_name=dataset_name,
            batch_size=batch_size,
            test_mode=False  # ç§»é™¤test_modeï¼Œå§‹ç»ˆä½¿ç”¨æ ‡å‡†æ¨¡å¼
        )
        
        print(f"ğŸ“Š æ•°æ®æ¨¡å—é…ç½®:")
        print(f"  - Trainæ–‡ä»¶: {data_module.train_file}")
        print(f"  - Testæ–‡ä»¶: {data_module.test_file}")
        print(f"  - Batchå¤§å°: {batch_size}")
        print(f"  - Train Shuffle: True, Test Shuffle: False")
        
        # åˆ›å»ºLightningæ¨¡å—
        model_path = config.get('model', {}).get('path', 'models/Qwen-Qwen2.5-0.5B')
        if not os.path.exists(model_path):
            model_path = config.get('model', {}).get('name', 'Qwen/Qwen2.5-0.5B')
        
        lora_config = {
            'r': config.get('lora', {}).get('r', 16),
            'lora_alpha': config.get('lora', {}).get('alpha', 32),
            'target_modules': config.get('lora', {}).get('target_modules', ["q_proj", "v_proj"]),
            'lora_dropout': config.get('lora', {}).get('dropout', 0.1),
            'bias': config.get('lora', {}).get('bias', "none"),
            'task_type': TaskType.CAUSAL_LM
        }
        
        lightning_module = LoRALightningModule(
            model_path=model_path,
            lora_config=lora_config,
            learning_rate_stage1=config['training']['learning_rate_stage1'],
            learning_rate_stage2=config['training']['learning_rate_stage2'],
            stage1_steps=config['training']['stage1_steps'],
            stage2_steps=config['training']['stage2_steps'],
        )
        
        # å°†SwanLab runæ·»åŠ åˆ°æ¨¡å—ä¸­
        lightning_module._swanlab_run = swanlab_run
        
        # è®¾ç½®å›è°ƒ
        callbacks = setup_callbacks(config)
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        tensorboard_logger = TensorBoardLogger(
            save_dir=config['paths']['tensorboard_dir'],
            name="",
            version=""
        )
        
        # åˆ›å»ºTrainerï¼ˆç¦ç”¨è‡ªåŠ¨checkpointä¿å­˜ï¼‰
        trainer = Trainer(
            max_steps=config['training']['max_steps'],
            callbacks=callbacks,
            logger=tensorboard_logger,
            enable_progress_bar=True,
            log_every_n_steps=1,
            enable_checkpointing=False,  # ç¦ç”¨æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
            precision='16-mixed' if torch.cuda.is_available() else 32,
            accelerator='auto',
            devices='auto',
            strategy='auto',
        )
        
        print(f"\nğŸƒâ€â™‚ï¸ å¼€å§‹Lightningè®­ç»ƒ...")
        print(f"ğŸ“Š è®­ç»ƒå™¨é…ç½®:")
        print(f"  - æœ€å¤§æ­¥æ•°: {config['training']['max_steps']}")
        print(f"  - ç²¾åº¦: {'16-mixed' if torch.cuda.is_available() else '32'}")
        print(f"  - åŠ é€Ÿå™¨: {trainer.accelerator}")
        print(f"  - è®¾å¤‡æ•°: {trainer.num_devices}")
        
        # å¼€å§‹è®­ç»ƒï¼ˆåŒ…å«éªŒè¯ï¼‰
        trainer.fit(lightning_module, datamodule=data_module)
        
        # åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        test_results = trainer.test(lightning_module, datamodule=data_module)
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        for key, value in test_results[0].items():
            print(f"  - {key}: {value:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = Path(config['paths']['final_model_dir'])
        final_model_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {final_model_dir}")
        lightning_module.model.save_pretrained(final_model_dir)
        lightning_module.tokenizer.save_pretrained(final_model_dir)
        
        # å…³é—­SwanLab
        swanlab.finish()
        
        results = {
            "status": "training_completed",
            "experiment_dir": str(experiment_dir),
            "final_model_dir": str(final_model_dir),
            "total_steps": config['training']['max_steps'],
            "test_results": test_results[0] if test_results else {},
            "framework": "lightning_swanlab"
        }
        
        print(f"\nâœ… Lightningè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_dir}")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹: {final_model_dir}")
        print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {config['training']['max_steps']}")
        print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ: {test_results[0] if test_results else 'æœªå®Œæˆ'}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Lightningè®­ç»ƒå¤±è´¥: {e}")
        if 'swanlab_run' in locals():
            swanlab.finish()
        raise
