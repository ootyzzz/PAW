#!/usr/bin/env python3
"""
è®­ç»ƒæ ¸å¿ƒæ¨¡å—
åŒ…å«è®­ç»ƒæ‰§è¡Œã€å›è°ƒè®¾ç½®ã€SwanLabé›†æˆç­‰åŠŸèƒ½
"""

import os
import yaml
import warnings
from pathlib import Path
from typing import Dict, Any, List

# å±è”½Lightningå†—é•¿è¾“å‡º
warnings.filterwarnings("ignore", message=".*sync_dist.*")
warnings.filterwarnings("ignore", message=".*recommended.*")
os.environ['PYTORCH_LIGHTNING_VERBOSITY'] = '1'

import torch
import pytorch_lightning as pl
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
import swanlab

from peft import TaskType
from .model import LoRALightningModule
from .data import TrainTestDataModule


class SwanLabLogger:
    """SwanLab Logger for Lightning"""
    
    def __init__(self, project_name: str, experiment_name: str, config: Dict[str, Any]):
        self.project_name = project_name
        self.experiment_name = experiment_name 
        self.config = config
        self._run = None
        
    def initialize_run(self):
        """åˆå§‹åŒ– SwanLab run"""
        if self._run is None:
            self._run = swanlab.init(
                project=self.project_name,
                experiment_name=self.experiment_name,
                config=self.config
            )
        return self._run


def setup_callbacks(config: Dict[str, Any], steps_per_epoch: int, max_steps: int) -> List[pl.Callback]:
    """è®¾ç½®Lightningå›è°ƒï¼ˆå­¦ä¹ ç‡ç›‘æ§ã€æ—©åœï¼‰"""
    callbacks = []
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # æ™ºèƒ½æ—©åœé…ç½® - åŸºäºepochåˆ†æ
    epochs_to_train = max_steps / steps_per_epoch
    
    if epochs_to_train < 1.0:
        # ä¸è¶³ä¸€ä¸ªepoch - ç¦ç”¨validationå’Œæ—©åœ
        print(f"ğŸ”§ è®­ç»ƒæ­¥æ•°ä¸è¶³ä¸€ä¸ªepoch ({max_steps} < {steps_per_epoch})ï¼šç¦ç”¨validationå’Œæ—©åœ")
        print(f"   ğŸ’¡ å»ºè®®ï¼šå¢åŠ max_stepsåˆ°è‡³å°‘{steps_per_epoch}ä»¥è¦†ç›–ä¸€ä¸ªå®Œæ•´epoch")
        # ä¸æ·»åŠ æ—©åœå›è°ƒ
    elif epochs_to_train < 2.0:
        # å°‘äº2ä¸ªepoch - ä»ç„¶ç¦ç”¨æ—©åœï¼Œé¿å…è¿‡æ—©åœæ­¢
        print(f"ğŸ”§ è®­ç»ƒepochæ•°è¾ƒå°‘ ({epochs_to_train:.2f} epochs)ï¼šç¦ç”¨æ—©åœé¿å…è¿‡æ—©ç»ˆæ­¢")
        # ä¸æ·»åŠ æ—©åœå›è°ƒ
    else:
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼ - å¯ç”¨æ—©åœ
        patience_steps = min(50, int(steps_per_epoch * 0.5))  # è€å¿ƒè®¾ä¸ºåŠä¸ªepochæˆ–50æ­¥ï¼Œå–è¾ƒå°å€¼
        print(f"âœ… æ­£å¸¸è®­ç»ƒæ¨¡å¼ ({epochs_to_train:.2f} epochs)ï¼šå¯ç”¨æ—©åœ (patience={patience_steps})")
        early_stopping = EarlyStopping(
            monitor='val_accuracy',
            patience=patience_steps,
            mode='max',
            verbose=True,
            min_delta=0.001  # è‡³å°‘æå‡0.1%æ‰ç®—æ”¹å–„
        )
        callbacks.append(early_stopping)
    
    return callbacks


def run_lightning_training(
    dataset_name: str,
    config: Dict[str, Any],
    dry_run: bool = False
) -> Dict[str, Any]:
    """è¿è¡ŒLightningè®­ç»ƒ"""
    
    print(f"\n{'=' * 70}")
    print(f"ğŸš€ Lightning LoRA è®­ç»ƒ: {dataset_name}")
    print(f"{'=' * 70}")
    
    # éªŒè¯æ•°æ®æ–‡ä»¶
    train_file = config['data']['train_file']
    test_file = config['data']['test_file']
    using_validation_as_test = config['data'].get('using_validation_as_test', False)
    
    for file_path, file_type in [(train_file, "è®­ç»ƒ"), (test_file, "æµ‹è¯•")]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_type}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    print(f"ğŸ“ è®­ç»ƒæ–‡ä»¶: {train_file}")
    if using_validation_as_test:
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file} (ä½¿ç”¨validationä½œä¸ºtest)")
        print("ğŸ’¡ æ•°æ®é›†é…ç½®: é€‚é…åªæœ‰train/validationçš„æ•°æ®é›†")
    else:
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file}")
    print(f"ğŸ¯ å®éªŒç›®å½•: {config['paths']['experiment_dir']}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: batch_size={config['training']['batch_size']}, steps={config['training']['max_steps']} ({config['training']['stage1_steps']}+{config['training']['stage2_steps']}) - ä¿å­˜æœ€å{config['training']['save_steps']}æ­¥")
    
    if dry_run:
        print("ğŸƒ Dry Run å®Œæˆ - å·²éªŒè¯:")
        print("  âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®")
        print("  âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®")
        print("  âœ… å®éªŒç›®å½•ç»“æ„åˆ›å»º")
        print("  âœ… LoRAé…ç½®æœ‰æ•ˆ")
        print("  âœ… è®­ç»ƒå‚æ•°åˆç†")
        print("  ğŸ’¡ è¦å®é™…è®­ç»ƒè¯·ç§»é™¤ --dry_run å‚æ•°")
        return {"status": "dry_run_completed"}
    
    try:
        # åˆ›å»ºå®éªŒç›®å½•
        experiment_dir = Path(config['paths']['experiment_dir'])
        experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        with open(config['paths']['config_file'], 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, indent=2, allow_unicode=True)
        
        # åˆå§‹åŒ– SwanLab
        print("ğŸ“Š åˆå§‹åŒ– SwanLab...")
        swanlab_run = swanlab.init(
            project=f"lora-training",
            experiment_name=config['experiment']['name'],
            config=config,
            logdir=config['paths']['swanlab_dir']
        )
        
        # åˆ›å»ºæ•°æ®æ¨¡å—
        batch_size = config['training']['batch_size']
            
        data_module = TrainTestDataModule(
            dataset_name=dataset_name,
            batch_size=batch_size,
            test_mode=False  # ç§»é™¤test_modeï¼Œå§‹ç»ˆä½¿ç”¨æ ‡å‡†æ¨¡å¼
        )
        
        print(f"ğŸ“Š æ•°æ®æ¨¡å—é…ç½®:")
        print(f"  - Trainæ–‡ä»¶: {data_module.train_file}")
        print(f"  - Testæ–‡ä»¶: {data_module.test_file}")
        print(f"  - Batchå¤§å°: {batch_size}")
        print(f"  - Train Shuffle: True, Test Shuffle: False")
        
        # åˆ›å»ºLightningæ¨¡å—
        model_path = config.get('model', {}).get('path', 'models/Qwen-Qwen2.5-0.5B')
        if not os.path.exists(model_path):
            model_path = config.get('model', {}).get('name', 'Qwen/Qwen2.5-0.5B')
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–LoRAé…ç½®ï¼Œæ”¯æŒé¢„è®¾æ¨¡æ¿
        lora_section = config.get('lora', {})
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é¢„è®¾é…ç½®
        preset_name = lora_section.get('use_preset', None)
        if preset_name and 'presets' in lora_section and preset_name in lora_section['presets']:
            print(f"ğŸ¯ ä½¿ç”¨LoRAé¢„è®¾é…ç½®: {preset_name}")
            preset_config = lora_section['presets'][preset_name]
            lora_config = {
                'r': preset_config.get('r', 16),
                'lora_alpha': preset_config.get('lora_alpha', 32),
                'target_modules': preset_config.get('target_modules', ["q_proj", "v_proj"]),
                'lora_dropout': lora_section.get('lora_dropout', 0.1),
                'bias': lora_section.get('bias', "none"),
                'task_type': TaskType.CAUSAL_LM
            }
        else:
            # ä½¿ç”¨ç›´æ¥é…ç½®
            lora_config = {
                'r': lora_section.get('r', 16),
                'lora_alpha': lora_section.get('lora_alpha', 32),
                'target_modules': lora_section.get('target_modules', ["q_proj", "v_proj"]),
                'lora_dropout': lora_section.get('lora_dropout', 0.1),
                'bias': lora_section.get('bias', "none"),
                'task_type': TaskType.CAUSAL_LM
            }
        
        # æ‰“å°LoRAé…ç½®ä¿¡æ¯
        print(f"ğŸ“Š LoRAé…ç½®:")
        print(f"  - ç§© (r): {lora_config['r']}")
        print(f"  - Alpha: {lora_config['lora_alpha']}")
        print(f"  - Dropout: {lora_config['lora_dropout']}")
        print(f"  - åç½®: {lora_config['bias']}")
        print(f"  - ç›®æ ‡å±‚: {lora_config['target_modules']}")
        print(f"  - ç›®æ ‡å±‚æ•°é‡: {len(lora_config['target_modules'])}")
        
        lightning_module = LoRALightningModule(
            model_path=model_path,
            lora_config=lora_config,
            learning_rate_stage1=config['training']['learning_rate_stage1'],
            learning_rate_stage2=config['training']['learning_rate_stage2'],
            stage1_steps=config['training']['stage1_steps'],
            stage2_steps=config['training']['stage2_steps'],
        )
        
        # å°†SwanLab runæ·»åŠ åˆ°æ¨¡å—ä¸­
        lightning_module._swanlab_run = swanlab_run
        
        # è®¡ç®—ä¸€ä¸ªepochçš„æ­¥æ•°ï¼Œç”¨äºå†³å®šæ˜¯å¦å¯ç”¨validation
        data_module.setup('fit')  # åˆå§‹åŒ–æ•°æ®æ¨¡å—
        train_dataloader = data_module.train_dataloader()
        steps_per_epoch = len(train_dataloader)
        max_steps = config['training']['max_steps']
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®åˆ†æ:")
        print(f"  - è®­ç»ƒæ ·æœ¬æ€»æ•°: {len(data_module.train_dataset)}")
        print(f"  - æ¯ä¸ªepochæ­¥æ•°: {steps_per_epoch}")
        print(f"  - è®¡åˆ’è®­ç»ƒæ­¥æ•°: {max_steps}")
        print(f"  - é¢„è®¡è®­ç»ƒepochæ•°: {max_steps / steps_per_epoch:.2f}")
        
        # è®¾ç½®å›è°ƒï¼ˆä¼ å…¥epochä¿¡æ¯ç”¨äºæ™ºèƒ½é…ç½®ï¼‰
        callbacks = setup_callbacks(config, steps_per_epoch, max_steps)
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        tensorboard_logger = TensorBoardLogger(
            save_dir=config['paths']['tensorboard_dir'],
            name="",
            version=""
        )
        
        # åˆ›å»ºTrainerï¼ˆç¦ç”¨è‡ªåŠ¨checkpointä¿å­˜ï¼Œå•è®¾å¤‡è®­ç»ƒé¿å…å¤šè¿›ç¨‹ï¼‰
        trainer = Trainer(
            max_steps=config['training']['max_steps'],
            callbacks=callbacks,
            logger=tensorboard_logger,
            enable_progress_bar=False,  # ç¦ç”¨è¿›åº¦æ¡é¿å…å†²çª
            log_every_n_steps=1,
            enable_checkpointing=False,  # ç¦ç”¨æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
            precision='16-mixed' if torch.cuda.is_available() else 32,
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,  # å¼ºåˆ¶å•è®¾å¤‡é¿å…å¤šè¿›ç¨‹å¯åŠ¨
            strategy='auto',  # å•è®¾å¤‡æ—¶autoä¼šé€‰æ‹©åˆé€‚ç­–ç•¥
            num_sanity_val_steps=2,  # è®¾ç½®ä¸º2è€Œä¸æ˜¯0ï¼Œé¿å…å®Œå…¨è·³è¿‡éªŒè¯
            enable_model_summary=False,  # ç¦ç”¨æ¨¡å‹æ‘˜è¦ï¼Œå‡å°‘è¾“å‡º
        )
        
        print(f"\nğŸƒâ€â™‚ï¸ å¼€å§‹Lightningè®­ç»ƒ...")
        print(f"ğŸ“Š è®­ç»ƒå™¨é…ç½®:")
        print(f"  - æœ€å¤§æ­¥æ•°: {config['training']['max_steps']}")
        print(f"  - ç²¾åº¦: {'16-mixed' if torch.cuda.is_available() else '32'}")
        print(f"  - åŠ é€Ÿå™¨: {trainer.accelerator}")
        print(f"  - è®¾å¤‡æ•°: {trainer.num_devices}")
        
        # å¼€å§‹è®­ç»ƒï¼ˆåŒ…å«éªŒè¯ï¼‰
        trainer.fit(lightning_module, datamodule=data_module)
        
        # åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°
        print(f"\nğŸ§ª å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        test_results = trainer.test(lightning_module, datamodule=data_module)
        
        print(f"ğŸ“Š æµ‹è¯•ç»“æœ:")
        for key, value in test_results[0].items():
            print(f"  - {key}: {value:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_dir = Path(config['paths']['final_model_dir'])
        final_model_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {final_model_dir}")
        lightning_module.model.save_pretrained(final_model_dir)
        lightning_module.tokenizer.save_pretrained(final_model_dir)
        
        # å…³é—­SwanLab
        swanlab.finish()
        
        results = {
            "status": "training_completed",
            "experiment_dir": str(experiment_dir),
            "final_model_dir": str(final_model_dir),
            "total_steps": config['training']['max_steps'],
            "test_results": test_results[0] if test_results else {},
            "framework": "lightning_swanlab"
        }
        
        print(f"\nâœ… Lightningè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_dir}")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹: {final_model_dir}")
        print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {config['training']['max_steps']}")
        # ä¸å†é‡å¤æ‰“å°è¯¦ç»†ç»“æœï¼ŒLightningå·²ç»åœ¨è¡¨æ ¼ä¸­æ˜¾ç¤ºäº†
        if test_results:
            print(f"ğŸ¯ æµ‹è¯•å®Œæˆ (è¯¦ç»†ç»“æœè§ä¸Šæ–¹è¡¨æ ¼)")
        else:
            print(f"ğŸ¯ æµ‹è¯•ç»“æœ: æœªå®Œæˆ")
        
        return results
        
    except Exception as e:
        print(f"âŒ Lightningè®­ç»ƒå¤±è´¥: {e}")
        if 'swanlab_run' in locals():
            swanlab.finish()
        raise
