#!/usr/bin/env python3
"""
æ¨¡å‹æ¨¡å—
åŒ…å«LoRA Lightningæ¨¡å—ã€æ¨¡å‹åˆå§‹åŒ–ã€è®­ç»ƒæ­¥éª¤ç­‰åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import pytorch_lightning as pl
import swanlab
import warnings
from typing import Dict, Any

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType


class LoRALightningModule(pl.LightningModule):
    """Lightning LoRA è®­ç»ƒæ¨¡å—"""
    
    def __init__(
        self,
        model_path: str,
        lora_config: Dict[str, Any],
        learning_rate_stage1: float = 1e-4,
        learning_rate_stage2: float = 1e-5,
        stage1_steps: int = 75,
        stage2_steps: int = 50,
        max_length: int = 512,
        **kwargs
    ):
        super().__init__()
        self.save_hyperparameters()
        
        self.model_path = model_path
        self.lora_config = lora_config
        self.learning_rate_stage1 = learning_rate_stage1
        self.learning_rate_stage2 = learning_rate_stage2
        self.stage1_steps = stage1_steps
        self.stage2_steps = stage2_steps
        self.max_length = max_length
        self.total_steps = stage1_steps + stage2_steps
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer
        self._init_model()
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.training_step_count = 0
        
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")

        # å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ï¼Œæ¨èå¤§å¤šæ•°åœºæ™¯
        if torch.cuda.is_available():
            torch.set_float32_matmul_precision('medium')  

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹ - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        self.base_model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=None,  # Lightning ä¼šå¤„ç†è®¾å¤‡åˆ†é…
            trust_remote_code=True,
            # å†…å­˜ä¼˜åŒ–é€‰é¡¹
            low_cpu_mem_usage=True,  # å‡å°‘CPUå†…å­˜ä½¿ç”¨
        )
        
        # åº”ç”¨ LoRA
        lora_config = LoraConfig(**self.lora_config)
        self.model = get_peft_model(self.base_model, lora_config)
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        # è·å–å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        trainable_params, total_params = self.model.get_nb_trainable_parameters()
        print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")
        
    def forward(self, input_ids, attention_mask, labels):
        """å‰å‘ä¼ æ’­"""
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return outputs
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        loss = self._compute_loss(batch, "train")
        batch_size = len(batch) if isinstance(batch, list) else batch['input_ids'].size(0)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        self.log('train/loss', loss, prog_bar=True, on_step=True, on_epoch=True, batch_size=batch_size)
        self.log('train/step', self.training_step_count, on_step=True, batch_size=batch_size)
        
        # è®°å½•å­¦ä¹ ç‡é˜¶æ®µä¿¡æ¯
        current_stage = 1 if self.training_step_count < self.stage1_steps else 2
        self.log('train/stage', current_stage, on_step=True, batch_size=batch_size)
        
        # è®°å½•åˆ° SwanLab
        if hasattr(self, '_swanlab_run'):
            swanlab.log({
                "train/loss": loss.item(),
                "train/step": self.training_step_count,
                "train/stage": current_stage,
                "train/epoch": self.current_epoch
            }, step=self.training_step_count)
        
        self.training_step_count += 1
        return loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ - ç”¨äºæ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦"""
        loss = self._compute_loss(batch, "val")
        batch_size = len(batch) if isinstance(batch, list) else batch['input_ids'].size(0)
        accuracy = self._compute_accuracy(batch)
        perplexity = torch.exp(loss)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        self.log('val/loss', loss, on_step=False, on_epoch=True, batch_size=batch_size)
        self.log('val_accuracy', accuracy, on_step=False, on_epoch=True, batch_size=batch_size)  # æ—©åœç›‘æ§è¿™ä¸ª
        self.log('val/perplexity', perplexity, on_step=False, on_epoch=True, batch_size=batch_size)
        
        return {
            'val_loss': loss,
            'val_accuracy': accuracy,
            'val_perplexity': perplexity
        }
        
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤"""
        loss = self._compute_loss(batch, "test")
        batch_size = len(batch) if isinstance(batch, list) else batch['input_ids'].size(0)
        accuracy = self._compute_accuracy(batch)
        perplexity = torch.exp(loss)
        
        # è®°å½•æµ‹è¯•æŒ‡æ ‡
        self.log('test/loss', loss, on_step=False, on_epoch=True, batch_size=batch_size)
        self.log('test/accuracy', accuracy, on_step=False, on_epoch=True, batch_size=batch_size)
        self.log('test/perplexity', perplexity, on_step=False, on_epoch=True, batch_size=batch_size)
        
        return {
            'test_loss': loss,
            'test_accuracy': accuracy,
            'test_perplexity': perplexity
        }
    
    def on_validation_epoch_end(self):
        """éªŒè¯epochç»“æŸæ—¶çš„å›è°ƒï¼ˆå·²ç§»é™¤ï¼‰"""
        pass  # ä¸å†éœ€è¦
    
    def on_test_epoch_end(self):
        """æµ‹è¯•epochç»“æŸæ—¶çš„å›è°ƒ"""
        if hasattr(self, '_swanlab_run'):
            # è®°å½•æµ‹è¯•æŒ‡æ ‡åˆ°SwanLab
            test_loss = self.trainer.callback_metrics.get('test/loss')
            test_acc = self.trainer.callback_metrics.get('test/accuracy') 
            test_ppl = self.trainer.callback_metrics.get('test/perplexity')
            
            if test_loss is not None:
                swanlab.log({
                    "test/loss": test_loss.item(),
                    "test/accuracy": test_acc.item() if test_acc is not None else 0,
                    "test/perplexity": test_ppl.item() if test_ppl is not None else 0,
                    "final_epoch": self.current_epoch
                }, step=self.training_step_count)
    
    def _compute_loss(self, batch, stage: str):
        """è®¡ç®—æŸå¤±çš„é€šç”¨æ–¹æ³•"""
        # å¤„ç†batchæ•°æ®
        if isinstance(batch, list):
            # å¦‚æœæ˜¯listï¼Œéœ€è¦tokenize
            inputs = []
            labels = []
            
            for item in batch:
                input_text = item.get('input', '')
                output_text = item.get('output', '')
                
                # ç»„åˆè¾“å…¥å’Œè¾“å‡º
                full_text = f"{input_text}{output_text}"
                
                # Tokenize
                encoding = self.tokenizer(
                    full_text,
                    truncation=True,
                    padding='max_length',
                    max_length=self.max_length,
                    return_tensors='pt'
                )
                
                inputs.append(encoding['input_ids'].squeeze())
                labels.append(encoding['input_ids'].squeeze())
            
            input_ids = torch.stack(inputs).to(self.device)
            attention_mask = torch.ones_like(input_ids).to(self.device)
            labels = torch.stack(labels).to(self.device)
            
        else:
            # å¦‚æœå·²ç»æ˜¯tensoræ ¼å¼
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask'] 
            labels = batch['labels']
        
        # å‰å‘ä¼ æ’­
        outputs = self(input_ids, attention_mask, labels)
        return outputs.loss
    
    def _compute_accuracy(self, batch):
        """è®¡ç®—å‡†ç¡®ç‡ï¼ˆçœŸå®çš„å¤šé€‰é¢˜å‡†ç¡®ç‡ï¼‰"""
        if not isinstance(batch, list):
            # å¦‚æœä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè¿”å›åŸºäºlossçš„ä»£ç†æŒ‡æ ‡ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            with torch.no_grad():
                loss = self._compute_loss(batch, "eval")
                accuracy = torch.exp(-loss)
                return torch.clamp(accuracy, 0.0, 1.0)
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for item in batch:
                try:
                    # è§£ææ•°æ®é¡¹
                    question = item.get('input', '')
                    options = item.get('options', [])
                    correct_answer = item.get('target', 'A')
                    
                    if not options:
                        total += 1
                        continue
                    
                    # æ ¼å¼åŒ–å¸¦é€‰é¡¹çš„é—®é¢˜
                    prompt = f"Question: {question}\n"
                    for option in options:
                        prompt += f"{option}\n"
                    prompt += "Answer:"
                    
                    # Tokenize
                    inputs = self.tokenizer(
                        prompt,
                        return_tensors='pt',
                        truncation=True,
                        max_length=self.max_length,
                        padding=True
                    ).to(self.device)
                    
                    # ç”Ÿæˆå‚æ•°é…ç½®ï¼ˆä¸è¯„ä¼°è„šæœ¬ä¸€è‡´ï¼‰
                    generation_kwargs = {
                        "max_new_tokens": 3,
                        "do_sample": False,
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "use_cache": False,
                        "output_attentions": False,
                        "output_hidden_states": False,
                    }
                    
                    # Gemmaæ¨¡å‹ç‰¹æ®Šå¤„ç†
                    if "gemma" in self.model_path.lower():
                        generation_kwargs.update({
                            "temperature": 1.0,
                            "top_p": 1.0,
                            "repetition_penalty": 1.0,
                        })
                    
                    # ç”Ÿæˆç­”æ¡ˆ (ä½¿ç”¨è­¦å‘ŠæŠ‘åˆ¶)
                    with warnings.catch_warnings():
                        warnings.filterwarnings("ignore", message=".*cache_implementation.*")
                        warnings.filterwarnings("ignore", message=".*generation flags are not valid.*")
                        outputs = self.model.generate(
                            **inputs,
                            **generation_kwargs
                        )
                    
                    # è§£ç ç”Ÿæˆçš„ç­”æ¡ˆ
                    generated_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                    generated_answer = generated_text.strip().upper()
                    
                    # æå–ç¬¬ä¸€ä¸ªå­—æ¯ (A, B, C, æˆ– D)
                    predicted_answer = None
                    for char in generated_answer:
                        if char in ['A', 'B', 'C', 'D']:
                            predicted_answer = char
                            break
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®ç­”æ¡ˆï¼Œå°è¯•åŒ¹é…é€‰é¡¹å‰ç¼€
                    if predicted_answer is None:
                        for option in options:
                            if option.startswith('A:') and 'A' in generated_answer:
                                predicted_answer = 'A'
                            elif option.startswith('B:') and 'B' in generated_answer:
                                predicted_answer = 'B'
                            elif option.startswith('C:') and 'C' in generated_answer:
                                predicted_answer = 'C'
                            elif option.startswith('D:') and 'D' in generated_answer:
                                predicted_answer = 'D'
                            if predicted_answer:
                                break
                    
                    # ä¸æ­£ç¡®ç­”æ¡ˆæ¯”è¾ƒ
                    if predicted_answer == correct_answer:
                        correct += 1
                    
                    total += 1
                    
                except Exception as e:
                    # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè·³è¿‡ç”Ÿæˆé”™è¯¯çš„æ ·æœ¬
                    total += 1
                    continue
        
        if total == 0:
            return torch.tensor(0.0)
        
        accuracy = correct / total
        return torch.tensor(accuracy)
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate_stage1,  # åˆå§‹å­¦ä¹ ç‡
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=0.01
        )
        
        # éªŒè¯å‡†ç¡®ç‡ç›‘æ§çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',           # ç›‘æ§éªŒè¯å‡†ç¡®ç‡æœ€å¤§å€¼
            factor=0.5,          # å­¦ä¹ ç‡å‡åŠ
            patience=30,         # 30æ­¥æ— æå‡å°±é™ä½
            min_lr=1e-7          # æœ€å°å­¦ä¹ ç‡
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_accuracy",  # ç›‘æ§éªŒè¯å‡†ç¡®ç‡
                "interval": "epoch",
                "frequency": 1,
            },
        }
