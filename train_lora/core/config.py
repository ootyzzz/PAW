#!/usr/bin/env python3
"""
é…ç½®ç®¡ç†æ¨¡å—
åŒ…å«é…ç½®åˆ›å»ºã€æ‰¹æ¬¡å¤§å°ä¼˜åŒ–ã€å®éªŒé…ç½®ç­‰åŠŸèƒ½
"""

import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

from .data import get_test_file_path


def get_optimal_batch_size(dataset_name: str, target_steps: int = 125) -> int:
    """æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜batch size"""
    dataset_sizes = {
        'arc-challenge': 1119,
        'arc-easy': 2251, 
        'boolq': 9427,
        'hellaswag': 39905,
        'openbookqa': 4957,
        'piqa': 16113,
        'winogrande': 40398
    }
    
    if dataset_name not in dataset_sizes:
        print(f"âš ï¸ æœªçŸ¥æ•°æ®é›† {dataset_name}ï¼Œä½¿ç”¨é»˜è®¤batch_size=32")
        return 32
    
    dataset_size = dataset_sizes[dataset_name]
    
    # é€‰æ‹©åˆé€‚çš„batch size
    if dataset_size <= 1200:  # å°æ•°æ®é›†
        return 8
    elif dataset_size <= 2500:  # ä¸­å°æ•°æ®é›†  
        return 16
    else:  # å¤§æ•°æ®é›†
        return 32


def analyze_batch_efficiency(dataset_name: str, batch_size: int, target_steps: int = 125):
    """åˆ†æbatch sizeæ•ˆç‡"""
    dataset_sizes = {
        'arc-challenge': 1119,
        'arc-easy': 2251, 
        'boolq': 9427,
        'hellaswag': 39905,
        'openbookqa': 4957,
        'piqa': 16113,
        'winogrande': 40398
    }
    
    if dataset_name not in dataset_sizes:
        return
        
    dataset_size = dataset_sizes[dataset_name]
    total_samples_needed = batch_size * target_steps
    epochs_needed = total_samples_needed / dataset_size
    
    print(f"ğŸ“Š Batchæ•ˆç‡åˆ†æ:")
    print(f"  - æ•°æ®é›†: {dataset_name} ({dataset_size} æ ·æœ¬)")
    print(f"  - Batchå¤§å°: {batch_size}")
    print(f"  - {target_steps}æ­¥éœ€è¦: {total_samples_needed} æ ·æœ¬")
    print(f"  - éœ€è¦å¾ªç¯: {epochs_needed:.2f} epochs")
    
    return epochs_needed


def create_lightning_config(
    dataset_name: str, 
    base_config: Dict[str, Any], 
    base_model_path: str = None, 
    batch_size: int = None, 
    max_steps: int = 125, 
    save_steps: int = 50, 
    learning_rate: float = 1e-4, 
    learning_rate_stage2: float = None,
    external_config_path: str = None
) -> Dict[str, Any]:
    """åˆ›å»ºLightningè®­ç»ƒé…ç½®"""
    config = base_config.copy()
    
    # å¦‚æœæä¾›äº†å¤–éƒ¨é…ç½®æ–‡ä»¶ï¼Œè¯»å–å¹¶åˆå¹¶LoRAé…ç½®
    if external_config_path and os.path.exists(external_config_path):
        import yaml
        try:
            with open(external_config_path, 'r', encoding='utf-8') as f:
                external_config = yaml.safe_load(f)
            
            # åˆå¹¶LoRAé…ç½®
            if 'lora' in external_config:
                config['lora'] = external_config['lora']
                print(f"ğŸ“ ä»å¤–éƒ¨é…ç½®æ–‡ä»¶è¯»å–LoRAè®¾ç½®: {external_config_path}")
                
                # æ˜¾ç¤ºLoRAé…ç½®ä¿¡æ¯
                lora_config = external_config['lora']
                if 'target_modules' in lora_config:
                    print(f"   - ç›®æ ‡å±‚: {lora_config['target_modules']}")
                if 'r' in lora_config:
                    print(f"   - ç§© (r): {lora_config['r']}")
                if 'lora_alpha' in lora_config:
                    print(f"   - Alpha: {lora_config['lora_alpha']}")
                if 'lora_dropout' in lora_config:
                    print(f"   - Dropout: {lora_config['lora_dropout']}")
                    
        except Exception as e:
            print(f"âš ï¸ è¯»å–å¤–éƒ¨é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            print(f"   ç»§ç»­ä½¿ç”¨é»˜è®¤LoRAé…ç½®")
    
    # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜batch sizeï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if batch_size is None:
        batch_size = get_optimal_batch_size(dataset_name)
        print(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©batch_size={batch_size}ç”¨äº{dataset_name}")
    
    # åˆ†æbatchæ•ˆç‡
    analyze_batch_efficiency(dataset_name, batch_size, max_steps)
    
    # æ™ºèƒ½é€‰æ‹©æµ‹è¯•æ–‡ä»¶è·¯å¾„ - æ”¯æŒvalidationä½œä¸ºfallback
    test_file_path, using_validation = get_test_file_path(dataset_name)
    
    # ç¡®ä¿configæœ‰æ‰€éœ€çš„sections
    if 'data' not in config:
        config['data'] = {}
    if 'training' not in config:
        config['training'] = {}
    if 'paths' not in config:
        config['paths'] = {}
    
    # æ›´æ–°æ•°æ®è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    config['data']['train_file'] = os.path.join(project_root, f"data_to_lora/cs/{dataset_name}/{dataset_name}_train_formatted.jsonl")
    config['data']['test_file'] = os.path.join(project_root, test_file_path) if not os.path.isabs(test_file_path) else test_file_path
    config['data']['using_validation_as_test'] = using_validation
    
    # ä»æ¨¡å‹è·¯å¾„æå–æ¨¡å‹åç§°ç”¨äºç›®å½•ç»„ç»‡
    if base_model_path:
        model_name = Path(base_model_path).name
        # å¤„ç† HuggingFace æ ¼å¼çš„åç§° (å¦‚ Qwen/Qwen2.5-1.5B -> Qwen2.5-1.5B)
        if '/' in model_name and not os.path.exists(base_model_path):
            model_name = model_name.split('/')[-1]
    else:
        model_name = "default-model"
    
    # ç”Ÿæˆå®éªŒåç§° - ä½¿ç”¨æ›´è§„èŒƒçš„å‘½å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"{dataset_name}_lora_{timestamp}"
    
    # Lightning + SwanLab ä¸“ç”¨é…ç½®
    stage1_ratio = 0.6  # 60% æ­¥æ•°ä¸ºStage 1
    stage1_steps = int(max_steps * stage1_ratio)
    stage2_steps = max_steps - stage1_steps
    
    # è‡ªåŠ¨è®¡ç®—ç¬¬äºŒé˜¶æ®µå­¦ä¹ ç‡
    if learning_rate_stage2 is None:
        learning_rate_stage2 = learning_rate / 10
    
    config['training'].update({
        'batch_size': batch_size,
        'max_steps': max_steps,  # ä½¿ç”¨å‚æ•°åŒ–çš„æ­¥æ•°
        'stage1_steps': stage1_steps,
        'stage2_steps': stage2_steps,
        'save_steps': save_steps,  # ä¿å­˜æœ€åå¤šå°‘æ­¥
        'learning_rate_stage1': learning_rate,
        'learning_rate_stage2': learning_rate_stage2,
    })
    
    # ç°ä»£åŒ–çš„è¾“å‡ºç›®å½•ç»“æ„ - æŒ‰æ•°æ®é›†åˆ†ç»„ï¼Œå†æŒ‰æ¨¡å‹åˆ†ç»„
    base_dir = Path("./runs") / dataset_name / model_name / experiment_name.split('_')[-1]  # åªä½¿ç”¨æ—¶é—´æˆ³éƒ¨åˆ†
    config['paths'].update({
        'experiment_dir': str(base_dir),
        'checkpoints_dir': str(base_dir / "checkpoints"),
        'tensorboard_dir': str(base_dir / "tensorboard_logs"),
        'swanlab_dir': str(base_dir / "swanlab_logs"),
        'final_model_dir': str(base_dir / "final_model"),
        'config_file': str(base_dir / "config.yaml")
    })
    
    # å®éªŒå…ƒæ•°æ®
    config['experiment'] = {
        'name': experiment_name,
        'dataset': dataset_name,
        'batch_size': batch_size,
        'model_name': model_name,  # æ·»åŠ æ¨¡å‹åç§°åˆ°å…ƒæ•°æ®
        'base_model_path': base_model_path,  # æ·»åŠ å®Œæ•´æ¨¡å‹è·¯å¾„
        'framework': 'lightning_swanlab',
        'created_at': datetime.now().isoformat(),
        'description': f"Lightning LoRA training on {dataset_name} with {model_name} - {max_steps} steps",
        'tags': ["lightning", "swanlab", "lora", model_name.lower(), dataset_name, f"batch{batch_size}", f"steps{max_steps}"]
    }
    
    return config
