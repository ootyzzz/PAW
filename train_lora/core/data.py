#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†æ¨¡å—
åŒ…å«æ•°æ®é›†ç±»ã€æ•°æ®æ¨¡å—ã€æ•°æ®è·¯å¾„å¤„ç†ç­‰åŠŸèƒ½
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple

import torch
from torch.utils.data import DataLoader, Dataset
import pytorch_lightning as pl


def get_test_file_path(dataset_name: str) -> Tuple[str, bool]:
    """
    æ™ºèƒ½é€‰æ‹©æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼Œä¼˜å…ˆtestï¼Œä¸å­˜åœ¨åˆ™ä½¿ç”¨validation
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        
    Returns:
        tuple: (æ–‡ä»¶è·¯å¾„, æ˜¯å¦ä½¿ç”¨validation)
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    data_dir = os.path.join(project_root, f"data_to_lora/cs/{dataset_name}")
    test_file = os.path.join(data_dir, f"{dataset_name}_test_formatted.jsonl")
    validation_file = os.path.join(data_dir, f"{dataset_name}_validation_formatted.jsonl")
    
    if os.path.exists(test_file):
        return test_file, False
    elif os.path.exists(validation_file):
        print(f"ğŸ“Š æ³¨æ„: {dataset_name} æ²¡æœ‰testæ–‡ä»¶ï¼Œå°†ä½¿ç”¨validationæ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†")
        return validation_file, True
    else:
        # è¿”å›é»˜è®¤testè·¯å¾„ï¼Œè®©åç»­å¤„ç†æŠ¥é”™
        return test_file, False


def custom_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œä¿æŒå­—å…¸ç»“æ„"""
    return batch


class SequentialDataset(Dataset):
    """å®Œæ•´çš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒtrain/validation/test"""
    
    def __init__(self, data_file: str, dataset_name: str, split: str = "train"):
        self.data_file = data_file
        self.dataset_name = dataset_name
        self.split = split
        self.data = self._load_data()
        self.total_samples = len(self.data)
        
    def _load_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ•°æ®"""
        data = []
        if not os.path.exists(self.data_file):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_file}")
            
        with open(self.data_file, 'r', encoding='utf-8') as f:
            for line_idx, line in enumerate(f):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        data.append(item)
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {line_idx}: {e}")
                        
        print(f"ğŸ“Š åŠ è½½{self.split}æ•°æ®é›† {self.dataset_name}: {len(data)} æ ·æœ¬")
        return data
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        # æ”¯æŒè¶…è¿‡æ•°æ®é›†é•¿åº¦çš„å¾ªç¯è®¿é—®ï¼ˆä¸»è¦ç”¨äºè®­ç»ƒé›†ï¼‰
        if self.split == "train":
            actual_idx = idx % self.total_samples
        else:
            # éªŒè¯å’Œæµ‹è¯•é›†ä¸å¾ªç¯è®¿é—®
            actual_idx = idx
        return self.data[actual_idx].copy()


class TrainTestDataModule(pl.LightningDataModule):
    """Lightningæ•°æ®æ¨¡å—ï¼Œç®¡ç†train/validation/testæ•°æ®"""
    
    def __init__(self, dataset_name: str, batch_size: int = 4, test_mode: bool = False):
        super().__init__()
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.test_mode = test_mode
        
        # æ•°æ®æ–‡ä»¶è·¯å¾„
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.data_dir = os.path.join(project_root, f"data_to_lora/cs/{dataset_name}")
        self.train_file = os.path.join(self.data_dir, f"{dataset_name}_train_formatted.jsonl")
        
        # éªŒè¯é›†æ–‡ä»¶è·¯å¾„
        self.validation_file = os.path.join(self.data_dir, f"{dataset_name}_validation_formatted.jsonl")
        
        # æ™ºèƒ½é€‰æ‹©æµ‹è¯•æ–‡ä»¶ï¼šä¼˜å…ˆtestï¼Œä¸å­˜åœ¨åˆ™ä½¿ç”¨validation
        self.test_file, self.using_validation_as_test = get_test_file_path(dataset_name)
        
    def setup(self, stage: str = None):
        """è®¾ç½®æ•°æ®é›†"""
        if stage == "fit" or stage is None:
            self.train_dataset = SequentialDataset(self.train_file, self.dataset_name, "train")
            # æ·»åŠ éªŒè¯é›†
            if os.path.exists(self.validation_file):
                self.val_dataset = SequentialDataset(self.validation_file, self.dataset_name, "validation")
                print(f"ğŸ“Š ä½¿ç”¨éªŒè¯é›†: {self.validation_file}")
            else:
                print(f"âš ï¸  éªŒè¯é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.validation_file}")
                self.val_dataset = None
            
        if stage == "test" or stage is None:
            self.test_dataset = SequentialDataset(self.test_file, self.dataset_name, "test")
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0 if self.test_mode else 4,
            pin_memory=not self.test_mode,
            drop_last=False,
            collate_fn=custom_collate_fn
        )
    
    def val_dataloader(self):
        """éªŒè¯é›†æ•°æ®åŠ è½½å™¨"""
        if hasattr(self, 'val_dataset') and self.val_dataset is not None:
            return DataLoader(
                self.val_dataset,
                batch_size=self.batch_size,
                shuffle=False,  # éªŒè¯é›†ä¸shuffle
                num_workers=0 if self.test_mode else 4,
                pin_memory=not self.test_mode,
                drop_last=False,
                collate_fn=custom_collate_fn
            )
        return None
    
    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,  # æµ‹è¯•é›†ä¸shuffleï¼Œä¿è¯ç»“æœå¯é‡ç°
            num_workers=0 if self.test_mode else 4,
            pin_memory=not self.test_mode,
            drop_last=False,
            collate_fn=custom_collate_fn
        )
