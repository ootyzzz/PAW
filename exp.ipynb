{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8793524f",
   "metadata": {},
   "source": [
    "## 训练系统更新 (2025-07-24)\n",
    "\n",
    "新增功能:\n",
    "- 早停机制: 验证准确率50步无提升自动停止\n",
    "- 学习率调度: 验证准确率plateau时自动降低学习率\n",
    "- 验证集支持: 自动使用validation_formatted.jsonl\n",
    "\n",
    "技术实现:\n",
    "- 早停: EarlyStopping回调，监控val_accuracy\n",
    "- 学习率调度: ReduceLROnPlateau，学习率减半，最小lr=1e-7\n",
    "- 数据分离: train/validation/test 三重分离\n",
    "\n",
    "训练命令模板:\n",
    "```bash\n",
    "cd /root/PAW/train_lora && python train_cs_lora_lightning.py \\\n",
    "  --dataset arc-challenge \\\n",
    "  --base_model /root/autodl-tmp/models/Qwen_Qwen2.5-1.5B \\\n",
    "  --lr 1e-5 \\\n",
    "  --max_steps 2000 \\\n",
    "  --bs 6\n",
    "```\n",
    "\n",
    "注意: max_steps现在是上限，实际可能因早停而提前结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefdd35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 训练命令模板\n",
    "\n",
    "基础训练:\n",
    "```bash\n",
    "cd /root/PAW/train_lora && python train_cs_lora_lightning.py \\\n",
    "  --dataset arc-challenge \\\n",
    "  --base_model /root/autodl-tmp/models/Qwen_Qwen2.5-1.5B \\\n",
    "  --lr 1e-5 \\\n",
    "  --max_steps 150 \\\n",
    "  --bs 4\n",
    "```\n",
    "\n",
    "参数说明:\n",
    "- lr: 学习率，推荐1e-5到1.5e-5之间\n",
    "- max_steps: 训练步数，600步左右效果较好\n",
    "- bs: 批大小，4-6适中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36747b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "7.24 0:15 修好了全repo的test的一致性 \n",
    "并且跑了2个baseline 和 分别微调(batch=4, step=600)的acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93ce37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python eval/lightning_eval.py --models_list /root/PAW/runs/Qwen_Qwen2.5-1.5B/arc-challenge_lora_2025\n",
    "0723_191421/final_model --dataset arc-challenge --base_model /root/autodl-tmp/model\n",
    "s/Qwen_Qwen2.5-1.5B\n",
    "\n",
    "\n",
    "lightning_eval.py 说明 如果是test trained model 要加base_model，这样才能识别是在什么基模之上的delta W.\n",
    "\n",
    "perplexity 请忽略 没啥用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fdba9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "基准测试结果 arc-challenge\n",
    "\n",
    "Model                                    Dataset         Loss     Accuracy   Perplexity   \n",
    "----------------------------------------------------------------------------------------\n",
    "gemma-2-2b-it                            arc-challenge   34.9178  0.7491     17695200835\n",
    "gemma-2-2b-it_lora_trained               arc-challenge   0.3224   0.7381     1.3813\n",
    "\n",
    "Qwen_Qwen2.5-1.5B                        arc-challenge   9.6764   0.7338     16587.6250\n",
    "Qwen_Qwen2.5-1.5B_20250723_191421        arc-challenge   0.2720   0.0597     1.3135    (失败训练)\n",
    "Qwen_Qwen2.5-1.5B_20250724_005835        arc-challenge   0.2675   0.2193     1.3075    (失败训练)\n",
    "Qwen_Qwen2.5-1.5B_20250724_013028        arc-challenge   0.1490   0.7346     1.1616    (150步)\n",
    "\n",
    "说明: \n",
    "- gemma基础模型性能较好 0.7491，LoRA后略降至0.7381\n",
    "- Qwen基础模型0.7338，经过参数调优后LoRA训练成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe055c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "实验记录 - Qwen2.5-1.5B LoRA训练\n",
    "\n",
    "基础模型准确率: 0.7338\n",
    "\n",
    "训练结果对比:\n",
    "步数150, lr=1e-5, bs=4:     0.7346 (+0.08%)\n",
    "步数200, lr=1.5e-5, bs=6:   0.7346 (+0.08%)  \n",
    "步数600, lr=1.5e-5, bs=6:   0.7457 (+1.19%)\n",
    "步数600, lr=2e-5, bs=6:     0.7363 (+0.25%) 过高学习率反而下降\n",
    "步数2000, lr=1e-5, bs=6:    0.7210 (-1.28%) 过拟合\n",
    "\n",
    "关键发现:\n",
    "1. 600步是较好的停止点，2000步开始过拟合\n",
    "2. lr=1.5e-5在600步时效果最佳\n",
    "3. lr=2e-5过激进，lr=1e-5在长步数下会过拟合\n",
    "4. 早停和验证集机制已实现但此次未触发\n",
    "\n",
    "最佳配置: lr=1.5e-5, max_steps=600, bs=6\n",
    "最佳准确率: 0.7457 (提升1.19个百分点)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dcebc",
   "metadata": {},
   "source": [
    "## 评估脚本参数说明\n",
    "\n",
    "### 当前命令分析：\n",
    "```bash\n",
    "python eval/lightning_eval.py \\\n",
    "  --models_list /root/PAW/runs/gemma-2-2b-it/arc-challenge_lora_20250723_235207/final_model \\\n",
    "  --dataset arc-challenge \\\n",
    "  --base_model /root/autodl-tmp/models/gemma-2-2b-it \\\n",
    "  --sample_ratio 0.2\n",
    "```\n",
    "\n",
    "### 参数含义：\n",
    "- `--models_list`: **要评估的模型路径**（可以是LoRA模型或基础模型）\n",
    "- `--base_model`: **仅当评估LoRA模型时需要**，用于加载基础模型再应用LoRA权重\n",
    "- `--dataset`: 数据集名称\n",
    "- `--sample_ratio`: 采样比例\n",
    "\n",
    "### 如何评估原始基础模型：\n",
    "```bash\n",
    "# 评估未训练的原始 Gemma 模型\n",
    "python eval/lightning_eval.py \\\n",
    "  --models_list /root/autodl-tmp/models/gemma-2-2b-it \\\n",
    "  --dataset arc-challenge\n",
    "  # 注意：不需要 --base_model 参数！\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
