#!/usr/bin/env python3
"""
validate_setup.py
éªŒè¯è„šæœ¬ - ç”¨äºæ‰‹åŠ¨éªŒè¯LoRAè®­ç»ƒç¯å¢ƒå’Œç»„ä»¶
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"ğŸ” {title}")
    print('='*60)

def check_file_exists(file_path, description):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    exists = os.path.exists(file_path)
    status = "âœ…" if exists else "âŒ"
    print(f"{status} {description}: {file_path}")
    if exists:
        size = os.path.getsize(file_path)
        print(f"    å¤§å°: {size:,} bytes")
    return exists

def check_directory_exists(dir_path, description):
    """æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨"""
    exists = os.path.exists(dir_path) and os.path.isdir(dir_path)
    status = "âœ…" if exists else "âŒ"
    print(f"{status} {description}: {dir_path}")
    if exists:
        files = list(Path(dir_path).iterdir())
        print(f"    åŒ…å« {len(files)} ä¸ªæ–‡ä»¶/ç›®å½•")
    return exists

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    print_section("æµ‹è¯•PythonåŒ…å¯¼å…¥")
    
    imports_to_test = [
        ("torch", "PyTorch"),
        ("transformers", "Transformers"),
        ("yaml", "PyYAML"),
        ("tqdm", "TQDMè¿›åº¦æ¡"),
        ("json", "JSON (å†…ç½®)"),
        ("logging", "Logging (å†…ç½®)")
    ]
    
    optional_imports = [
        ("peft", "PEFT (LoRAæ”¯æŒ)"),
        ("wandb", "Weights & Biases"),
        ("tensorboard", "TensorBoard")
    ]
    
    all_success = True
    
    # å¿…éœ€çš„å¯¼å…¥
    for module, description in imports_to_test:
        try:
            __import__(module)
            print(f"âœ… {description}: å¯ç”¨")
        except ImportError as e:
            print(f"âŒ {description}: ä¸å¯ç”¨ ({e})")
            all_success = False
    
    # å¯é€‰çš„å¯¼å…¥
    print(f"\nå¯é€‰ä¾èµ–:")
    for module, description in optional_imports:
        try:
            __import__(module)
            print(f"âœ… {description}: å¯ç”¨")
        except ImportError:
            print(f"âš ï¸ {description}: ä¸å¯ç”¨ (å¯é€‰)")
    
    return all_success

def test_project_structure():
    """æµ‹è¯•é¡¹ç›®ç»“æ„"""
    print_section("éªŒè¯é¡¹ç›®æ–‡ä»¶ç»“æ„")
    
    required_files = [
        ("configs/training_config.yaml", "è®­ç»ƒé…ç½®æ–‡ä»¶"),
        ("utils/data_processor.py", "æ•°æ®å¤„ç†å™¨"),
        ("utils/scheduler.py", "å­¦ä¹ ç‡è°ƒåº¦å™¨"),
        ("lora/checkpoint_utils.py", "Checkpointç®¡ç†"),
        ("core/train.py", "è®­ç»ƒæ ¸å¿ƒ"),
        ("scripts/experiment_manager_enhanced.py", "å®éªŒç®¡ç†å™¨"),
        ("scripts/model_manager.py", "æ¨¡å‹ç®¡ç†å™¨"),
        ("train_commonsense_lora.py", "ä¸»è®­ç»ƒè„šæœ¬")
    ]
    
    required_dirs = [
        ("models", "æ¨¡å‹ç›®å½•"),
        ("raw_datasets/commonsense", "æ•°æ®é›†ç›®å½•"),
        ("utils", "å·¥å…·ç›®å½•"),
        ("lora", "LoRAç›®å½•"),
        ("core", "æ ¸å¿ƒç›®å½•"),
        ("scripts", "è„šæœ¬ç›®å½•")
    ]
    
    all_files_exist = True
    all_dirs_exist = True
    
    print("å¿…éœ€æ–‡ä»¶:")
    for file_path, description in required_files:
        if not check_file_exists(file_path, description):
            all_files_exist = False
    
    print(f"\nå¿…éœ€ç›®å½•:")
    for dir_path, description in required_dirs:
        if not check_directory_exists(dir_path, description):
            all_dirs_exist = False
    
    return all_files_exist and all_dirs_exist

def test_model_and_data():
    """æµ‹è¯•æ¨¡å‹å’Œæ•°æ®"""
    print_section("éªŒè¯æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = "models/Qwen-Qwen2.5-0.5B"
    model_files = [
        "config.json",
        "tokenizer.json",
        "model.safetensors",
        "tokenizer_config.json"
    ]
    
    print("æ¨¡å‹æ–‡ä»¶:")
    model_valid = check_directory_exists(model_path, "Qwen2.5æ¨¡å‹ç›®å½•")
    
    if model_valid:
        for file_name in model_files:
            file_path = os.path.join(model_path, file_name)
            check_file_exists(file_path, f"  {file_name}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    print(f"\næ•°æ®æ–‡ä»¶:")
    data_path = "raw_datasets/commonsense/cs_all_unbalanced.jsonl"
    data_valid = check_file_exists(data_path, "Commonsenseæ•°æ®é›†")
    
    if data_valid:
        # æ£€æŸ¥æ•°æ®æ ¼å¼ - æ”¯æŒä¸¤ç§æ ¼å¼
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                if first_line:
                    sample = json.loads(first_line)
                    print(f"ğŸ“ æ•°æ®æ ·æœ¬è°ƒè¯•ä¿¡æ¯:")
                    print(f"    å®é™…é”®: {sorted(sample.keys())}")
                    print(f"    æ ·æœ¬å†…å®¹é¢„è§ˆ: {dict(list(sample.items())[:3])}")
                    
                    # æ£€æŸ¥æ ‡å‡†æ ¼å¼
                    standard_keys = ['instruction', 'input', 'output']
                    has_standard = all(key in sample for key in standard_keys)
                    
                    # æ£€æŸ¥commonsenseæ ¼å¼
                    commonsense_keys = ['id', 'dataset', 'task_type', 'input', 'options', 'target']
                    has_commonsense = all(key in sample for key in commonsense_keys)
                    
                    print(f"    æ ‡å‡†æ ¼å¼æ£€æŸ¥ ({standard_keys}):")
                    for key in standard_keys:
                        status = "âœ…" if key in sample else "âŒ"
                        print(f"      {status} {key}")
                    
                    print(f"    Commonsenseæ ¼å¼æ£€æŸ¥ ({commonsense_keys}):")
                    for key in commonsense_keys:
                        status = "âœ…" if key in sample else "âŒ"
                        print(f"      {status} {key}")
                    
                    if has_standard:
                        print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡ (æ ‡å‡†æ ¼å¼)")
                    elif has_commonsense:
                        print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡ (Commonsenseæ ¼å¼)")
                        print(f"    ä»»åŠ¡ç±»å‹: {sample.get('task_type', 'N/A')}")
                        print(f"    æ•°æ®é›†: {sample.get('dataset', 'N/A')}")
                        print(f"    é€‰é¡¹æ•°é‡: {len(sample.get('options', []))}")
                    else:
                        print("âŒ æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥")
                        missing_standard = set(standard_keys) - set(sample.keys())
                        missing_commonsense = set(commonsense_keys) - set(sample.keys())
                        print(f"    ç¼ºå°‘æ ‡å‡†æ ¼å¼é”®: {missing_standard}")
                        print(f"    ç¼ºå°‘Commonsenseæ ¼å¼é”®: {missing_commonsense}")
                        data_valid = False
                        data_valid = False
        except Exception as e:
            print(f"âŒ æ•°æ®æ ¼å¼æ£€æŸ¥å¤±è´¥: {e}")
            data_valid = False
    
    return model_valid and data_valid

def test_configuration():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print_section("éªŒè¯é…ç½®æ–‡ä»¶")
    
    config_path = "configs/training_config.yaml"
    
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return False
    
    try:
        import yaml
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æ£€æŸ¥å…³é”®é…ç½®
        required_sections = [
            'model',
            'data', 
            'lora',
            'training',
            'checkpoint'
        ]
        
        all_sections_present = True
        for section in required_sections:
            if section in config:
                print(f"âœ… é…ç½®èŠ‚ '{section}': å­˜åœ¨")
            else:
                print(f"âŒ é…ç½®èŠ‚ '{section}': ç¼ºå¤±")
                all_sections_present = False
        
        # æ£€æŸ¥å…³é”®è·¯å¾„
        if 'model' in config and 'local_path' in config['model']:
            model_path = config['model']['local_path']
            if os.path.exists(model_path):
                print(f"âœ… é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„: æœ‰æ•ˆ")
            else:
                print(f"âŒ é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„: æ— æ•ˆ ({model_path})")
                all_sections_present = False
        
        if 'data' in config and 'train_file' in config['data']:
            data_path = config['data']['train_file']
            if os.path.exists(data_path):
                print(f"âœ… é…ç½®ä¸­çš„æ•°æ®è·¯å¾„: æœ‰æ•ˆ")
            else:
                print(f"âŒ é…ç½®ä¸­çš„æ•°æ®è·¯å¾„: æ— æ•ˆ ({data_path})")
                all_sections_present = False
        
        return all_sections_present
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
        return False

def test_training_components():
    """æµ‹è¯•è®­ç»ƒç»„ä»¶å¯¼å…¥"""
    print_section("æµ‹è¯•è®­ç»ƒç»„ä»¶")
    
    # æ·»åŠ é¡¹ç›®è·¯å¾„
    project_root = os.path.dirname(os.path.abspath(__file__))
    if project_root not in sys.path:
        sys.path.append(project_root)
    
    components_to_test = [
        ("utils.data_processor", "DataProcessor", "æ•°æ®å¤„ç†å™¨"),
        ("utils.scheduler", "TwoStageScheduler", "ä¸¤é˜¶æ®µè°ƒåº¦å™¨"),
        ("lora.checkpoint_utils", "CheckpointManager", "Checkpointç®¡ç†å™¨"),
        ("scripts.model_manager", "ModelManager", "æ¨¡å‹ç®¡ç†å™¨")
    ]
    
    all_components_ok = True
    
    for module_name, class_name, description in components_to_test:
        try:
            module = __import__(module_name, fromlist=[class_name])
            component_class = getattr(module, class_name)
            print(f"âœ… {description}: å¯å¯¼å…¥")
        except ImportError as e:
            print(f"âŒ {description}: å¯¼å…¥å¤±è´¥ ({e})")
            all_components_ok = False
        except AttributeError as e:
            print(f"âŒ {description}: ç±»ä¸å­˜åœ¨ ({e})")
            all_components_ok = False
        except Exception as e:
            print(f"âŒ {description}: å…¶ä»–é”™è¯¯ ({e})")
            all_components_ok = False
    
    return all_components_ok

def generate_validation_report():
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    print_section("ç”ŸæˆéªŒè¯æŠ¥å‘Š")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = {
        "timestamp": datetime.now().isoformat(),
        "tests": {
            "imports": test_imports(),
            "project_structure": test_project_structure(),
            "model_and_data": test_model_and_data(),
            "configuration": test_configuration(),
            "training_components": test_training_components()
        }
    }
    
    # è®¡ç®—æ€»ä½“ç»“æœ
    all_passed = all(results["tests"].values())
    results["overall_status"] = "PASS" if all_passed else "FAIL"
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = "validation_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ€»ç»“
    print_section("éªŒè¯æ€»ç»“")
    print(f"æ€»ä½“çŠ¶æ€: {'âœ… é€šè¿‡' if all_passed else 'âŒ å¤±è´¥'}")
    print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    for test_name, status in results["tests"].items():
        status_icon = "âœ…" if status else "âŒ"
        print(f"{status_icon} {test_name}: {'é€šè¿‡' if status else 'å¤±è´¥'}")
    
    # ä¸ºå¤±è´¥çš„æµ‹è¯•æä¾›é¢å¤–ä¿¡æ¯
    if not all_passed:
        print(f"\nğŸ” å¤±è´¥é¡¹ç›®è¯¦ç»†ä¿¡æ¯:")
        failed_tests = [name for name, status in results["tests"].items() if not status]
        
        for test_name in failed_tests:
            print(f"\nâŒ {test_name} å¤±è´¥:")
            
            if test_name == "model_and_data":
                print(f"   - è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼ˆconfig.json, model.safetensorsç­‰ï¼‰")
                print(f"   - è¯·ç¡®è®¤æ•°æ®æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ˆæ”¯æŒcommonsenseæ ¼å¼ï¼‰")
                print(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: raw_datasets/commonsense/cs_all_unbalanced.jsonl")
                print(f"   - é¢„æœŸæ•°æ®é”®: ['id', 'dataset', 'task_type', 'input', 'options', 'target']")
            
            elif test_name == "imports":
                print(f"   - è¯·å®‰è£…ç¼ºå¤±çš„PythonåŒ…")
                print(f"   - è¿è¡Œ: conda install peft transformers torch")
            
            elif test_name == "project_structure":
                print(f"   - è¯·æ£€æŸ¥é¡¹ç›®æ–‡ä»¶ç»“æ„æ˜¯å¦å®Œæ•´")
                print(f"   - ç¡®è®¤æ‰€æœ‰å¿…éœ€çš„è„šæœ¬å’Œé…ç½®æ–‡ä»¶å­˜åœ¨")
            
            elif test_name == "configuration":
                print(f"   - è¯·æ£€æŸ¥configs/training_config.yamlé…ç½®æ–‡ä»¶")
                print(f"   - ç¡®è®¤æ‰€æœ‰å¿…éœ€çš„é…ç½®èŠ‚å­˜åœ¨")
            
            elif test_name == "training_components":
                print(f"   - è¯·æ£€æŸ¥è®­ç»ƒç»„ä»¶æ˜¯å¦èƒ½æ­£å¸¸å¯¼å…¥")
                print(f"   - å¯èƒ½å­˜åœ¨Pythonè¯­æ³•é”™è¯¯æˆ–ä¾èµ–é—®é¢˜")
    
    if all_passed:
        print(f"\nğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
        print(f"è¿è¡Œå‘½ä»¤: python train_commonsense_lora.py --validate_only")
    else:
        print(f"\nâš ï¸ å­˜åœ¨é—®é¢˜ï¼Œè¯·å…ˆè§£å†³åå†å¼€å§‹è®­ç»ƒã€‚")
        print(f"ğŸ’¡ æç¤º: æŸ¥çœ‹ä¸Šæ–¹çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œæˆ–æ£€æŸ¥ {report_file} è·å–å®Œæ•´æŠ¥å‘Š")
    
    return all_passed

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” P2Wé¡¹ç›® - LoRAè®­ç»ƒç¯å¢ƒéªŒè¯")
    print(f"éªŒè¯æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    success = generate_validation_report()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
